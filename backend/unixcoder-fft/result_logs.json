{
    "training_logs": [
        {
            "loss": 1.7383,
            "grad_norm": 11.432941436767578,
            "learning_rate": 9.983333333333333e-06,
            "epoch": 0.016666666666666666,
            "step": 10
        },
        {
            "loss": 1.6786,
            "grad_norm": 14.19034481048584,
            "learning_rate": 9.966666666666667e-06,
            "epoch": 0.03333333333333333,
            "step": 20
        },
        {
            "loss": 1.5424,
            "grad_norm": 14.330848693847656,
            "learning_rate": 9.950000000000001e-06,
            "epoch": 0.05,
            "step": 30
        },
        {
            "loss": 1.3505,
            "grad_norm": 13.92200756072998,
            "learning_rate": 9.933333333333334e-06,
            "epoch": 0.06666666666666667,
            "step": 40
        },
        {
            "loss": 1.2158,
            "grad_norm": 14.548402786254883,
            "learning_rate": 9.916666666666668e-06,
            "epoch": 0.08333333333333333,
            "step": 50
        },
        {
            "loss": 1.0477,
            "grad_norm": 12.870659828186035,
            "learning_rate": 9.9e-06,
            "epoch": 0.1,
            "step": 60
        },
        {
            "loss": 0.8992,
            "grad_norm": 9.54006290435791,
            "learning_rate": 9.883333333333334e-06,
            "epoch": 0.11666666666666667,
            "step": 70
        },
        {
            "loss": 0.8412,
            "grad_norm": 9.568286895751953,
            "learning_rate": 9.866666666666668e-06,
            "epoch": 0.13333333333333333,
            "step": 80
        },
        {
            "loss": 0.7851,
            "grad_norm": 9.131248474121094,
            "learning_rate": 9.85e-06,
            "epoch": 0.15,
            "step": 90
        },
        {
            "loss": 0.5967,
            "grad_norm": 8.444557189941406,
            "learning_rate": 9.833333333333333e-06,
            "epoch": 0.16666666666666666,
            "step": 100
        },
        {
            "loss": 0.5783,
            "grad_norm": 10.013657569885254,
            "learning_rate": 9.816666666666667e-06,
            "epoch": 0.18333333333333332,
            "step": 110
        },
        {
            "loss": 0.5051,
            "grad_norm": 14.4801664352417,
            "learning_rate": 9.800000000000001e-06,
            "epoch": 0.2,
            "step": 120
        },
        {
            "loss": 0.3302,
            "grad_norm": 9.115341186523438,
            "learning_rate": 9.783333333333335e-06,
            "epoch": 0.21666666666666667,
            "step": 130
        },
        {
            "loss": 0.4857,
            "grad_norm": 2.595121383666992,
            "learning_rate": 9.766666666666667e-06,
            "epoch": 0.23333333333333334,
            "step": 140
        },
        {
            "loss": 0.5409,
            "grad_norm": 4.498383522033691,
            "learning_rate": 9.75e-06,
            "epoch": 0.25,
            "step": 150
        },
        {
            "loss": 0.6772,
            "grad_norm": 4.507877349853516,
            "learning_rate": 9.733333333333334e-06,
            "epoch": 0.26666666666666666,
            "step": 160
        },
        {
            "loss": 0.3515,
            "grad_norm": 6.755524158477783,
            "learning_rate": 9.716666666666668e-06,
            "epoch": 0.2833333333333333,
            "step": 170
        },
        {
            "loss": 0.3607,
            "grad_norm": 9.421173095703125,
            "learning_rate": 9.7e-06,
            "epoch": 0.3,
            "step": 180
        },
        {
            "loss": 0.348,
            "grad_norm": 9.882153511047363,
            "learning_rate": 9.683333333333334e-06,
            "epoch": 0.31666666666666665,
            "step": 190
        },
        {
            "loss": 0.4531,
            "grad_norm": 4.663234710693359,
            "learning_rate": 9.666666666666667e-06,
            "epoch": 0.3333333333333333,
            "step": 200
        },
        {
            "loss": 0.3483,
            "grad_norm": 11.590539932250977,
            "learning_rate": 9.65e-06,
            "epoch": 0.35,
            "step": 210
        },
        {
            "loss": 0.4137,
            "grad_norm": 9.283149719238281,
            "learning_rate": 9.633333333333335e-06,
            "epoch": 0.36666666666666664,
            "step": 220
        },
        {
            "loss": 0.4686,
            "grad_norm": 9.722925186157227,
            "learning_rate": 9.616666666666667e-06,
            "epoch": 0.38333333333333336,
            "step": 230
        },
        {
            "loss": 0.3233,
            "grad_norm": 11.621769905090332,
            "learning_rate": 9.600000000000001e-06,
            "epoch": 0.4,
            "step": 240
        },
        {
            "loss": 0.3236,
            "grad_norm": 8.264516830444336,
            "learning_rate": 9.583333333333335e-06,
            "epoch": 0.4166666666666667,
            "step": 250
        },
        {
            "loss": 0.1378,
            "grad_norm": 10.197396278381348,
            "learning_rate": 9.566666666666668e-06,
            "epoch": 0.43333333333333335,
            "step": 260
        },
        {
            "loss": 0.3063,
            "grad_norm": 15.607884407043457,
            "learning_rate": 9.55e-06,
            "epoch": 0.45,
            "step": 270
        },
        {
            "loss": 0.2786,
            "grad_norm": 1.3239308595657349,
            "learning_rate": 9.533333333333334e-06,
            "epoch": 0.4666666666666667,
            "step": 280
        },
        {
            "loss": 0.3908,
            "grad_norm": 1.3521387577056885,
            "learning_rate": 9.516666666666668e-06,
            "epoch": 0.48333333333333334,
            "step": 290
        },
        {
            "loss": 0.2829,
            "grad_norm": 8.52108383178711,
            "learning_rate": 9.5e-06,
            "epoch": 0.5,
            "step": 300
        },
        {
            "loss": 0.2964,
            "grad_norm": 9.722640991210938,
            "learning_rate": 9.483333333333335e-06,
            "epoch": 0.5166666666666667,
            "step": 310
        },
        {
            "loss": 0.2021,
            "grad_norm": 1.2596265077590942,
            "learning_rate": 9.466666666666667e-06,
            "epoch": 0.5333333333333333,
            "step": 320
        },
        {
            "loss": 0.2363,
            "grad_norm": 0.8847758769989014,
            "learning_rate": 9.450000000000001e-06,
            "epoch": 0.55,
            "step": 330
        },
        {
            "loss": 0.2765,
            "grad_norm": 11.31650447845459,
            "learning_rate": 9.433333333333335e-06,
            "epoch": 0.5666666666666667,
            "step": 340
        },
        {
            "loss": 0.258,
            "grad_norm": 8.291569709777832,
            "learning_rate": 9.416666666666667e-06,
            "epoch": 0.5833333333333334,
            "step": 350
        },
        {
            "loss": 0.2655,
            "grad_norm": 0.24184851348400116,
            "learning_rate": 9.4e-06,
            "epoch": 0.6,
            "step": 360
        },
        {
            "loss": 0.2322,
            "grad_norm": 11.118288040161133,
            "learning_rate": 9.383333333333334e-06,
            "epoch": 0.6166666666666667,
            "step": 370
        },
        {
            "loss": 0.3896,
            "grad_norm": 22.305715560913086,
            "learning_rate": 9.366666666666668e-06,
            "epoch": 0.6333333333333333,
            "step": 380
        },
        {
            "loss": 0.2147,
            "grad_norm": 24.285232543945312,
            "learning_rate": 9.350000000000002e-06,
            "epoch": 0.65,
            "step": 390
        },
        {
            "loss": 0.2751,
            "grad_norm": 16.673067092895508,
            "learning_rate": 9.333333333333334e-06,
            "epoch": 0.6666666666666666,
            "step": 400
        },
        {
            "loss": 0.0881,
            "grad_norm": 0.8021448850631714,
            "learning_rate": 9.316666666666667e-06,
            "epoch": 0.6833333333333333,
            "step": 410
        },
        {
            "loss": 0.2733,
            "grad_norm": 8.975751876831055,
            "learning_rate": 9.3e-06,
            "epoch": 0.7,
            "step": 420
        },
        {
            "loss": 0.3147,
            "grad_norm": 12.833009719848633,
            "learning_rate": 9.283333333333335e-06,
            "epoch": 0.7166666666666667,
            "step": 430
        },
        {
            "loss": 0.2259,
            "grad_norm": 3.122584104537964,
            "learning_rate": 9.266666666666667e-06,
            "epoch": 0.7333333333333333,
            "step": 440
        },
        {
            "loss": 0.2579,
            "grad_norm": 0.3902605175971985,
            "learning_rate": 9.250000000000001e-06,
            "epoch": 0.75,
            "step": 450
        },
        {
            "loss": 0.2002,
            "grad_norm": 10.056445121765137,
            "learning_rate": 9.233333333333334e-06,
            "epoch": 0.7666666666666667,
            "step": 460
        },
        {
            "loss": 0.1931,
            "grad_norm": 12.986915588378906,
            "learning_rate": 9.216666666666668e-06,
            "epoch": 0.7833333333333333,
            "step": 470
        },
        {
            "loss": 0.2531,
            "grad_norm": 2.6460113525390625,
            "learning_rate": 9.200000000000002e-06,
            "epoch": 0.8,
            "step": 480
        },
        {
            "loss": 0.2475,
            "grad_norm": 0.2732101380825043,
            "learning_rate": 9.183333333333334e-06,
            "epoch": 0.8166666666666667,
            "step": 490
        },
        {
            "loss": 0.286,
            "grad_norm": 5.030919551849365,
            "learning_rate": 9.166666666666666e-06,
            "epoch": 0.8333333333333334,
            "step": 500
        },
        {
            "loss": 0.0765,
            "grad_norm": 6.701961517333984,
            "learning_rate": 9.15e-06,
            "epoch": 0.85,
            "step": 510
        },
        {
            "loss": 0.2484,
            "grad_norm": 14.845745086669922,
            "learning_rate": 9.133333333333335e-06,
            "epoch": 0.8666666666666667,
            "step": 520
        },
        {
            "loss": 0.379,
            "grad_norm": 5.400842666625977,
            "learning_rate": 9.116666666666667e-06,
            "epoch": 0.8833333333333333,
            "step": 530
        },
        {
            "loss": 0.1415,
            "grad_norm": 9.397371292114258,
            "learning_rate": 9.100000000000001e-06,
            "epoch": 0.9,
            "step": 540
        },
        {
            "loss": 0.1415,
            "grad_norm": 3.878653049468994,
            "learning_rate": 9.083333333333333e-06,
            "epoch": 0.9166666666666666,
            "step": 550
        },
        {
            "loss": 0.1576,
            "grad_norm": 3.23270320892334,
            "learning_rate": 9.066666666666667e-06,
            "epoch": 0.9333333333333333,
            "step": 560
        },
        {
            "loss": 0.0788,
            "grad_norm": 8.759906768798828,
            "learning_rate": 9.050000000000001e-06,
            "epoch": 0.95,
            "step": 570
        },
        {
            "loss": 0.3119,
            "grad_norm": 24.68311882019043,
            "learning_rate": 9.033333333333334e-06,
            "epoch": 0.9666666666666667,
            "step": 580
        },
        {
            "loss": 0.2278,
            "grad_norm": 9.78148078918457,
            "learning_rate": 9.016666666666666e-06,
            "epoch": 0.9833333333333333,
            "step": 590
        },
        {
            "loss": 0.3017,
            "grad_norm": 11.348222732543945,
            "learning_rate": 9e-06,
            "epoch": 1.0,
            "step": 600
        },
        {
            "eval_loss": 0.18261080980300903,
            "eval_accuracy": 0.94,
            "eval_precision": 0.9433786440815369,
            "eval_recall": 0.94,
            "eval_f1": 0.9401308746502015,
            "eval_runtime": 118.8188,
            "eval_samples_per_second": 10.099,
            "eval_steps_per_second": 1.262,
            "epoch": 1.0,
            "step": 600
        },
        {
            "loss": 0.1581,
            "grad_norm": 0.3314168453216553,
            "learning_rate": 8.983333333333334e-06,
            "epoch": 1.0166666666666666,
            "step": 610
        },
        {
            "loss": 0.1418,
            "grad_norm": 1.1948506832122803,
            "learning_rate": 8.966666666666667e-06,
            "epoch": 1.0333333333333334,
            "step": 620
        },
        {
            "loss": 0.0829,
            "grad_norm": 9.928191184997559,
            "learning_rate": 8.95e-06,
            "epoch": 1.05,
            "step": 630
        },
        {
            "loss": 0.1973,
            "grad_norm": 10.2186861038208,
            "learning_rate": 8.933333333333333e-06,
            "epoch": 1.0666666666666667,
            "step": 640
        },
        {
            "loss": 0.1975,
            "grad_norm": 13.628978729248047,
            "learning_rate": 8.916666666666667e-06,
            "epoch": 1.0833333333333333,
            "step": 650
        },
        {
            "loss": 0.2686,
            "grad_norm": 11.395072937011719,
            "learning_rate": 8.900000000000001e-06,
            "epoch": 1.1,
            "step": 660
        },
        {
            "loss": 0.2379,
            "grad_norm": 1.462990403175354,
            "learning_rate": 8.883333333333334e-06,
            "epoch": 1.1166666666666667,
            "step": 670
        },
        {
            "loss": 0.1144,
            "grad_norm": 1.8012938499450684,
            "learning_rate": 8.866666666666668e-06,
            "epoch": 1.1333333333333333,
            "step": 680
        },
        {
            "loss": 0.2,
            "grad_norm": 9.171284675598145,
            "learning_rate": 8.85e-06,
            "epoch": 1.15,
            "step": 690
        },
        {
            "loss": 0.1676,
            "grad_norm": 0.13544844090938568,
            "learning_rate": 8.833333333333334e-06,
            "epoch": 1.1666666666666667,
            "step": 700
        },
        {
            "loss": 0.0746,
            "grad_norm": 1.4323152303695679,
            "learning_rate": 8.816666666666668e-06,
            "epoch": 1.1833333333333333,
            "step": 710
        },
        {
            "loss": 0.2642,
            "grad_norm": 1.7547316551208496,
            "learning_rate": 8.8e-06,
            "epoch": 1.2,
            "step": 720
        },
        {
            "loss": 0.0921,
            "grad_norm": 3.5125932693481445,
            "learning_rate": 8.783333333333335e-06,
            "epoch": 1.2166666666666668,
            "step": 730
        },
        {
            "loss": 0.1651,
            "grad_norm": 1.6321663856506348,
            "learning_rate": 8.766666666666669e-06,
            "epoch": 1.2333333333333334,
            "step": 740
        },
        {
            "loss": 0.236,
            "grad_norm": 7.587852478027344,
            "learning_rate": 8.750000000000001e-06,
            "epoch": 1.25,
            "step": 750
        },
        {
            "loss": 0.2848,
            "grad_norm": 0.28395044803619385,
            "learning_rate": 8.733333333333333e-06,
            "epoch": 1.2666666666666666,
            "step": 760
        },
        {
            "loss": 0.1772,
            "grad_norm": 21.88641929626465,
            "learning_rate": 8.716666666666667e-06,
            "epoch": 1.2833333333333332,
            "step": 770
        },
        {
            "loss": 0.1602,
            "grad_norm": 0.09726864099502563,
            "learning_rate": 8.700000000000001e-06,
            "epoch": 1.3,
            "step": 780
        },
        {
            "loss": 0.1556,
            "grad_norm": 0.2109643667936325,
            "learning_rate": 8.683333333333334e-06,
            "epoch": 1.3166666666666667,
            "step": 790
        },
        {
            "loss": 0.0145,
            "grad_norm": 0.0333416648209095,
            "learning_rate": 8.666666666666668e-06,
            "epoch": 1.3333333333333333,
            "step": 800
        },
        {
            "loss": 0.2655,
            "grad_norm": 0.0979752168059349,
            "learning_rate": 8.65e-06,
            "epoch": 1.35,
            "step": 810
        },
        {
            "loss": 0.2164,
            "grad_norm": 0.259640634059906,
            "learning_rate": 8.633333333333334e-06,
            "epoch": 1.3666666666666667,
            "step": 820
        },
        {
            "loss": 0.1313,
            "grad_norm": 0.05315262824296951,
            "learning_rate": 8.616666666666668e-06,
            "epoch": 1.3833333333333333,
            "step": 830
        },
        {
            "loss": 0.09,
            "grad_norm": 0.28059133887290955,
            "learning_rate": 8.6e-06,
            "epoch": 1.4,
            "step": 840
        },
        {
            "loss": 0.2688,
            "grad_norm": 0.0521833710372448,
            "learning_rate": 8.583333333333333e-06,
            "epoch": 1.4166666666666667,
            "step": 850
        },
        {
            "loss": 0.225,
            "grad_norm": 24.159326553344727,
            "learning_rate": 8.566666666666667e-06,
            "epoch": 1.4333333333333333,
            "step": 860
        },
        {
            "loss": 0.1868,
            "grad_norm": 0.3825375735759735,
            "learning_rate": 8.550000000000001e-06,
            "epoch": 1.45,
            "step": 870
        },
        {
            "loss": 0.2319,
            "grad_norm": 2.1184284687042236,
            "learning_rate": 8.533333333333335e-06,
            "epoch": 1.4666666666666668,
            "step": 880
        },
        {
            "loss": 0.195,
            "grad_norm": 0.07062701880931854,
            "learning_rate": 8.516666666666668e-06,
            "epoch": 1.4833333333333334,
            "step": 890
        },
        {
            "loss": 0.2389,
            "grad_norm": 6.458399295806885,
            "learning_rate": 8.5e-06,
            "epoch": 1.5,
            "step": 900
        },
        {
            "loss": 0.1275,
            "grad_norm": 15.409672737121582,
            "learning_rate": 8.483333333333334e-06,
            "epoch": 1.5166666666666666,
            "step": 910
        },
        {
            "loss": 0.1363,
            "grad_norm": 10.316125869750977,
            "learning_rate": 8.466666666666668e-06,
            "epoch": 1.5333333333333332,
            "step": 920
        },
        {
            "loss": 0.2044,
            "grad_norm": 14.371610641479492,
            "learning_rate": 8.45e-06,
            "epoch": 1.55,
            "step": 930
        },
        {
            "loss": 0.053,
            "grad_norm": 0.1754542887210846,
            "learning_rate": 8.433333333333334e-06,
            "epoch": 1.5666666666666667,
            "step": 940
        },
        {
            "loss": 0.4166,
            "grad_norm": 7.957895755767822,
            "learning_rate": 8.416666666666667e-06,
            "epoch": 1.5833333333333335,
            "step": 950
        },
        {
            "loss": 0.2182,
            "grad_norm": 1.6599057912826538,
            "learning_rate": 8.400000000000001e-06,
            "epoch": 1.6,
            "step": 960
        },
        {
            "loss": 0.1287,
            "grad_norm": 9.93993091583252,
            "learning_rate": 8.383333333333335e-06,
            "epoch": 1.6166666666666667,
            "step": 970
        },
        {
            "loss": 0.1136,
            "grad_norm": 0.13848870992660522,
            "learning_rate": 8.366666666666667e-06,
            "epoch": 1.6333333333333333,
            "step": 980
        },
        {
            "loss": 0.1271,
            "grad_norm": 15.165847778320312,
            "learning_rate": 8.35e-06,
            "epoch": 1.65,
            "step": 990
        },
        {
            "loss": 0.2334,
            "grad_norm": 0.07027367502450943,
            "learning_rate": 8.333333333333334e-06,
            "epoch": 1.6666666666666665,
            "step": 1000
        },
        {
            "loss": 0.1167,
            "grad_norm": 0.04609718918800354,
            "learning_rate": 8.316666666666668e-06,
            "epoch": 1.6833333333333333,
            "step": 1010
        },
        {
            "loss": 0.1616,
            "grad_norm": 20.989755630493164,
            "learning_rate": 8.3e-06,
            "epoch": 1.7,
            "step": 1020
        },
        {
            "loss": 0.1434,
            "grad_norm": 0.3677842915058136,
            "learning_rate": 8.283333333333334e-06,
            "epoch": 1.7166666666666668,
            "step": 1030
        },
        {
            "loss": 0.1614,
            "grad_norm": 0.09606911987066269,
            "learning_rate": 8.266666666666667e-06,
            "epoch": 1.7333333333333334,
            "step": 1040
        },
        {
            "loss": 0.1053,
            "grad_norm": 0.4662019610404968,
            "learning_rate": 8.25e-06,
            "epoch": 1.75,
            "step": 1050
        },
        {
            "loss": 0.1495,
            "grad_norm": 18.755815505981445,
            "learning_rate": 8.233333333333335e-06,
            "epoch": 1.7666666666666666,
            "step": 1060
        },
        {
            "loss": 0.0909,
            "grad_norm": 0.06034258380532265,
            "learning_rate": 8.216666666666667e-06,
            "epoch": 1.7833333333333332,
            "step": 1070
        },
        {
            "loss": 0.1832,
            "grad_norm": 0.5567213296890259,
            "learning_rate": 8.2e-06,
            "epoch": 1.8,
            "step": 1080
        },
        {
            "loss": 0.066,
            "grad_norm": 1.1310830116271973,
            "learning_rate": 8.183333333333333e-06,
            "epoch": 1.8166666666666667,
            "step": 1090
        },
        {
            "loss": 0.1379,
            "grad_norm": 0.051646146923303604,
            "learning_rate": 8.166666666666668e-06,
            "epoch": 1.8333333333333335,
            "step": 1100
        },
        {
            "loss": 0.1557,
            "grad_norm": 3.49533748626709,
            "learning_rate": 8.15e-06,
            "epoch": 1.85,
            "step": 1110
        },
        {
            "loss": 0.0398,
            "grad_norm": 0.5545110702514648,
            "learning_rate": 8.133333333333334e-06,
            "epoch": 1.8666666666666667,
            "step": 1120
        },
        {
            "loss": 0.1808,
            "grad_norm": 10.176505088806152,
            "learning_rate": 8.116666666666666e-06,
            "epoch": 1.8833333333333333,
            "step": 1130
        },
        {
            "loss": 0.1406,
            "grad_norm": 0.03961896151304245,
            "learning_rate": 8.1e-06,
            "epoch": 1.9,
            "step": 1140
        },
        {
            "loss": 0.0806,
            "grad_norm": 0.37254831194877625,
            "learning_rate": 8.083333333333334e-06,
            "epoch": 1.9166666666666665,
            "step": 1150
        },
        {
            "loss": 0.2079,
            "grad_norm": 0.15390458703041077,
            "learning_rate": 8.066666666666667e-06,
            "epoch": 1.9333333333333333,
            "step": 1160
        },
        {
            "loss": 0.0375,
            "grad_norm": 0.0793287456035614,
            "learning_rate": 8.050000000000001e-06,
            "epoch": 1.95,
            "step": 1170
        },
        {
            "loss": 0.2219,
            "grad_norm": 15.721748352050781,
            "learning_rate": 8.033333333333335e-06,
            "epoch": 1.9666666666666668,
            "step": 1180
        },
        {
            "loss": 0.1879,
            "grad_norm": 0.8312768340110779,
            "learning_rate": 8.016666666666667e-06,
            "epoch": 1.9833333333333334,
            "step": 1190
        },
        {
            "loss": 0.0336,
            "grad_norm": 4.72592306137085,
            "learning_rate": 8.000000000000001e-06,
            "epoch": 2.0,
            "step": 1200
        },
        {
            "eval_loss": 0.19865907728672028,
            "eval_accuracy": 0.9475,
            "eval_precision": 0.950204200658308,
            "eval_recall": 0.9475,
            "eval_f1": 0.9476846763859693,
            "eval_runtime": 75.8962,
            "eval_samples_per_second": 15.811,
            "eval_steps_per_second": 1.976,
            "epoch": 2.0,
            "step": 1200
        },
        {
            "loss": 0.2283,
            "grad_norm": 21.006715774536133,
            "learning_rate": 7.983333333333334e-06,
            "epoch": 2.0166666666666666,
            "step": 1210
        },
        {
            "loss": 0.0996,
            "grad_norm": 0.4137037992477417,
            "learning_rate": 7.966666666666668e-06,
            "epoch": 2.033333333333333,
            "step": 1220
        },
        {
            "loss": 0.0921,
            "grad_norm": 9.836175918579102,
            "learning_rate": 7.950000000000002e-06,
            "epoch": 2.05,
            "step": 1230
        },
        {
            "loss": 0.1709,
            "grad_norm": 7.4577789306640625,
            "learning_rate": 7.933333333333334e-06,
            "epoch": 2.066666666666667,
            "step": 1240
        },
        {
            "loss": 0.0464,
            "grad_norm": 23.27369499206543,
            "learning_rate": 7.916666666666667e-06,
            "epoch": 2.0833333333333335,
            "step": 1250
        },
        {
            "loss": 0.1892,
            "grad_norm": 0.09680437296628952,
            "learning_rate": 7.9e-06,
            "epoch": 2.1,
            "step": 1260
        },
        {
            "loss": 0.1985,
            "grad_norm": 10.637593269348145,
            "learning_rate": 7.883333333333335e-06,
            "epoch": 2.1166666666666667,
            "step": 1270
        },
        {
            "loss": 0.0712,
            "grad_norm": 1.1281434297561646,
            "learning_rate": 7.866666666666667e-06,
            "epoch": 2.1333333333333333,
            "step": 1280
        },
        {
            "loss": 0.0258,
            "grad_norm": 0.2405150681734085,
            "learning_rate": 7.850000000000001e-06,
            "epoch": 2.15,
            "step": 1290
        },
        {
            "loss": 0.1049,
            "grad_norm": 0.10346455127000809,
            "learning_rate": 7.833333333333333e-06,
            "epoch": 2.1666666666666665,
            "step": 1300
        },
        {
            "loss": 0.2143,
            "grad_norm": 15.858503341674805,
            "learning_rate": 7.816666666666667e-06,
            "epoch": 2.183333333333333,
            "step": 1310
        },
        {
            "loss": 0.1813,
            "grad_norm": 2.383065700531006,
            "learning_rate": 7.800000000000002e-06,
            "epoch": 2.2,
            "step": 1320
        },
        {
            "loss": 0.0877,
            "grad_norm": 0.020290043205022812,
            "learning_rate": 7.783333333333334e-06,
            "epoch": 2.216666666666667,
            "step": 1330
        },
        {
            "loss": 0.0683,
            "grad_norm": 0.051980335265398026,
            "learning_rate": 7.766666666666666e-06,
            "epoch": 2.2333333333333334,
            "step": 1340
        },
        {
            "loss": 0.0179,
            "grad_norm": 0.03955931216478348,
            "learning_rate": 7.75e-06,
            "epoch": 2.25,
            "step": 1350
        },
        {
            "loss": 0.247,
            "grad_norm": 18.616220474243164,
            "learning_rate": 7.733333333333334e-06,
            "epoch": 2.2666666666666666,
            "step": 1360
        },
        {
            "loss": 0.1191,
            "grad_norm": 19.473052978515625,
            "learning_rate": 7.716666666666667e-06,
            "epoch": 2.283333333333333,
            "step": 1370
        },
        {
            "loss": 0.0047,
            "grad_norm": 0.01913151517510414,
            "learning_rate": 7.7e-06,
            "epoch": 2.3,
            "step": 1380
        },
        {
            "loss": 0.13,
            "grad_norm": 0.28296390175819397,
            "learning_rate": 7.683333333333333e-06,
            "epoch": 2.3166666666666664,
            "step": 1390
        },
        {
            "loss": 0.1932,
            "grad_norm": 1.9111236333847046,
            "learning_rate": 7.666666666666667e-06,
            "epoch": 2.3333333333333335,
            "step": 1400
        },
        {
            "loss": 0.2447,
            "grad_norm": 9.872632026672363,
            "learning_rate": 7.650000000000001e-06,
            "epoch": 2.35,
            "step": 1410
        },
        {
            "loss": 0.118,
            "grad_norm": 10.619919776916504,
            "learning_rate": 7.633333333333334e-06,
            "epoch": 2.3666666666666667,
            "step": 1420
        },
        {
            "loss": 0.0552,
            "grad_norm": 12.382402420043945,
            "learning_rate": 7.616666666666668e-06,
            "epoch": 2.3833333333333333,
            "step": 1430
        },
        {
            "loss": 0.2128,
            "grad_norm": 15.440667152404785,
            "learning_rate": 7.600000000000001e-06,
            "epoch": 2.4,
            "step": 1440
        },
        {
            "loss": 0.0476,
            "grad_norm": 0.054897699505090714,
            "learning_rate": 7.583333333333333e-06,
            "epoch": 2.4166666666666665,
            "step": 1450
        },
        {
            "loss": 0.2145,
            "grad_norm": 1.4737387895584106,
            "learning_rate": 7.566666666666667e-06,
            "epoch": 2.4333333333333336,
            "step": 1460
        },
        {
            "loss": 0.3469,
            "grad_norm": 44.44685363769531,
            "learning_rate": 7.5500000000000006e-06,
            "epoch": 2.45,
            "step": 1470
        },
        {
            "loss": 0.1164,
            "grad_norm": 3.421713352203369,
            "learning_rate": 7.533333333333334e-06,
            "epoch": 2.466666666666667,
            "step": 1480
        },
        {
            "loss": 0.1487,
            "grad_norm": 4.801156520843506,
            "learning_rate": 7.516666666666668e-06,
            "epoch": 2.4833333333333334,
            "step": 1490
        },
        {
            "loss": 0.0415,
            "grad_norm": 38.93131637573242,
            "learning_rate": 7.500000000000001e-06,
            "epoch": 2.5,
            "step": 1500
        },
        {
            "loss": 0.0668,
            "grad_norm": 0.04426191374659538,
            "learning_rate": 7.483333333333333e-06,
            "epoch": 2.5166666666666666,
            "step": 1510
        },
        {
            "loss": 0.2197,
            "grad_norm": 5.448146343231201,
            "learning_rate": 7.4666666666666675e-06,
            "epoch": 2.533333333333333,
            "step": 1520
        },
        {
            "loss": 0.1448,
            "grad_norm": 0.13123677670955658,
            "learning_rate": 7.450000000000001e-06,
            "epoch": 2.55,
            "step": 1530
        },
        {
            "loss": 0.2163,
            "grad_norm": 11.946282386779785,
            "learning_rate": 7.433333333333334e-06,
            "epoch": 2.5666666666666664,
            "step": 1540
        },
        {
            "loss": 0.1317,
            "grad_norm": 20.331682205200195,
            "learning_rate": 7.416666666666668e-06,
            "epoch": 2.5833333333333335,
            "step": 1550
        },
        {
            "loss": 0.0338,
            "grad_norm": 8.718707084655762,
            "learning_rate": 7.4e-06,
            "epoch": 2.6,
            "step": 1560
        },
        {
            "loss": 0.049,
            "grad_norm": 0.1495114415884018,
            "learning_rate": 7.3833333333333335e-06,
            "epoch": 2.6166666666666667,
            "step": 1570
        },
        {
            "loss": 0.085,
            "grad_norm": 5.646066665649414,
            "learning_rate": 7.3666666666666676e-06,
            "epoch": 2.6333333333333333,
            "step": 1580
        },
        {
            "loss": 0.1564,
            "grad_norm": 0.0681656152009964,
            "learning_rate": 7.350000000000001e-06,
            "epoch": 2.65,
            "step": 1590
        },
        {
            "loss": 0.0998,
            "grad_norm": 0.03644033148884773,
            "learning_rate": 7.333333333333333e-06,
            "epoch": 2.6666666666666665,
            "step": 1600
        },
        {
            "loss": 0.0532,
            "grad_norm": 0.0136871337890625,
            "learning_rate": 7.316666666666667e-06,
            "epoch": 2.6833333333333336,
            "step": 1610
        },
        {
            "loss": 0.1493,
            "grad_norm": 19.947473526000977,
            "learning_rate": 7.3e-06,
            "epoch": 2.7,
            "step": 1620
        },
        {
            "loss": 0.079,
            "grad_norm": 0.056774791330099106,
            "learning_rate": 7.2833333333333345e-06,
            "epoch": 2.716666666666667,
            "step": 1630
        },
        {
            "loss": 0.2655,
            "grad_norm": 18.315744400024414,
            "learning_rate": 7.266666666666668e-06,
            "epoch": 2.7333333333333334,
            "step": 1640
        },
        {
            "loss": 0.1729,
            "grad_norm": 0.13296185433864594,
            "learning_rate": 7.25e-06,
            "epoch": 2.75,
            "step": 1650
        },
        {
            "loss": 0.1163,
            "grad_norm": 0.03958125784993172,
            "learning_rate": 7.233333333333334e-06,
            "epoch": 2.7666666666666666,
            "step": 1660
        },
        {
            "loss": 0.0973,
            "grad_norm": 16.653711318969727,
            "learning_rate": 7.216666666666667e-06,
            "epoch": 2.783333333333333,
            "step": 1670
        },
        {
            "loss": 0.0638,
            "grad_norm": 0.016337551176548004,
            "learning_rate": 7.2000000000000005e-06,
            "epoch": 2.8,
            "step": 1680
        },
        {
            "loss": 0.1071,
            "grad_norm": 1.8842747211456299,
            "learning_rate": 7.183333333333335e-06,
            "epoch": 2.8166666666666664,
            "step": 1690
        },
        {
            "loss": 0.1057,
            "grad_norm": 0.26965975761413574,
            "learning_rate": 7.166666666666667e-06,
            "epoch": 2.8333333333333335,
            "step": 1700
        },
        {
            "loss": 0.0269,
            "grad_norm": 0.6526921391487122,
            "learning_rate": 7.15e-06,
            "epoch": 2.85,
            "step": 1710
        },
        {
            "loss": 0.0941,
            "grad_norm": 23.512937545776367,
            "learning_rate": 7.133333333333334e-06,
            "epoch": 2.8666666666666667,
            "step": 1720
        },
        {
            "loss": 0.2337,
            "grad_norm": 0.20643901824951172,
            "learning_rate": 7.116666666666667e-06,
            "epoch": 2.8833333333333333,
            "step": 1730
        },
        {
            "loss": 0.0787,
            "grad_norm": 0.04006657749414444,
            "learning_rate": 7.100000000000001e-06,
            "epoch": 2.9,
            "step": 1740
        },
        {
            "loss": 0.1513,
            "grad_norm": 7.45322322845459,
            "learning_rate": 7.083333333333335e-06,
            "epoch": 2.9166666666666665,
            "step": 1750
        },
        {
            "loss": 0.0473,
            "grad_norm": 0.08294875919818878,
            "learning_rate": 7.066666666666667e-06,
            "epoch": 2.9333333333333336,
            "step": 1760
        },
        {
            "loss": 0.1829,
            "grad_norm": 0.012164446525275707,
            "learning_rate": 7.05e-06,
            "epoch": 2.95,
            "step": 1770
        },
        {
            "loss": 0.2258,
            "grad_norm": 0.07945564389228821,
            "learning_rate": 7.033333333333334e-06,
            "epoch": 2.966666666666667,
            "step": 1780
        },
        {
            "loss": 0.0916,
            "grad_norm": 17.000839233398438,
            "learning_rate": 7.0166666666666675e-06,
            "epoch": 2.9833333333333334,
            "step": 1790
        },
        {
            "loss": 0.0829,
            "grad_norm": 23.072063446044922,
            "learning_rate": 7e-06,
            "epoch": 3.0,
            "step": 1800
        },
        {
            "eval_loss": 0.19421949982643127,
            "eval_accuracy": 0.9508333333333333,
            "eval_precision": 0.9528124286678668,
            "eval_recall": 0.9508333333333333,
            "eval_f1": 0.9508081489251905,
            "eval_runtime": 103.4126,
            "eval_samples_per_second": 11.604,
            "eval_steps_per_second": 1.451,
            "epoch": 3.0,
            "step": 1800
        },
        {
            "loss": 0.0492,
            "grad_norm": 0.03196703642606735,
            "learning_rate": 6.983333333333334e-06,
            "epoch": 3.0166666666666666,
            "step": 1810
        },
        {
            "loss": 0.0807,
            "grad_norm": 0.03125358000397682,
            "learning_rate": 6.966666666666667e-06,
            "epoch": 3.033333333333333,
            "step": 1820
        },
        {
            "loss": 0.0627,
            "grad_norm": 12.627906799316406,
            "learning_rate": 6.95e-06,
            "epoch": 3.05,
            "step": 1830
        },
        {
            "loss": 0.0724,
            "grad_norm": 0.3290760815143585,
            "learning_rate": 6.9333333333333344e-06,
            "epoch": 3.066666666666667,
            "step": 1840
        },
        {
            "loss": 0.1583,
            "grad_norm": 0.10323018580675125,
            "learning_rate": 6.916666666666667e-06,
            "epoch": 3.0833333333333335,
            "step": 1850
        },
        {
            "loss": 0.1019,
            "grad_norm": 10.466974258422852,
            "learning_rate": 6.9e-06,
            "epoch": 3.1,
            "step": 1860
        },
        {
            "loss": 0.1233,
            "grad_norm": 19.444358825683594,
            "learning_rate": 6.883333333333334e-06,
            "epoch": 3.1166666666666667,
            "step": 1870
        },
        {
            "loss": 0.1189,
            "grad_norm": 0.013515563681721687,
            "learning_rate": 6.866666666666667e-06,
            "epoch": 3.1333333333333333,
            "step": 1880
        },
        {
            "loss": 0.0764,
            "grad_norm": 19.516983032226562,
            "learning_rate": 6.850000000000001e-06,
            "epoch": 3.15,
            "step": 1890
        },
        {
            "loss": 0.0797,
            "grad_norm": 16.898143768310547,
            "learning_rate": 6.833333333333334e-06,
            "epoch": 3.1666666666666665,
            "step": 1900
        },
        {
            "loss": 0.1182,
            "grad_norm": 0.07221509516239166,
            "learning_rate": 6.816666666666667e-06,
            "epoch": 3.183333333333333,
            "step": 1910
        },
        {
            "loss": 0.1392,
            "grad_norm": 0.020846577361226082,
            "learning_rate": 6.800000000000001e-06,
            "epoch": 3.2,
            "step": 1920
        },
        {
            "loss": 0.0587,
            "grad_norm": 0.14138160645961761,
            "learning_rate": 6.783333333333334e-06,
            "epoch": 3.216666666666667,
            "step": 1930
        },
        {
            "loss": 0.0301,
            "grad_norm": 0.17331542074680328,
            "learning_rate": 6.7666666666666665e-06,
            "epoch": 3.2333333333333334,
            "step": 1940
        },
        {
            "loss": 0.0027,
            "grad_norm": 1.4678153991699219,
            "learning_rate": 6.750000000000001e-06,
            "epoch": 3.25,
            "step": 1950
        },
        {
            "loss": 0.1848,
            "grad_norm": 13.959856033325195,
            "learning_rate": 6.733333333333334e-06,
            "epoch": 3.2666666666666666,
            "step": 1960
        },
        {
            "loss": 0.0752,
            "grad_norm": 0.013378876261413097,
            "learning_rate": 6.716666666666667e-06,
            "epoch": 3.283333333333333,
            "step": 1970
        },
        {
            "loss": 0.1074,
            "grad_norm": 0.022636355832219124,
            "learning_rate": 6.700000000000001e-06,
            "epoch": 3.3,
            "step": 1980
        },
        {
            "loss": 0.1247,
            "grad_norm": 1.0400941371917725,
            "learning_rate": 6.683333333333334e-06,
            "epoch": 3.3166666666666664,
            "step": 1990
        },
        {
            "loss": 0.0203,
            "grad_norm": 1.153517723083496,
            "learning_rate": 6.666666666666667e-06,
            "epoch": 3.3333333333333335,
            "step": 2000
        },
        {
            "loss": 0.0323,
            "grad_norm": 0.052990905940532684,
            "learning_rate": 6.650000000000001e-06,
            "epoch": 3.35,
            "step": 2010
        },
        {
            "loss": 0.1065,
            "grad_norm": 0.05744972452521324,
            "learning_rate": 6.633333333333334e-06,
            "epoch": 3.3666666666666667,
            "step": 2020
        },
        {
            "loss": 0.0038,
            "grad_norm": 2.499812602996826,
            "learning_rate": 6.616666666666667e-06,
            "epoch": 3.3833333333333333,
            "step": 2030
        },
        {
            "loss": 0.089,
            "grad_norm": 14.286272048950195,
            "learning_rate": 6.600000000000001e-06,
            "epoch": 3.4,
            "step": 2040
        },
        {
            "loss": 0.21,
            "grad_norm": 0.05289841815829277,
            "learning_rate": 6.5833333333333335e-06,
            "epoch": 3.4166666666666665,
            "step": 2050
        },
        {
            "loss": 0.1481,
            "grad_norm": 0.01955900527536869,
            "learning_rate": 6.566666666666667e-06,
            "epoch": 3.4333333333333336,
            "step": 2060
        },
        {
            "loss": 0.1365,
            "grad_norm": 0.03188459575176239,
            "learning_rate": 6.550000000000001e-06,
            "epoch": 3.45,
            "step": 2070
        },
        {
            "loss": 0.0399,
            "grad_norm": 1.9026027917861938,
            "learning_rate": 6.533333333333334e-06,
            "epoch": 3.466666666666667,
            "step": 2080
        },
        {
            "loss": 0.1007,
            "grad_norm": 37.22414016723633,
            "learning_rate": 6.516666666666666e-06,
            "epoch": 3.4833333333333334,
            "step": 2090
        },
        {
            "loss": 0.1532,
            "grad_norm": 0.030096611008048058,
            "learning_rate": 6.5000000000000004e-06,
            "epoch": 3.5,
            "step": 2100
        },
        {
            "loss": 0.0391,
            "grad_norm": 0.0580308623611927,
            "learning_rate": 6.483333333333334e-06,
            "epoch": 3.5166666666666666,
            "step": 2110
        },
        {
            "loss": 0.0451,
            "grad_norm": 0.968156635761261,
            "learning_rate": 6.466666666666667e-06,
            "epoch": 3.533333333333333,
            "step": 2120
        },
        {
            "loss": 0.1685,
            "grad_norm": 0.010762601159512997,
            "learning_rate": 6.450000000000001e-06,
            "epoch": 3.55,
            "step": 2130
        },
        {
            "loss": 0.0446,
            "grad_norm": 19.948368072509766,
            "learning_rate": 6.433333333333333e-06,
            "epoch": 3.5666666666666664,
            "step": 2140
        },
        {
            "loss": 0.0053,
            "grad_norm": 0.09426188468933105,
            "learning_rate": 6.416666666666667e-06,
            "epoch": 3.5833333333333335,
            "step": 2150
        },
        {
            "loss": 0.0262,
            "grad_norm": 0.006699519231915474,
            "learning_rate": 6.4000000000000006e-06,
            "epoch": 3.6,
            "step": 2160
        },
        {
            "loss": 0.0605,
            "grad_norm": 0.031548261642456055,
            "learning_rate": 6.383333333333334e-06,
            "epoch": 3.6166666666666667,
            "step": 2170
        },
        {
            "loss": 0.0132,
            "grad_norm": 12.078657150268555,
            "learning_rate": 6.366666666666668e-06,
            "epoch": 3.6333333333333333,
            "step": 2180
        },
        {
            "loss": 0.2206,
            "grad_norm": 0.8363491892814636,
            "learning_rate": 6.35e-06,
            "epoch": 3.65,
            "step": 2190
        },
        {
            "loss": 0.0093,
            "grad_norm": 0.019873550161719322,
            "learning_rate": 6.333333333333333e-06,
            "epoch": 3.6666666666666665,
            "step": 2200
        },
        {
            "loss": 0.0178,
            "grad_norm": 0.014781535603106022,
            "learning_rate": 6.3166666666666675e-06,
            "epoch": 3.6833333333333336,
            "step": 2210
        },
        {
            "loss": 0.1023,
            "grad_norm": 0.020012591034173965,
            "learning_rate": 6.300000000000001e-06,
            "epoch": 3.7,
            "step": 2220
        },
        {
            "loss": 0.0606,
            "grad_norm": 0.13284888863563538,
            "learning_rate": 6.283333333333334e-06,
            "epoch": 3.716666666666667,
            "step": 2230
        },
        {
            "loss": 0.0015,
            "grad_norm": 0.05066511407494545,
            "learning_rate": 6.266666666666668e-06,
            "epoch": 3.7333333333333334,
            "step": 2240
        },
        {
            "loss": 0.0085,
            "grad_norm": 13.689215660095215,
            "learning_rate": 6.25e-06,
            "epoch": 3.75,
            "step": 2250
        },
        {
            "loss": 0.1115,
            "grad_norm": 28.910768508911133,
            "learning_rate": 6.2333333333333335e-06,
            "epoch": 3.7666666666666666,
            "step": 2260
        },
        {
            "loss": 0.0831,
            "grad_norm": 4.227603912353516,
            "learning_rate": 6.2166666666666676e-06,
            "epoch": 3.783333333333333,
            "step": 2270
        },
        {
            "loss": 0.1321,
            "grad_norm": 0.009190833196043968,
            "learning_rate": 6.200000000000001e-06,
            "epoch": 3.8,
            "step": 2280
        },
        {
            "loss": 0.0941,
            "grad_norm": 14.346056938171387,
            "learning_rate": 6.183333333333333e-06,
            "epoch": 3.8166666666666664,
            "step": 2290
        },
        {
            "loss": 0.2162,
            "grad_norm": 13.731152534484863,
            "learning_rate": 6.166666666666667e-06,
            "epoch": 3.8333333333333335,
            "step": 2300
        },
        {
            "loss": 0.0449,
            "grad_norm": 0.02442394569516182,
            "learning_rate": 6.15e-06,
            "epoch": 3.85,
            "step": 2310
        },
        {
            "loss": 0.0192,
            "grad_norm": 0.9901253581047058,
            "learning_rate": 6.133333333333334e-06,
            "epoch": 3.8666666666666667,
            "step": 2320
        },
        {
            "loss": 0.2413,
            "grad_norm": 0.20249412953853607,
            "learning_rate": 6.116666666666668e-06,
            "epoch": 3.8833333333333333,
            "step": 2330
        },
        {
            "loss": 0.0237,
            "grad_norm": 0.03606408089399338,
            "learning_rate": 6.1e-06,
            "epoch": 3.9,
            "step": 2340
        },
        {
            "loss": 0.0227,
            "grad_norm": 0.02673036977648735,
            "learning_rate": 6.083333333333333e-06,
            "epoch": 3.9166666666666665,
            "step": 2350
        },
        {
            "loss": 0.1456,
            "grad_norm": 0.02865658700466156,
            "learning_rate": 6.066666666666667e-06,
            "epoch": 3.9333333333333336,
            "step": 2360
        },
        {
            "loss": 0.0082,
            "grad_norm": 0.03732419013977051,
            "learning_rate": 6.0500000000000005e-06,
            "epoch": 3.95,
            "step": 2370
        },
        {
            "loss": 0.085,
            "grad_norm": 1.392074465751648,
            "learning_rate": 6.033333333333335e-06,
            "epoch": 3.966666666666667,
            "step": 2380
        },
        {
            "loss": 0.2397,
            "grad_norm": 0.016285652294754982,
            "learning_rate": 6.016666666666667e-06,
            "epoch": 3.9833333333333334,
            "step": 2390
        },
        {
            "loss": 0.0019,
            "grad_norm": 0.008975864388048649,
            "learning_rate": 6e-06,
            "epoch": 4.0,
            "step": 2400
        },
        {
            "eval_loss": 0.16484378278255463,
            "eval_accuracy": 0.96,
            "eval_precision": 0.9603746589967318,
            "eval_recall": 0.96,
            "eval_f1": 0.9600864559919994,
            "eval_runtime": 160.6219,
            "eval_samples_per_second": 7.471,
            "eval_steps_per_second": 0.934,
            "epoch": 4.0,
            "step": 2400
        },
        {
            "loss": 0.0677,
            "grad_norm": 0.6594200730323792,
            "learning_rate": 5.983333333333334e-06,
            "epoch": 4.016666666666667,
            "step": 2410
        },
        {
            "loss": 0.0342,
            "grad_norm": 19.78432846069336,
            "learning_rate": 5.966666666666667e-06,
            "epoch": 4.033333333333333,
            "step": 2420
        },
        {
            "loss": 0.067,
            "grad_norm": 21.91470718383789,
            "learning_rate": 5.950000000000001e-06,
            "epoch": 4.05,
            "step": 2430
        },
        {
            "loss": 0.129,
            "grad_norm": 0.08362503349781036,
            "learning_rate": 5.933333333333335e-06,
            "epoch": 4.066666666666666,
            "step": 2440
        },
        {
            "loss": 0.0682,
            "grad_norm": 0.057015445083379745,
            "learning_rate": 5.916666666666667e-06,
            "epoch": 4.083333333333333,
            "step": 2450
        },
        {
            "loss": 0.1782,
            "grad_norm": 32.870399475097656,
            "learning_rate": 5.9e-06,
            "epoch": 4.1,
            "step": 2460
        },
        {
            "loss": 0.0258,
            "grad_norm": 0.18004654347896576,
            "learning_rate": 5.883333333333334e-06,
            "epoch": 4.116666666666666,
            "step": 2470
        },
        {
            "loss": 0.0569,
            "grad_norm": 0.13676391541957855,
            "learning_rate": 5.8666666666666675e-06,
            "epoch": 4.133333333333334,
            "step": 2480
        },
        {
            "loss": 0.0762,
            "grad_norm": 0.02609882690012455,
            "learning_rate": 5.85e-06,
            "epoch": 4.15,
            "step": 2490
        },
        {
            "loss": 0.0353,
            "grad_norm": 0.019783848896622658,
            "learning_rate": 5.833333333333334e-06,
            "epoch": 4.166666666666667,
            "step": 2500
        },
        {
            "loss": 0.0657,
            "grad_norm": 0.06875307112932205,
            "learning_rate": 5.816666666666667e-06,
            "epoch": 4.183333333333334,
            "step": 2510
        },
        {
            "loss": 0.0707,
            "grad_norm": 1.4360065460205078,
            "learning_rate": 5.8e-06,
            "epoch": 4.2,
            "step": 2520
        },
        {
            "loss": 0.0147,
            "grad_norm": 0.17649239301681519,
            "learning_rate": 5.7833333333333344e-06,
            "epoch": 4.216666666666667,
            "step": 2530
        },
        {
            "loss": 0.0353,
            "grad_norm": 0.0134014543145895,
            "learning_rate": 5.766666666666667e-06,
            "epoch": 4.233333333333333,
            "step": 2540
        },
        {
            "loss": 0.1102,
            "grad_norm": 34.5535774230957,
            "learning_rate": 5.75e-06,
            "epoch": 4.25,
            "step": 2550
        },
        {
            "loss": 0.0089,
            "grad_norm": 0.023660272359848022,
            "learning_rate": 5.733333333333334e-06,
            "epoch": 4.266666666666667,
            "step": 2560
        },
        {
            "loss": 0.0633,
            "grad_norm": 20.492300033569336,
            "learning_rate": 5.716666666666667e-06,
            "epoch": 4.283333333333333,
            "step": 2570
        },
        {
            "loss": 0.1386,
            "grad_norm": 39.928428649902344,
            "learning_rate": 5.7e-06,
            "epoch": 4.3,
            "step": 2580
        },
        {
            "loss": 0.0603,
            "grad_norm": 0.1577814668416977,
            "learning_rate": 5.683333333333334e-06,
            "epoch": 4.316666666666666,
            "step": 2590
        },
        {
            "loss": 0.0089,
            "grad_norm": 0.010511072352528572,
            "learning_rate": 5.666666666666667e-06,
            "epoch": 4.333333333333333,
            "step": 2600
        },
        {
            "loss": 0.0166,
            "grad_norm": 10.556167602539062,
            "learning_rate": 5.65e-06,
            "epoch": 4.35,
            "step": 2610
        },
        {
            "loss": 0.0167,
            "grad_norm": 0.4205780327320099,
            "learning_rate": 5.633333333333334e-06,
            "epoch": 4.366666666666666,
            "step": 2620
        },
        {
            "loss": 0.0629,
            "grad_norm": 0.042771678417921066,
            "learning_rate": 5.6166666666666665e-06,
            "epoch": 4.383333333333334,
            "step": 2630
        },
        {
            "loss": 0.174,
            "grad_norm": 19.962825775146484,
            "learning_rate": 5.600000000000001e-06,
            "epoch": 4.4,
            "step": 2640
        },
        {
            "loss": 0.1488,
            "grad_norm": 0.024855228140950203,
            "learning_rate": 5.583333333333334e-06,
            "epoch": 4.416666666666667,
            "step": 2650
        },
        {
            "loss": 0.1591,
            "grad_norm": 10.858918190002441,
            "learning_rate": 5.566666666666667e-06,
            "epoch": 4.433333333333334,
            "step": 2660
        },
        {
            "loss": 0.0853,
            "grad_norm": 0.00907648354768753,
            "learning_rate": 5.550000000000001e-06,
            "epoch": 4.45,
            "step": 2670
        },
        {
            "loss": 0.0029,
            "grad_norm": 0.02317400649189949,
            "learning_rate": 5.533333333333334e-06,
            "epoch": 4.466666666666667,
            "step": 2680
        },
        {
            "loss": 0.1289,
            "grad_norm": 0.025136800482869148,
            "learning_rate": 5.516666666666667e-06,
            "epoch": 4.483333333333333,
            "step": 2690
        },
        {
            "loss": 0.0569,
            "grad_norm": 0.014798983931541443,
            "learning_rate": 5.500000000000001e-06,
            "epoch": 4.5,
            "step": 2700
        },
        {
            "loss": 0.199,
            "grad_norm": 12.942214012145996,
            "learning_rate": 5.483333333333334e-06,
            "epoch": 4.516666666666667,
            "step": 2710
        },
        {
            "loss": 0.0111,
            "grad_norm": 0.4718112349510193,
            "learning_rate": 5.466666666666667e-06,
            "epoch": 4.533333333333333,
            "step": 2720
        },
        {
            "loss": 0.0096,
            "grad_norm": 0.8849044442176819,
            "learning_rate": 5.450000000000001e-06,
            "epoch": 4.55,
            "step": 2730
        },
        {
            "loss": 0.1345,
            "grad_norm": 9.144948959350586,
            "learning_rate": 5.4333333333333335e-06,
            "epoch": 4.566666666666666,
            "step": 2740
        },
        {
            "loss": 0.0337,
            "grad_norm": 0.010049929842352867,
            "learning_rate": 5.416666666666667e-06,
            "epoch": 4.583333333333333,
            "step": 2750
        },
        {
            "loss": 0.0919,
            "grad_norm": 0.011057051829993725,
            "learning_rate": 5.400000000000001e-06,
            "epoch": 4.6,
            "step": 2760
        },
        {
            "loss": 0.0062,
            "grad_norm": 0.02227642387151718,
            "learning_rate": 5.383333333333334e-06,
            "epoch": 4.616666666666667,
            "step": 2770
        },
        {
            "loss": 0.001,
            "grad_norm": 0.05070340260863304,
            "learning_rate": 5.366666666666666e-06,
            "epoch": 4.633333333333333,
            "step": 2780
        },
        {
            "loss": 0.071,
            "grad_norm": 0.00865051057189703,
            "learning_rate": 5.3500000000000004e-06,
            "epoch": 4.65,
            "step": 2790
        },
        {
            "loss": 0.1005,
            "grad_norm": 0.0055068922229111195,
            "learning_rate": 5.333333333333334e-06,
            "epoch": 4.666666666666667,
            "step": 2800
        },
        {
            "loss": 0.2018,
            "grad_norm": 11.531192779541016,
            "learning_rate": 5.316666666666667e-06,
            "epoch": 4.683333333333334,
            "step": 2810
        },
        {
            "loss": 0.0656,
            "grad_norm": 20.159542083740234,
            "learning_rate": 5.300000000000001e-06,
            "epoch": 4.7,
            "step": 2820
        },
        {
            "loss": 0.012,
            "grad_norm": 0.01661159284412861,
            "learning_rate": 5.283333333333333e-06,
            "epoch": 4.716666666666667,
            "step": 2830
        },
        {
            "loss": 0.1223,
            "grad_norm": 0.04721270874142647,
            "learning_rate": 5.2666666666666665e-06,
            "epoch": 4.733333333333333,
            "step": 2840
        },
        {
            "loss": 0.01,
            "grad_norm": 0.04277130216360092,
            "learning_rate": 5.2500000000000006e-06,
            "epoch": 4.75,
            "step": 2850
        },
        {
            "loss": 0.0541,
            "grad_norm": 18.825061798095703,
            "learning_rate": 5.233333333333334e-06,
            "epoch": 4.766666666666667,
            "step": 2860
        },
        {
            "loss": 0.0421,
            "grad_norm": 0.036958690732717514,
            "learning_rate": 5.216666666666666e-06,
            "epoch": 4.783333333333333,
            "step": 2870
        },
        {
            "loss": 0.0935,
            "grad_norm": 30.933305740356445,
            "learning_rate": 5.2e-06,
            "epoch": 4.8,
            "step": 2880
        },
        {
            "loss": 0.074,
            "grad_norm": 0.05454830080270767,
            "learning_rate": 5.183333333333333e-06,
            "epoch": 4.816666666666666,
            "step": 2890
        },
        {
            "loss": 0.0925,
            "grad_norm": 1.0492422580718994,
            "learning_rate": 5.1666666666666675e-06,
            "epoch": 4.833333333333333,
            "step": 2900
        },
        {
            "loss": 0.1471,
            "grad_norm": 0.009859845042228699,
            "learning_rate": 5.150000000000001e-06,
            "epoch": 4.85,
            "step": 2910
        },
        {
            "loss": 0.0322,
            "grad_norm": 3.5742688179016113,
            "learning_rate": 5.133333333333334e-06,
            "epoch": 4.866666666666667,
            "step": 2920
        },
        {
            "loss": 0.0037,
            "grad_norm": 0.009062101133167744,
            "learning_rate": 5.116666666666668e-06,
            "epoch": 4.883333333333333,
            "step": 2930
        },
        {
            "loss": 0.2621,
            "grad_norm": 23.666898727416992,
            "learning_rate": 5.1e-06,
            "epoch": 4.9,
            "step": 2940
        },
        {
            "loss": 0.1484,
            "grad_norm": 0.2511524260044098,
            "learning_rate": 5.0833333333333335e-06,
            "epoch": 4.916666666666667,
            "step": 2950
        },
        {
            "loss": 0.0411,
            "grad_norm": 0.008023008704185486,
            "learning_rate": 5.0666666666666676e-06,
            "epoch": 4.933333333333334,
            "step": 2960
        },
        {
            "loss": 0.1718,
            "grad_norm": 3.6417887210845947,
            "learning_rate": 5.050000000000001e-06,
            "epoch": 4.95,
            "step": 2970
        },
        {
            "loss": 0.0357,
            "grad_norm": 1.6662603616714478,
            "learning_rate": 5.033333333333333e-06,
            "epoch": 4.966666666666667,
            "step": 2980
        },
        {
            "loss": 0.0017,
            "grad_norm": 0.012926564551889896,
            "learning_rate": 5.016666666666667e-06,
            "epoch": 4.983333333333333,
            "step": 2990
        },
        {
            "loss": 0.0256,
            "grad_norm": 0.021769607439637184,
            "learning_rate": 5e-06,
            "epoch": 5.0,
            "step": 3000
        },
        {
            "eval_loss": 0.1915881633758545,
            "eval_accuracy": 0.96,
            "eval_precision": 0.960512254974835,
            "eval_recall": 0.96,
            "eval_f1": 0.9599835233742198,
            "eval_runtime": 142.1101,
            "eval_samples_per_second": 8.444,
            "eval_steps_per_second": 1.056,
            "epoch": 5.0,
            "step": 3000
        },
        {
            "loss": 0.0964,
            "grad_norm": 0.006624800618737936,
            "learning_rate": 4.983333333333334e-06,
            "epoch": 5.016666666666667,
            "step": 3010
        },
        {
            "loss": 0.2828,
            "grad_norm": 14.24642562866211,
            "learning_rate": 4.966666666666667e-06,
            "epoch": 5.033333333333333,
            "step": 3020
        },
        {
            "loss": 0.0419,
            "grad_norm": 0.009177783504128456,
            "learning_rate": 4.95e-06,
            "epoch": 5.05,
            "step": 3030
        },
        {
            "loss": 0.0953,
            "grad_norm": 0.03956599161028862,
            "learning_rate": 4.933333333333334e-06,
            "epoch": 5.066666666666666,
            "step": 3040
        },
        {
            "loss": 0.116,
            "grad_norm": 2.7014083862304688,
            "learning_rate": 4.9166666666666665e-06,
            "epoch": 5.083333333333333,
            "step": 3050
        },
        {
            "loss": 0.0571,
            "grad_norm": 7.009766578674316,
            "learning_rate": 4.9000000000000005e-06,
            "epoch": 5.1,
            "step": 3060
        },
        {
            "loss": 0.0213,
            "grad_norm": 0.09398648142814636,
            "learning_rate": 4.883333333333334e-06,
            "epoch": 5.116666666666666,
            "step": 3070
        },
        {
            "loss": 0.062,
            "grad_norm": 0.43188565969467163,
            "learning_rate": 4.866666666666667e-06,
            "epoch": 5.133333333333334,
            "step": 3080
        },
        {
            "loss": 0.0123,
            "grad_norm": 0.030010290443897247,
            "learning_rate": 4.85e-06,
            "epoch": 5.15,
            "step": 3090
        },
        {
            "loss": 0.107,
            "grad_norm": 0.009097707457840443,
            "learning_rate": 4.833333333333333e-06,
            "epoch": 5.166666666666667,
            "step": 3100
        },
        {
            "loss": 0.0742,
            "grad_norm": 0.054910533130168915,
            "learning_rate": 4.816666666666667e-06,
            "epoch": 5.183333333333334,
            "step": 3110
        },
        {
            "loss": 0.0594,
            "grad_norm": 0.023775767534971237,
            "learning_rate": 4.800000000000001e-06,
            "epoch": 5.2,
            "step": 3120
        },
        {
            "loss": 0.0443,
            "grad_norm": 11.464689254760742,
            "learning_rate": 4.783333333333334e-06,
            "epoch": 5.216666666666667,
            "step": 3130
        },
        {
            "loss": 0.0358,
            "grad_norm": 0.01072134543210268,
            "learning_rate": 4.766666666666667e-06,
            "epoch": 5.233333333333333,
            "step": 3140
        },
        {
            "loss": 0.0263,
            "grad_norm": 0.1430715173482895,
            "learning_rate": 4.75e-06,
            "epoch": 5.25,
            "step": 3150
        },
        {
            "loss": 0.0018,
            "grad_norm": 0.048500142991542816,
            "learning_rate": 4.7333333333333335e-06,
            "epoch": 5.266666666666667,
            "step": 3160
        },
        {
            "loss": 0.038,
            "grad_norm": 0.013155069202184677,
            "learning_rate": 4.7166666666666675e-06,
            "epoch": 5.283333333333333,
            "step": 3170
        },
        {
            "loss": 0.0811,
            "grad_norm": 0.0073976051062345505,
            "learning_rate": 4.7e-06,
            "epoch": 5.3,
            "step": 3180
        },
        {
            "loss": 0.0013,
            "grad_norm": 0.011133018881082535,
            "learning_rate": 4.683333333333334e-06,
            "epoch": 5.316666666666666,
            "step": 3190
        },
        {
            "loss": 0.0587,
            "grad_norm": 0.005375103559345007,
            "learning_rate": 4.666666666666667e-06,
            "epoch": 5.333333333333333,
            "step": 3200
        },
        {
            "loss": 0.0131,
            "grad_norm": 0.006852058228105307,
            "learning_rate": 4.65e-06,
            "epoch": 5.35,
            "step": 3210
        },
        {
            "loss": 0.0286,
            "grad_norm": 0.08053284883499146,
            "learning_rate": 4.633333333333334e-06,
            "epoch": 5.366666666666666,
            "step": 3220
        },
        {
            "loss": 0.0674,
            "grad_norm": 0.010949607007205486,
            "learning_rate": 4.616666666666667e-06,
            "epoch": 5.383333333333334,
            "step": 3230
        },
        {
            "loss": 0.0014,
            "grad_norm": 0.0214704442769289,
            "learning_rate": 4.600000000000001e-06,
            "epoch": 5.4,
            "step": 3240
        },
        {
            "loss": 0.0247,
            "grad_norm": 0.004153249319642782,
            "learning_rate": 4.583333333333333e-06,
            "epoch": 5.416666666666667,
            "step": 3250
        },
        {
            "loss": 0.0423,
            "grad_norm": 0.026531897485256195,
            "learning_rate": 4.566666666666667e-06,
            "epoch": 5.433333333333334,
            "step": 3260
        },
        {
            "loss": 0.0007,
            "grad_norm": 0.017278306186199188,
            "learning_rate": 4.5500000000000005e-06,
            "epoch": 5.45,
            "step": 3270
        },
        {
            "loss": 0.0694,
            "grad_norm": 0.008887629956007004,
            "learning_rate": 4.533333333333334e-06,
            "epoch": 5.466666666666667,
            "step": 3280
        },
        {
            "loss": 0.0393,
            "grad_norm": 0.017349030822515488,
            "learning_rate": 4.516666666666667e-06,
            "epoch": 5.483333333333333,
            "step": 3290
        },
        {
            "loss": 0.1008,
            "grad_norm": 0.6203066110610962,
            "learning_rate": 4.5e-06,
            "epoch": 5.5,
            "step": 3300
        },
        {
            "loss": 0.0814,
            "grad_norm": 31.247520446777344,
            "learning_rate": 4.483333333333333e-06,
            "epoch": 5.516666666666667,
            "step": 3310
        },
        {
            "loss": 0.2019,
            "grad_norm": 0.025825461372733116,
            "learning_rate": 4.4666666666666665e-06,
            "epoch": 5.533333333333333,
            "step": 3320
        },
        {
            "loss": 0.0093,
            "grad_norm": 0.0145300617441535,
            "learning_rate": 4.450000000000001e-06,
            "epoch": 5.55,
            "step": 3330
        },
        {
            "loss": 0.012,
            "grad_norm": 0.019517559558153152,
            "learning_rate": 4.433333333333334e-06,
            "epoch": 5.566666666666666,
            "step": 3340
        },
        {
            "loss": 0.0441,
            "grad_norm": 0.7964878082275391,
            "learning_rate": 4.416666666666667e-06,
            "epoch": 5.583333333333333,
            "step": 3350
        },
        {
            "loss": 0.0046,
            "grad_norm": 0.0201504398137331,
            "learning_rate": 4.4e-06,
            "epoch": 5.6,
            "step": 3360
        },
        {
            "loss": 0.0348,
            "grad_norm": 0.7039677500724792,
            "learning_rate": 4.383333333333334e-06,
            "epoch": 5.616666666666667,
            "step": 3370
        },
        {
            "loss": 0.0464,
            "grad_norm": 0.005109158344566822,
            "learning_rate": 4.366666666666667e-06,
            "epoch": 5.633333333333333,
            "step": 3380
        },
        {
            "loss": 0.0736,
            "grad_norm": 12.814813613891602,
            "learning_rate": 4.350000000000001e-06,
            "epoch": 5.65,
            "step": 3390
        },
        {
            "loss": 0.0951,
            "grad_norm": 0.2617665231227875,
            "learning_rate": 4.333333333333334e-06,
            "epoch": 5.666666666666667,
            "step": 3400
        },
        {
            "loss": 0.0054,
            "grad_norm": 0.012656161561608315,
            "learning_rate": 4.316666666666667e-06,
            "epoch": 5.683333333333334,
            "step": 3410
        },
        {
            "loss": 0.0171,
            "grad_norm": 0.006122475489974022,
            "learning_rate": 4.3e-06,
            "epoch": 5.7,
            "step": 3420
        },
        {
            "loss": 0.0327,
            "grad_norm": 21.19595718383789,
            "learning_rate": 4.2833333333333335e-06,
            "epoch": 5.716666666666667,
            "step": 3430
        },
        {
            "loss": 0.0012,
            "grad_norm": 0.01628839410841465,
            "learning_rate": 4.266666666666668e-06,
            "epoch": 5.733333333333333,
            "step": 3440
        },
        {
            "loss": 0.0271,
            "grad_norm": 0.05519164353609085,
            "learning_rate": 4.25e-06,
            "epoch": 5.75,
            "step": 3450
        },
        {
            "loss": 0.0064,
            "grad_norm": 0.019639983773231506,
            "learning_rate": 4.233333333333334e-06,
            "epoch": 5.766666666666667,
            "step": 3460
        },
        {
            "loss": 0.0082,
            "grad_norm": 0.011120875366032124,
            "learning_rate": 4.216666666666667e-06,
            "epoch": 5.783333333333333,
            "step": 3470
        },
        {
            "loss": 0.1158,
            "grad_norm": 0.10487066954374313,
            "learning_rate": 4.2000000000000004e-06,
            "epoch": 5.8,
            "step": 3480
        },
        {
            "loss": 0.0232,
            "grad_norm": 0.021882064640522003,
            "learning_rate": 4.183333333333334e-06,
            "epoch": 5.816666666666666,
            "step": 3490
        },
        {
            "loss": 0.2392,
            "grad_norm": 14.338157653808594,
            "learning_rate": 4.166666666666667e-06,
            "epoch": 5.833333333333333,
            "step": 3500
        },
        {
            "loss": 0.0019,
            "grad_norm": 0.07372688502073288,
            "learning_rate": 4.15e-06,
            "epoch": 5.85,
            "step": 3510
        },
        {
            "loss": 0.172,
            "grad_norm": 0.5102858543395996,
            "learning_rate": 4.133333333333333e-06,
            "epoch": 5.866666666666667,
            "step": 3520
        },
        {
            "loss": 0.0284,
            "grad_norm": 0.013046861626207829,
            "learning_rate": 4.116666666666667e-06,
            "epoch": 5.883333333333333,
            "step": 3530
        },
        {
            "loss": 0.1336,
            "grad_norm": 0.08388765156269073,
            "learning_rate": 4.1e-06,
            "epoch": 5.9,
            "step": 3540
        },
        {
            "loss": 0.001,
            "grad_norm": 0.017893098294734955,
            "learning_rate": 4.083333333333334e-06,
            "epoch": 5.916666666666667,
            "step": 3550
        },
        {
            "loss": 0.2366,
            "grad_norm": 0.13546790182590485,
            "learning_rate": 4.066666666666667e-06,
            "epoch": 5.933333333333334,
            "step": 3560
        },
        {
            "loss": 0.0286,
            "grad_norm": 0.007654874585568905,
            "learning_rate": 4.05e-06,
            "epoch": 5.95,
            "step": 3570
        },
        {
            "loss": 0.0479,
            "grad_norm": 0.03101418912410736,
            "learning_rate": 4.033333333333333e-06,
            "epoch": 5.966666666666667,
            "step": 3580
        },
        {
            "loss": 0.0298,
            "grad_norm": 0.013322200626134872,
            "learning_rate": 4.0166666666666675e-06,
            "epoch": 5.983333333333333,
            "step": 3590
        },
        {
            "loss": 0.0846,
            "grad_norm": 0.009396607987582684,
            "learning_rate": 4.000000000000001e-06,
            "epoch": 6.0,
            "step": 3600
        },
        {
            "eval_loss": 0.18058153986930847,
            "eval_accuracy": 0.9625,
            "eval_precision": 0.962582374256888,
            "eval_recall": 0.9625,
            "eval_f1": 0.9624842413894888,
            "eval_runtime": 109.4419,
            "eval_samples_per_second": 10.965,
            "eval_steps_per_second": 1.371,
            "epoch": 6.0,
            "step": 3600
        },
        {
            "loss": 0.0018,
            "grad_norm": 0.034638844430446625,
            "learning_rate": 3.983333333333334e-06,
            "epoch": 6.016666666666667,
            "step": 3610
        },
        {
            "loss": 0.0038,
            "grad_norm": 0.004822689574211836,
            "learning_rate": 3.966666666666667e-06,
            "epoch": 6.033333333333333,
            "step": 3620
        },
        {
            "loss": 0.0333,
            "grad_norm": 13.114704132080078,
            "learning_rate": 3.95e-06,
            "epoch": 6.05,
            "step": 3630
        },
        {
            "loss": 0.0658,
            "grad_norm": 0.005064257886260748,
            "learning_rate": 3.9333333333333335e-06,
            "epoch": 6.066666666666666,
            "step": 3640
        },
        {
            "loss": 0.0085,
            "grad_norm": 0.1264328807592392,
            "learning_rate": 3.916666666666667e-06,
            "epoch": 6.083333333333333,
            "step": 3650
        },
        {
            "loss": 0.0005,
            "grad_norm": 0.07048264145851135,
            "learning_rate": 3.900000000000001e-06,
            "epoch": 6.1,
            "step": 3660
        },
        {
            "loss": 0.0532,
            "grad_norm": 0.08809787034988403,
            "learning_rate": 3.883333333333333e-06,
            "epoch": 6.116666666666666,
            "step": 3670
        },
        {
            "loss": 0.0069,
            "grad_norm": 6.114525318145752,
            "learning_rate": 3.866666666666667e-06,
            "epoch": 6.133333333333334,
            "step": 3680
        },
        {
            "loss": 0.0395,
            "grad_norm": 0.049816496670246124,
            "learning_rate": 3.85e-06,
            "epoch": 6.15,
            "step": 3690
        },
        {
            "loss": 0.0005,
            "grad_norm": 0.02087191864848137,
            "learning_rate": 3.833333333333334e-06,
            "epoch": 6.166666666666667,
            "step": 3700
        },
        {
            "loss": 0.1471,
            "grad_norm": 15.820014953613281,
            "learning_rate": 3.816666666666667e-06,
            "epoch": 6.183333333333334,
            "step": 3710
        },
        {
            "loss": 0.105,
            "grad_norm": 24.113832473754883,
            "learning_rate": 3.8000000000000005e-06,
            "epoch": 6.2,
            "step": 3720
        },
        {
            "loss": 0.1173,
            "grad_norm": 0.007964209653437138,
            "learning_rate": 3.7833333333333337e-06,
            "epoch": 6.216666666666667,
            "step": 3730
        },
        {
            "loss": 0.0144,
            "grad_norm": 0.006303997710347176,
            "learning_rate": 3.766666666666667e-06,
            "epoch": 6.233333333333333,
            "step": 3740
        },
        {
            "loss": 0.0494,
            "grad_norm": 0.5903245806694031,
            "learning_rate": 3.7500000000000005e-06,
            "epoch": 6.25,
            "step": 3750
        },
        {
            "loss": 0.0635,
            "grad_norm": 0.12321049720048904,
            "learning_rate": 3.7333333333333337e-06,
            "epoch": 6.266666666666667,
            "step": 3760
        },
        {
            "loss": 0.0763,
            "grad_norm": 0.01961970515549183,
            "learning_rate": 3.716666666666667e-06,
            "epoch": 6.283333333333333,
            "step": 3770
        },
        {
            "loss": 0.0213,
            "grad_norm": 0.004006362985819578,
            "learning_rate": 3.7e-06,
            "epoch": 6.3,
            "step": 3780
        },
        {
            "loss": 0.0007,
            "grad_norm": 0.10601496696472168,
            "learning_rate": 3.6833333333333338e-06,
            "epoch": 6.316666666666666,
            "step": 3790
        },
        {
            "loss": 0.0015,
            "grad_norm": 0.03641341254115105,
            "learning_rate": 3.6666666666666666e-06,
            "epoch": 6.333333333333333,
            "step": 3800
        },
        {
            "loss": 0.0146,
            "grad_norm": 14.838274955749512,
            "learning_rate": 3.65e-06,
            "epoch": 6.35,
            "step": 3810
        },
        {
            "loss": 0.0683,
            "grad_norm": 0.009937960654497147,
            "learning_rate": 3.633333333333334e-06,
            "epoch": 6.366666666666666,
            "step": 3820
        },
        {
            "loss": 0.0468,
            "grad_norm": 32.29153823852539,
            "learning_rate": 3.616666666666667e-06,
            "epoch": 6.383333333333334,
            "step": 3830
        },
        {
            "loss": 0.0611,
            "grad_norm": 0.006135054863989353,
            "learning_rate": 3.6000000000000003e-06,
            "epoch": 6.4,
            "step": 3840
        },
        {
            "loss": 0.0439,
            "grad_norm": 0.013954752124845982,
            "learning_rate": 3.5833333333333335e-06,
            "epoch": 6.416666666666667,
            "step": 3850
        },
        {
            "loss": 0.0824,
            "grad_norm": 0.029508374631404877,
            "learning_rate": 3.566666666666667e-06,
            "epoch": 6.433333333333334,
            "step": 3860
        },
        {
            "loss": 0.0677,
            "grad_norm": 0.19814734160900116,
            "learning_rate": 3.5500000000000003e-06,
            "epoch": 6.45,
            "step": 3870
        },
        {
            "loss": 0.003,
            "grad_norm": 3.9102108478546143,
            "learning_rate": 3.5333333333333335e-06,
            "epoch": 6.466666666666667,
            "step": 3880
        },
        {
            "loss": 0.0006,
            "grad_norm": 0.5854262113571167,
            "learning_rate": 3.516666666666667e-06,
            "epoch": 6.483333333333333,
            "step": 3890
        },
        {
            "loss": 0.0827,
            "grad_norm": 0.006349007599055767,
            "learning_rate": 3.5e-06,
            "epoch": 6.5,
            "step": 3900
        },
        {
            "loss": 0.1607,
            "grad_norm": 0.0376855731010437,
            "learning_rate": 3.4833333333333336e-06,
            "epoch": 6.516666666666667,
            "step": 3910
        },
        {
            "loss": 0.0529,
            "grad_norm": 0.014390375465154648,
            "learning_rate": 3.4666666666666672e-06,
            "epoch": 6.533333333333333,
            "step": 3920
        },
        {
            "loss": 0.0638,
            "grad_norm": 0.015898315235972404,
            "learning_rate": 3.45e-06,
            "epoch": 6.55,
            "step": 3930
        },
        {
            "loss": 0.1176,
            "grad_norm": 0.230859637260437,
            "learning_rate": 3.4333333333333336e-06,
            "epoch": 6.566666666666666,
            "step": 3940
        },
        {
            "loss": 0.1206,
            "grad_norm": 15.637592315673828,
            "learning_rate": 3.416666666666667e-06,
            "epoch": 6.583333333333333,
            "step": 3950
        },
        {
            "loss": 0.0006,
            "grad_norm": 0.005451935809105635,
            "learning_rate": 3.4000000000000005e-06,
            "epoch": 6.6,
            "step": 3960
        },
        {
            "loss": 0.0013,
            "grad_norm": 0.011794811114668846,
            "learning_rate": 3.3833333333333333e-06,
            "epoch": 6.616666666666667,
            "step": 3970
        },
        {
            "loss": 0.0642,
            "grad_norm": 0.011774447746574879,
            "learning_rate": 3.366666666666667e-06,
            "epoch": 6.633333333333333,
            "step": 3980
        },
        {
            "loss": 0.0843,
            "grad_norm": 0.06673814356327057,
            "learning_rate": 3.3500000000000005e-06,
            "epoch": 6.65,
            "step": 3990
        },
        {
            "loss": 0.0167,
            "grad_norm": 0.004598062951117754,
            "learning_rate": 3.3333333333333333e-06,
            "epoch": 6.666666666666667,
            "step": 4000
        },
        {
            "loss": 0.0144,
            "grad_norm": 0.005785166285932064,
            "learning_rate": 3.316666666666667e-06,
            "epoch": 6.683333333333334,
            "step": 4010
        },
        {
            "loss": 0.0451,
            "grad_norm": 1.1933540105819702,
            "learning_rate": 3.3000000000000006e-06,
            "epoch": 6.7,
            "step": 4020
        },
        {
            "loss": 0.0756,
            "grad_norm": 0.05653655529022217,
            "learning_rate": 3.2833333333333334e-06,
            "epoch": 6.716666666666667,
            "step": 4030
        },
        {
            "loss": 0.127,
            "grad_norm": 0.28034234046936035,
            "learning_rate": 3.266666666666667e-06,
            "epoch": 6.733333333333333,
            "step": 4040
        },
        {
            "loss": 0.0266,
            "grad_norm": 0.015461551025509834,
            "learning_rate": 3.2500000000000002e-06,
            "epoch": 6.75,
            "step": 4050
        },
        {
            "loss": 0.0979,
            "grad_norm": 4.114436626434326,
            "learning_rate": 3.2333333333333334e-06,
            "epoch": 6.766666666666667,
            "step": 4060
        },
        {
            "loss": 0.2377,
            "grad_norm": 57.312713623046875,
            "learning_rate": 3.2166666666666666e-06,
            "epoch": 6.783333333333333,
            "step": 4070
        },
        {
            "loss": 0.0485,
            "grad_norm": 10.571235656738281,
            "learning_rate": 3.2000000000000003e-06,
            "epoch": 6.8,
            "step": 4080
        },
        {
            "loss": 0.0737,
            "grad_norm": 0.014146332629024982,
            "learning_rate": 3.183333333333334e-06,
            "epoch": 6.816666666666666,
            "step": 4090
        },
        {
            "loss": 0.0952,
            "grad_norm": 0.08703987300395966,
            "learning_rate": 3.1666666666666667e-06,
            "epoch": 6.833333333333333,
            "step": 4100
        },
        {
            "loss": 0.1315,
            "grad_norm": 24.639896392822266,
            "learning_rate": 3.1500000000000003e-06,
            "epoch": 6.85,
            "step": 4110
        },
        {
            "loss": 0.0477,
            "grad_norm": 0.033724814653396606,
            "learning_rate": 3.133333333333334e-06,
            "epoch": 6.866666666666667,
            "step": 4120
        },
        {
            "loss": 0.0422,
            "grad_norm": 0.009411323815584183,
            "learning_rate": 3.1166666666666668e-06,
            "epoch": 6.883333333333333,
            "step": 4130
        },
        {
            "loss": 0.0126,
            "grad_norm": 0.1444111168384552,
            "learning_rate": 3.1000000000000004e-06,
            "epoch": 6.9,
            "step": 4140
        },
        {
            "loss": 0.0163,
            "grad_norm": 0.03256047144532204,
            "learning_rate": 3.0833333333333336e-06,
            "epoch": 6.916666666666667,
            "step": 4150
        },
        {
            "loss": 0.0008,
            "grad_norm": 0.026030555367469788,
            "learning_rate": 3.066666666666667e-06,
            "epoch": 6.933333333333334,
            "step": 4160
        },
        {
            "loss": 0.0009,
            "grad_norm": 0.027741068974137306,
            "learning_rate": 3.05e-06,
            "epoch": 6.95,
            "step": 4170
        },
        {
            "loss": 0.0486,
            "grad_norm": 0.013728765770792961,
            "learning_rate": 3.0333333333333337e-06,
            "epoch": 6.966666666666667,
            "step": 4180
        },
        {
            "loss": 0.0009,
            "grad_norm": 0.00661741616204381,
            "learning_rate": 3.0166666666666673e-06,
            "epoch": 6.983333333333333,
            "step": 4190
        },
        {
            "loss": 0.0949,
            "grad_norm": 0.1785765439271927,
            "learning_rate": 3e-06,
            "epoch": 7.0,
            "step": 4200
        },
        {
            "eval_loss": 0.1734713464975357,
            "eval_accuracy": 0.965,
            "eval_precision": 0.9651799642114475,
            "eval_recall": 0.965,
            "eval_f1": 0.9650276113373538,
            "eval_runtime": 88.2283,
            "eval_samples_per_second": 13.601,
            "eval_steps_per_second": 1.7,
            "epoch": 7.0,
            "step": 4200
        },
        {
            "loss": 0.0337,
            "grad_norm": 0.25411126017570496,
            "learning_rate": 2.9833333333333337e-06,
            "epoch": 7.016666666666667,
            "step": 4210
        },
        {
            "loss": 0.066,
            "grad_norm": 7.450236797332764,
            "learning_rate": 2.9666666666666673e-06,
            "epoch": 7.033333333333333,
            "step": 4220
        },
        {
            "loss": 0.0014,
            "grad_norm": 0.06843936443328857,
            "learning_rate": 2.95e-06,
            "epoch": 7.05,
            "step": 4230
        },
        {
            "loss": 0.0841,
            "grad_norm": 0.06501881778240204,
            "learning_rate": 2.9333333333333338e-06,
            "epoch": 7.066666666666666,
            "step": 4240
        },
        {
            "loss": 0.0012,
            "grad_norm": 0.057700179517269135,
            "learning_rate": 2.916666666666667e-06,
            "epoch": 7.083333333333333,
            "step": 4250
        },
        {
            "loss": 0.098,
            "grad_norm": 0.054625384509563446,
            "learning_rate": 2.9e-06,
            "epoch": 7.1,
            "step": 4260
        },
        {
            "loss": 0.0431,
            "grad_norm": 0.0273280069231987,
            "learning_rate": 2.8833333333333334e-06,
            "epoch": 7.116666666666666,
            "step": 4270
        },
        {
            "loss": 0.1258,
            "grad_norm": 16.648239135742188,
            "learning_rate": 2.866666666666667e-06,
            "epoch": 7.133333333333334,
            "step": 4280
        },
        {
            "loss": 0.082,
            "grad_norm": 0.005416599102318287,
            "learning_rate": 2.85e-06,
            "epoch": 7.15,
            "step": 4290
        },
        {
            "loss": 0.0394,
            "grad_norm": 0.012101455591619015,
            "learning_rate": 2.8333333333333335e-06,
            "epoch": 7.166666666666667,
            "step": 4300
        },
        {
            "loss": 0.0148,
            "grad_norm": 0.027157222852110863,
            "learning_rate": 2.816666666666667e-06,
            "epoch": 7.183333333333334,
            "step": 4310
        },
        {
            "loss": 0.0938,
            "grad_norm": 0.022502629086375237,
            "learning_rate": 2.8000000000000003e-06,
            "epoch": 7.2,
            "step": 4320
        },
        {
            "loss": 0.0881,
            "grad_norm": 0.006631501950323582,
            "learning_rate": 2.7833333333333335e-06,
            "epoch": 7.216666666666667,
            "step": 4330
        },
        {
            "loss": 0.0515,
            "grad_norm": 0.057890214025974274,
            "learning_rate": 2.766666666666667e-06,
            "epoch": 7.233333333333333,
            "step": 4340
        },
        {
            "loss": 0.0388,
            "grad_norm": 5.64212703704834,
            "learning_rate": 2.7500000000000004e-06,
            "epoch": 7.25,
            "step": 4350
        },
        {
            "loss": 0.0097,
            "grad_norm": 0.009975112974643707,
            "learning_rate": 2.7333333333333336e-06,
            "epoch": 7.266666666666667,
            "step": 4360
        },
        {
            "loss": 0.0012,
            "grad_norm": 0.2850410044193268,
            "learning_rate": 2.7166666666666668e-06,
            "epoch": 7.283333333333333,
            "step": 4370
        },
        {
            "loss": 0.0455,
            "grad_norm": 0.016361765563488007,
            "learning_rate": 2.7000000000000004e-06,
            "epoch": 7.3,
            "step": 4380
        },
        {
            "loss": 0.0483,
            "grad_norm": 0.006504841148853302,
            "learning_rate": 2.683333333333333e-06,
            "epoch": 7.316666666666666,
            "step": 4390
        },
        {
            "loss": 0.0939,
            "grad_norm": 0.00948572251945734,
            "learning_rate": 2.666666666666667e-06,
            "epoch": 7.333333333333333,
            "step": 4400
        },
        {
            "loss": 0.0005,
            "grad_norm": 0.011171018704771996,
            "learning_rate": 2.6500000000000005e-06,
            "epoch": 7.35,
            "step": 4410
        },
        {
            "loss": 0.0477,
            "grad_norm": 0.0037345585878938437,
            "learning_rate": 2.6333333333333332e-06,
            "epoch": 7.366666666666666,
            "step": 4420
        },
        {
            "loss": 0.0521,
            "grad_norm": 0.008656749501824379,
            "learning_rate": 2.616666666666667e-06,
            "epoch": 7.383333333333334,
            "step": 4430
        },
        {
            "loss": 0.0395,
            "grad_norm": 0.012291834689676762,
            "learning_rate": 2.6e-06,
            "epoch": 7.4,
            "step": 4440
        },
        {
            "loss": 0.0048,
            "grad_norm": 0.007288340944796801,
            "learning_rate": 2.5833333333333337e-06,
            "epoch": 7.416666666666667,
            "step": 4450
        },
        {
            "loss": 0.0014,
            "grad_norm": 0.017407862469553947,
            "learning_rate": 2.566666666666667e-06,
            "epoch": 7.433333333333334,
            "step": 4460
        },
        {
            "loss": 0.0357,
            "grad_norm": 14.10167407989502,
            "learning_rate": 2.55e-06,
            "epoch": 7.45,
            "step": 4470
        },
        {
            "loss": 0.0066,
            "grad_norm": 0.003071034327149391,
            "learning_rate": 2.5333333333333338e-06,
            "epoch": 7.466666666666667,
            "step": 4480
        },
        {
            "loss": 0.0811,
            "grad_norm": 14.992449760437012,
            "learning_rate": 2.5166666666666666e-06,
            "epoch": 7.483333333333333,
            "step": 4490
        },
        {
            "loss": 0.0146,
            "grad_norm": 0.26718857884407043,
            "learning_rate": 2.5e-06,
            "epoch": 7.5,
            "step": 4500
        },
        {
            "loss": 0.0004,
            "grad_norm": 0.07046078890562057,
            "learning_rate": 2.4833333333333334e-06,
            "epoch": 7.516666666666667,
            "step": 4510
        },
        {
            "loss": 0.0004,
            "grad_norm": 0.003777753794565797,
            "learning_rate": 2.466666666666667e-06,
            "epoch": 7.533333333333333,
            "step": 4520
        },
        {
            "loss": 0.0004,
            "grad_norm": 0.016047067940235138,
            "learning_rate": 2.4500000000000003e-06,
            "epoch": 7.55,
            "step": 4530
        },
        {
            "loss": 0.0439,
            "grad_norm": 0.18906697630882263,
            "learning_rate": 2.4333333333333335e-06,
            "epoch": 7.566666666666666,
            "step": 4540
        },
        {
            "loss": 0.0437,
            "grad_norm": 0.2840888202190399,
            "learning_rate": 2.4166666666666667e-06,
            "epoch": 7.583333333333333,
            "step": 4550
        },
        {
            "loss": 0.0851,
            "grad_norm": 0.019034236669540405,
            "learning_rate": 2.4000000000000003e-06,
            "epoch": 7.6,
            "step": 4560
        },
        {
            "loss": 0.0021,
            "grad_norm": 0.0568312369287014,
            "learning_rate": 2.3833333333333335e-06,
            "epoch": 7.616666666666667,
            "step": 4570
        },
        {
            "loss": 0.0008,
            "grad_norm": 0.045191045850515366,
            "learning_rate": 2.3666666666666667e-06,
            "epoch": 7.633333333333333,
            "step": 4580
        },
        {
            "loss": 0.0081,
            "grad_norm": 0.004006659612059593,
            "learning_rate": 2.35e-06,
            "epoch": 7.65,
            "step": 4590
        },
        {
            "loss": 0.0473,
            "grad_norm": 0.05550522357225418,
            "learning_rate": 2.3333333333333336e-06,
            "epoch": 7.666666666666667,
            "step": 4600
        },
        {
            "loss": 0.0461,
            "grad_norm": 0.01913468912243843,
            "learning_rate": 2.316666666666667e-06,
            "epoch": 7.683333333333334,
            "step": 4610
        },
        {
            "loss": 0.0252,
            "grad_norm": 0.29156437516212463,
            "learning_rate": 2.3000000000000004e-06,
            "epoch": 7.7,
            "step": 4620
        },
        {
            "loss": 0.0316,
            "grad_norm": 0.04535951837897301,
            "learning_rate": 2.2833333333333336e-06,
            "epoch": 7.716666666666667,
            "step": 4630
        },
        {
            "loss": 0.0147,
            "grad_norm": 0.35004183650016785,
            "learning_rate": 2.266666666666667e-06,
            "epoch": 7.733333333333333,
            "step": 4640
        },
        {
            "loss": 0.0436,
            "grad_norm": 0.00598996551707387,
            "learning_rate": 2.25e-06,
            "epoch": 7.75,
            "step": 4650
        },
        {
            "loss": 0.0008,
            "grad_norm": 0.004801080096513033,
            "learning_rate": 2.2333333333333333e-06,
            "epoch": 7.766666666666667,
            "step": 4660
        },
        {
            "loss": 0.0021,
            "grad_norm": 0.002656983444467187,
            "learning_rate": 2.216666666666667e-06,
            "epoch": 7.783333333333333,
            "step": 4670
        },
        {
            "loss": 0.0372,
            "grad_norm": 0.003602446522563696,
            "learning_rate": 2.2e-06,
            "epoch": 7.8,
            "step": 4680
        },
        {
            "loss": 0.0014,
            "grad_norm": 0.0050614322535693645,
            "learning_rate": 2.1833333333333333e-06,
            "epoch": 7.816666666666666,
            "step": 4690
        },
        {
            "loss": 0.011,
            "grad_norm": 0.04408786818385124,
            "learning_rate": 2.166666666666667e-06,
            "epoch": 7.833333333333333,
            "step": 4700
        },
        {
            "loss": 0.0901,
            "grad_norm": 0.0037429030053317547,
            "learning_rate": 2.15e-06,
            "epoch": 7.85,
            "step": 4710
        },
        {
            "loss": 0.0437,
            "grad_norm": 0.003465409157797694,
            "learning_rate": 2.133333333333334e-06,
            "epoch": 7.866666666666667,
            "step": 4720
        },
        {
            "loss": 0.0738,
            "grad_norm": 28.747758865356445,
            "learning_rate": 2.116666666666667e-06,
            "epoch": 7.883333333333333,
            "step": 4730
        },
        {
            "loss": 0.0436,
            "grad_norm": 0.10553641617298126,
            "learning_rate": 2.1000000000000002e-06,
            "epoch": 7.9,
            "step": 4740
        },
        {
            "loss": 0.011,
            "grad_norm": 0.004542275797575712,
            "learning_rate": 2.0833333333333334e-06,
            "epoch": 7.916666666666667,
            "step": 4750
        },
        {
            "loss": 0.1211,
            "grad_norm": 0.04362428933382034,
            "learning_rate": 2.0666666666666666e-06,
            "epoch": 7.933333333333334,
            "step": 4760
        },
        {
            "loss": 0.0449,
            "grad_norm": 0.10626885294914246,
            "learning_rate": 2.05e-06,
            "epoch": 7.95,
            "step": 4770
        },
        {
            "loss": 0.0009,
            "grad_norm": 0.007538128644227982,
            "learning_rate": 2.0333333333333335e-06,
            "epoch": 7.966666666666667,
            "step": 4780
        },
        {
            "loss": 0.0297,
            "grad_norm": 0.38446131348609924,
            "learning_rate": 2.0166666666666667e-06,
            "epoch": 7.983333333333333,
            "step": 4790
        },
        {
            "loss": 0.101,
            "grad_norm": 0.012848311103880405,
            "learning_rate": 2.0000000000000003e-06,
            "epoch": 8.0,
            "step": 4800
        },
        {
            "eval_loss": 0.18355411291122437,
            "eval_accuracy": 0.965,
            "eval_precision": 0.965293437383764,
            "eval_recall": 0.965,
            "eval_f1": 0.9650283747202756,
            "eval_runtime": 218.9377,
            "eval_samples_per_second": 5.481,
            "eval_steps_per_second": 0.685,
            "epoch": 8.0,
            "step": 4800
        },
        {
            "loss": 0.0012,
            "grad_norm": 0.00929854717105627,
            "learning_rate": 1.9833333333333335e-06,
            "epoch": 8.016666666666667,
            "step": 4810
        },
        {
            "loss": 0.0376,
            "grad_norm": 21.19739532470703,
            "learning_rate": 1.9666666666666668e-06,
            "epoch": 8.033333333333333,
            "step": 4820
        },
        {
            "loss": 0.001,
            "grad_norm": 1.2494951486587524,
            "learning_rate": 1.9500000000000004e-06,
            "epoch": 8.05,
            "step": 4830
        },
        {
            "loss": 0.0616,
            "grad_norm": 0.008714417926967144,
            "learning_rate": 1.9333333333333336e-06,
            "epoch": 8.066666666666666,
            "step": 4840
        },
        {
            "loss": 0.0231,
            "grad_norm": 0.2420571893453598,
            "learning_rate": 1.916666666666667e-06,
            "epoch": 8.083333333333334,
            "step": 4850
        },
        {
            "loss": 0.0011,
            "grad_norm": 2.889092206954956,
            "learning_rate": 1.9000000000000002e-06,
            "epoch": 8.1,
            "step": 4860
        },
        {
            "loss": 0.0353,
            "grad_norm": 0.0068716853857040405,
            "learning_rate": 1.8833333333333334e-06,
            "epoch": 8.116666666666667,
            "step": 4870
        },
        {
            "loss": 0.0031,
            "grad_norm": 0.004319793079048395,
            "learning_rate": 1.8666666666666669e-06,
            "epoch": 8.133333333333333,
            "step": 4880
        },
        {
            "loss": 0.0212,
            "grad_norm": 0.004381299484521151,
            "learning_rate": 1.85e-06,
            "epoch": 8.15,
            "step": 4890
        },
        {
            "loss": 0.0029,
            "grad_norm": 0.03353932127356529,
            "learning_rate": 1.8333333333333333e-06,
            "epoch": 8.166666666666666,
            "step": 4900
        },
        {
            "loss": 0.1361,
            "grad_norm": 29.713790893554688,
            "learning_rate": 1.816666666666667e-06,
            "epoch": 8.183333333333334,
            "step": 4910
        },
        {
            "loss": 0.0118,
            "grad_norm": 30.595226287841797,
            "learning_rate": 1.8000000000000001e-06,
            "epoch": 8.2,
            "step": 4920
        },
        {
            "loss": 0.0007,
            "grad_norm": 0.17830869555473328,
            "learning_rate": 1.7833333333333336e-06,
            "epoch": 8.216666666666667,
            "step": 4930
        },
        {
            "loss": 0.0207,
            "grad_norm": 0.0067756641656160355,
            "learning_rate": 1.7666666666666668e-06,
            "epoch": 8.233333333333333,
            "step": 4940
        },
        {
            "loss": 0.0574,
            "grad_norm": 11.77376651763916,
            "learning_rate": 1.75e-06,
            "epoch": 8.25,
            "step": 4950
        },
        {
            "loss": 0.0851,
            "grad_norm": 0.011610018089413643,
            "learning_rate": 1.7333333333333336e-06,
            "epoch": 8.266666666666667,
            "step": 4960
        },
        {
            "loss": 0.0292,
            "grad_norm": 2.1099562644958496,
            "learning_rate": 1.7166666666666668e-06,
            "epoch": 8.283333333333333,
            "step": 4970
        },
        {
            "loss": 0.0062,
            "grad_norm": 0.013325865380465984,
            "learning_rate": 1.7000000000000002e-06,
            "epoch": 8.3,
            "step": 4980
        },
        {
            "loss": 0.0917,
            "grad_norm": 0.005347143858671188,
            "learning_rate": 1.6833333333333335e-06,
            "epoch": 8.316666666666666,
            "step": 4990
        },
        {
            "loss": 0.0005,
            "grad_norm": 0.01652819849550724,
            "learning_rate": 1.6666666666666667e-06,
            "epoch": 8.333333333333334,
            "step": 5000
        },
        {
            "loss": 0.0074,
            "grad_norm": 0.007333468180149794,
            "learning_rate": 1.6500000000000003e-06,
            "epoch": 8.35,
            "step": 5010
        },
        {
            "loss": 0.0238,
            "grad_norm": 0.006949371658265591,
            "learning_rate": 1.6333333333333335e-06,
            "epoch": 8.366666666666667,
            "step": 5020
        },
        {
            "loss": 0.0178,
            "grad_norm": 0.004628316964954138,
            "learning_rate": 1.6166666666666667e-06,
            "epoch": 8.383333333333333,
            "step": 5030
        },
        {
            "loss": 0.07,
            "grad_norm": 0.1220986545085907,
            "learning_rate": 1.6000000000000001e-06,
            "epoch": 8.4,
            "step": 5040
        },
        {
            "loss": 0.0015,
            "grad_norm": 0.06120165064930916,
            "learning_rate": 1.5833333333333333e-06,
            "epoch": 8.416666666666666,
            "step": 5050
        },
        {
            "loss": 0.0671,
            "grad_norm": 0.004675278440117836,
            "learning_rate": 1.566666666666667e-06,
            "epoch": 8.433333333333334,
            "step": 5060
        },
        {
            "loss": 0.0343,
            "grad_norm": 0.050147026777267456,
            "learning_rate": 1.5500000000000002e-06,
            "epoch": 8.45,
            "step": 5070
        },
        {
            "loss": 0.1631,
            "grad_norm": 0.004386086016893387,
            "learning_rate": 1.5333333333333334e-06,
            "epoch": 8.466666666666667,
            "step": 5080
        },
        {
            "loss": 0.0061,
            "grad_norm": 0.010283995419740677,
            "learning_rate": 1.5166666666666668e-06,
            "epoch": 8.483333333333333,
            "step": 5090
        },
        {
            "loss": 0.0079,
            "grad_norm": 0.6093735694885254,
            "learning_rate": 1.5e-06,
            "epoch": 8.5,
            "step": 5100
        },
        {
            "loss": 0.0007,
            "grad_norm": 0.011906513944268227,
            "learning_rate": 1.4833333333333337e-06,
            "epoch": 8.516666666666667,
            "step": 5110
        },
        {
            "loss": 0.0277,
            "grad_norm": 19.77117347717285,
            "learning_rate": 1.4666666666666669e-06,
            "epoch": 8.533333333333333,
            "step": 5120
        },
        {
            "loss": 0.0003,
            "grad_norm": 0.01809779182076454,
            "learning_rate": 1.45e-06,
            "epoch": 8.55,
            "step": 5130
        },
        {
            "loss": 0.04,
            "grad_norm": 1.3305118083953857,
            "learning_rate": 1.4333333333333335e-06,
            "epoch": 8.566666666666666,
            "step": 5140
        },
        {
            "loss": 0.0158,
            "grad_norm": 0.0237461905926466,
            "learning_rate": 1.4166666666666667e-06,
            "epoch": 8.583333333333334,
            "step": 5150
        },
        {
            "loss": 0.0847,
            "grad_norm": 0.0890582799911499,
            "learning_rate": 1.4000000000000001e-06,
            "epoch": 8.6,
            "step": 5160
        },
        {
            "loss": 0.0003,
            "grad_norm": 0.00885780155658722,
            "learning_rate": 1.3833333333333336e-06,
            "epoch": 8.616666666666667,
            "step": 5170
        },
        {
            "loss": 0.0735,
            "grad_norm": 30.97936248779297,
            "learning_rate": 1.3666666666666668e-06,
            "epoch": 8.633333333333333,
            "step": 5180
        },
        {
            "loss": 0.0639,
            "grad_norm": 0.01629648357629776,
            "learning_rate": 1.3500000000000002e-06,
            "epoch": 8.65,
            "step": 5190
        },
        {
            "loss": 0.0006,
            "grad_norm": 0.07027857005596161,
            "learning_rate": 1.3333333333333334e-06,
            "epoch": 8.666666666666666,
            "step": 5200
        },
        {
            "loss": 0.0015,
            "grad_norm": 2.229137659072876,
            "learning_rate": 1.3166666666666666e-06,
            "epoch": 8.683333333333334,
            "step": 5210
        },
        {
            "loss": 0.0506,
            "grad_norm": 12.207786560058594,
            "learning_rate": 1.3e-06,
            "epoch": 8.7,
            "step": 5220
        },
        {
            "loss": 0.0872,
            "grad_norm": 0.10401338338851929,
            "learning_rate": 1.2833333333333335e-06,
            "epoch": 8.716666666666667,
            "step": 5230
        },
        {
            "loss": 0.0011,
            "grad_norm": 0.0075405254028737545,
            "learning_rate": 1.2666666666666669e-06,
            "epoch": 8.733333333333333,
            "step": 5240
        },
        {
            "loss": 0.0798,
            "grad_norm": 0.11524048447608948,
            "learning_rate": 1.25e-06,
            "epoch": 8.75,
            "step": 5250
        },
        {
            "loss": 0.0164,
            "grad_norm": 9.537919044494629,
            "learning_rate": 1.2333333333333335e-06,
            "epoch": 8.766666666666667,
            "step": 5260
        },
        {
            "loss": 0.0003,
            "grad_norm": 0.005278615280985832,
            "learning_rate": 1.2166666666666667e-06,
            "epoch": 8.783333333333333,
            "step": 5270
        },
        {
            "loss": 0.0011,
            "grad_norm": 0.002613738877698779,
            "learning_rate": 1.2000000000000002e-06,
            "epoch": 8.8,
            "step": 5280
        },
        {
            "loss": 0.0233,
            "grad_norm": 0.07503842562437057,
            "learning_rate": 1.1833333333333334e-06,
            "epoch": 8.816666666666666,
            "step": 5290
        },
        {
            "loss": 0.0087,
            "grad_norm": 0.06701983511447906,
            "learning_rate": 1.1666666666666668e-06,
            "epoch": 8.833333333333334,
            "step": 5300
        },
        {
            "loss": 0.0552,
            "grad_norm": 0.01403059158474207,
            "learning_rate": 1.1500000000000002e-06,
            "epoch": 8.85,
            "step": 5310
        },
        {
            "loss": 0.0554,
            "grad_norm": 0.007438851986080408,
            "learning_rate": 1.1333333333333334e-06,
            "epoch": 8.866666666666667,
            "step": 5320
        },
        {
            "loss": 0.0429,
            "grad_norm": 7.820013046264648,
            "learning_rate": 1.1166666666666666e-06,
            "epoch": 8.883333333333333,
            "step": 5330
        },
        {
            "loss": 0.0016,
            "grad_norm": 0.1071518287062645,
            "learning_rate": 1.1e-06,
            "epoch": 8.9,
            "step": 5340
        },
        {
            "loss": 0.0004,
            "grad_norm": 0.0077326674945652485,
            "learning_rate": 1.0833333333333335e-06,
            "epoch": 8.916666666666666,
            "step": 5350
        },
        {
            "loss": 0.0271,
            "grad_norm": 0.041097261011600494,
            "learning_rate": 1.066666666666667e-06,
            "epoch": 8.933333333333334,
            "step": 5360
        },
        {
            "loss": 0.0067,
            "grad_norm": 4.950902938842773,
            "learning_rate": 1.0500000000000001e-06,
            "epoch": 8.95,
            "step": 5370
        },
        {
            "loss": 0.0002,
            "grad_norm": 0.005769360810518265,
            "learning_rate": 1.0333333333333333e-06,
            "epoch": 8.966666666666667,
            "step": 5380
        },
        {
            "loss": 0.0536,
            "grad_norm": 0.984252393245697,
            "learning_rate": 1.0166666666666667e-06,
            "epoch": 8.983333333333333,
            "step": 5390
        },
        {
            "loss": 0.012,
            "grad_norm": 0.019234078004956245,
            "learning_rate": 1.0000000000000002e-06,
            "epoch": 9.0,
            "step": 5400
        },
        {
            "eval_loss": 0.18948696553707123,
            "eval_accuracy": 0.9675,
            "eval_precision": 0.9677625833503999,
            "eval_recall": 0.9675,
            "eval_f1": 0.9675161851660783,
            "eval_runtime": 114.4054,
            "eval_samples_per_second": 10.489,
            "eval_steps_per_second": 1.311,
            "epoch": 9.0,
            "step": 5400
        },
        {
            "loss": 0.0206,
            "grad_norm": 4.505910396575928,
            "learning_rate": 9.833333333333334e-07,
            "epoch": 9.016666666666667,
            "step": 5410
        },
        {
            "loss": 0.0867,
            "grad_norm": 0.005855808034539223,
            "learning_rate": 9.666666666666668e-07,
            "epoch": 9.033333333333333,
            "step": 5420
        },
        {
            "loss": 0.0008,
            "grad_norm": 1.6368074417114258,
            "learning_rate": 9.500000000000001e-07,
            "epoch": 9.05,
            "step": 5430
        },
        {
            "loss": 0.004,
            "grad_norm": 0.004921637009829283,
            "learning_rate": 9.333333333333334e-07,
            "epoch": 9.066666666666666,
            "step": 5440
        },
        {
            "loss": 0.0005,
            "grad_norm": 0.006393840536475182,
            "learning_rate": 9.166666666666666e-07,
            "epoch": 9.083333333333334,
            "step": 5450
        },
        {
            "loss": 0.0935,
            "grad_norm": 16.736635208129883,
            "learning_rate": 9.000000000000001e-07,
            "epoch": 9.1,
            "step": 5460
        },
        {
            "loss": 0.0559,
            "grad_norm": 0.025993289425969124,
            "learning_rate": 8.833333333333334e-07,
            "epoch": 9.116666666666667,
            "step": 5470
        },
        {
            "loss": 0.0034,
            "grad_norm": 0.003970523364841938,
            "learning_rate": 8.666666666666668e-07,
            "epoch": 9.133333333333333,
            "step": 5480
        },
        {
            "loss": 0.0124,
            "grad_norm": 0.005134547129273415,
            "learning_rate": 8.500000000000001e-07,
            "epoch": 9.15,
            "step": 5490
        },
        {
            "loss": 0.0006,
            "grad_norm": 0.14608624577522278,
            "learning_rate": 8.333333333333333e-07,
            "epoch": 9.166666666666666,
            "step": 5500
        },
        {
            "loss": 0.0534,
            "grad_norm": 0.002732072724029422,
            "learning_rate": 8.166666666666668e-07,
            "epoch": 9.183333333333334,
            "step": 5510
        },
        {
            "loss": 0.0279,
            "grad_norm": 0.009972967207431793,
            "learning_rate": 8.000000000000001e-07,
            "epoch": 9.2,
            "step": 5520
        },
        {
            "loss": 0.0839,
            "grad_norm": 0.02441009320318699,
            "learning_rate": 7.833333333333335e-07,
            "epoch": 9.216666666666667,
            "step": 5530
        },
        {
            "loss": 0.001,
            "grad_norm": 0.008435076102614403,
            "learning_rate": 7.666666666666667e-07,
            "epoch": 9.233333333333333,
            "step": 5540
        },
        {
            "loss": 0.0874,
            "grad_norm": 0.008767464198172092,
            "learning_rate": 7.5e-07,
            "epoch": 9.25,
            "step": 5550
        },
        {
            "loss": 0.0212,
            "grad_norm": 1.3328063488006592,
            "learning_rate": 7.333333333333334e-07,
            "epoch": 9.266666666666667,
            "step": 5560
        },
        {
            "loss": 0.0526,
            "grad_norm": 0.43638020753860474,
            "learning_rate": 7.166666666666668e-07,
            "epoch": 9.283333333333333,
            "step": 5570
        },
        {
            "loss": 0.0895,
            "grad_norm": 0.0039046918973326683,
            "learning_rate": 7.000000000000001e-07,
            "epoch": 9.3,
            "step": 5580
        },
        {
            "loss": 0.0015,
            "grad_norm": 1.4376249313354492,
            "learning_rate": 6.833333333333334e-07,
            "epoch": 9.316666666666666,
            "step": 5590
        },
        {
            "loss": 0.0164,
            "grad_norm": 0.0037423025351017714,
            "learning_rate": 6.666666666666667e-07,
            "epoch": 9.333333333333334,
            "step": 5600
        },
        {
            "loss": 0.0044,
            "grad_norm": 0.18632300198078156,
            "learning_rate": 6.5e-07,
            "epoch": 9.35,
            "step": 5610
        },
        {
            "loss": 0.0005,
            "grad_norm": 0.007009525317698717,
            "learning_rate": 6.333333333333334e-07,
            "epoch": 9.366666666666667,
            "step": 5620
        },
        {
            "loss": 0.0023,
            "grad_norm": 0.003149272408336401,
            "learning_rate": 6.166666666666668e-07,
            "epoch": 9.383333333333333,
            "step": 5630
        },
        {
            "loss": 0.0003,
            "grad_norm": 0.00467056967318058,
            "learning_rate": 6.000000000000001e-07,
            "epoch": 9.4,
            "step": 5640
        },
        {
            "loss": 0.0259,
            "grad_norm": 0.027754146605730057,
            "learning_rate": 5.833333333333334e-07,
            "epoch": 9.416666666666666,
            "step": 5650
        },
        {
            "loss": 0.0185,
            "grad_norm": 18.166114807128906,
            "learning_rate": 5.666666666666667e-07,
            "epoch": 9.433333333333334,
            "step": 5660
        },
        {
            "loss": 0.0466,
            "grad_norm": 3.1427578926086426,
            "learning_rate": 5.5e-07,
            "epoch": 9.45,
            "step": 5670
        },
        {
            "loss": 0.0778,
            "grad_norm": 0.01348127331584692,
            "learning_rate": 5.333333333333335e-07,
            "epoch": 9.466666666666667,
            "step": 5680
        },
        {
            "loss": 0.0339,
            "grad_norm": 6.1866021156311035,
            "learning_rate": 5.166666666666667e-07,
            "epoch": 9.483333333333333,
            "step": 5690
        },
        {
            "loss": 0.0366,
            "grad_norm": 0.01600966975092888,
            "learning_rate": 5.000000000000001e-07,
            "epoch": 9.5,
            "step": 5700
        },
        {
            "loss": 0.0026,
            "grad_norm": 0.014537394046783447,
            "learning_rate": 4.833333333333334e-07,
            "epoch": 9.516666666666667,
            "step": 5710
        },
        {
            "loss": 0.0004,
            "grad_norm": 0.0443265400826931,
            "learning_rate": 4.666666666666667e-07,
            "epoch": 9.533333333333333,
            "step": 5720
        },
        {
            "loss": 0.0007,
            "grad_norm": 0.02615167759358883,
            "learning_rate": 4.5000000000000003e-07,
            "epoch": 9.55,
            "step": 5730
        },
        {
            "loss": 0.0006,
            "grad_norm": 0.07429289817810059,
            "learning_rate": 4.333333333333334e-07,
            "epoch": 9.566666666666666,
            "step": 5740
        },
        {
            "loss": 0.0005,
            "grad_norm": 0.004275038838386536,
            "learning_rate": 4.1666666666666667e-07,
            "epoch": 9.583333333333334,
            "step": 5750
        },
        {
            "loss": 0.0016,
            "grad_norm": 0.4700046181678772,
            "learning_rate": 4.0000000000000003e-07,
            "epoch": 9.6,
            "step": 5760
        },
        {
            "loss": 0.0901,
            "grad_norm": 0.004176682326942682,
            "learning_rate": 3.8333333333333335e-07,
            "epoch": 9.616666666666667,
            "step": 5770
        },
        {
            "loss": 0.0115,
            "grad_norm": 0.012462547048926353,
            "learning_rate": 3.666666666666667e-07,
            "epoch": 9.633333333333333,
            "step": 5780
        },
        {
            "loss": 0.0188,
            "grad_norm": 0.007881115190684795,
            "learning_rate": 3.5000000000000004e-07,
            "epoch": 9.65,
            "step": 5790
        },
        {
            "loss": 0.0007,
            "grad_norm": 0.008786770515143871,
            "learning_rate": 3.3333333333333335e-07,
            "epoch": 9.666666666666666,
            "step": 5800
        },
        {
            "loss": 0.0031,
            "grad_norm": 0.0024352166801691055,
            "learning_rate": 3.166666666666667e-07,
            "epoch": 9.683333333333334,
            "step": 5810
        },
        {
            "loss": 0.0264,
            "grad_norm": 0.009593155235052109,
            "learning_rate": 3.0000000000000004e-07,
            "epoch": 9.7,
            "step": 5820
        },
        {
            "loss": 0.0003,
            "grad_norm": 0.025203246623277664,
            "learning_rate": 2.8333333333333336e-07,
            "epoch": 9.716666666666667,
            "step": 5830
        },
        {
            "loss": 0.057,
            "grad_norm": 0.048121582716703415,
            "learning_rate": 2.666666666666667e-07,
            "epoch": 9.733333333333333,
            "step": 5840
        },
        {
            "loss": 0.0229,
            "grad_norm": 0.003732373472303152,
            "learning_rate": 2.5000000000000004e-07,
            "epoch": 9.75,
            "step": 5850
        },
        {
            "loss": 0.0608,
            "grad_norm": 0.004932283423841,
            "learning_rate": 2.3333333333333336e-07,
            "epoch": 9.766666666666667,
            "step": 5860
        },
        {
            "loss": 0.0359,
            "grad_norm": 0.030499236658215523,
            "learning_rate": 2.166666666666667e-07,
            "epoch": 9.783333333333333,
            "step": 5870
        },
        {
            "loss": 0.011,
            "grad_norm": 0.0031837548594921827,
            "learning_rate": 2.0000000000000002e-07,
            "epoch": 9.8,
            "step": 5880
        },
        {
            "loss": 0.0868,
            "grad_norm": 0.07266710698604584,
            "learning_rate": 1.8333333333333336e-07,
            "epoch": 9.816666666666666,
            "step": 5890
        },
        {
            "loss": 0.0096,
            "grad_norm": 0.01174941472709179,
            "learning_rate": 1.6666666666666668e-07,
            "epoch": 9.833333333333334,
            "step": 5900
        },
        {
            "loss": 0.0881,
            "grad_norm": 0.022479882463812828,
            "learning_rate": 1.5000000000000002e-07,
            "epoch": 9.85,
            "step": 5910
        },
        {
            "loss": 0.0184,
            "grad_norm": 17.773773193359375,
            "learning_rate": 1.3333333333333336e-07,
            "epoch": 9.866666666666667,
            "step": 5920
        },
        {
            "loss": 0.0551,
            "grad_norm": 0.004227271769195795,
            "learning_rate": 1.1666666666666668e-07,
            "epoch": 9.883333333333333,
            "step": 5930
        },
        {
            "loss": 0.0714,
            "grad_norm": 0.005529151763767004,
            "learning_rate": 1.0000000000000001e-07,
            "epoch": 9.9,
            "step": 5940
        },
        {
            "loss": 0.0891,
            "grad_norm": 0.16508668661117554,
            "learning_rate": 8.333333333333334e-08,
            "epoch": 9.916666666666666,
            "step": 5950
        },
        {
            "loss": 0.04,
            "grad_norm": 0.003929554019123316,
            "learning_rate": 6.666666666666668e-08,
            "epoch": 9.933333333333334,
            "step": 5960
        },
        {
            "loss": 0.0232,
            "grad_norm": 11.259281158447266,
            "learning_rate": 5.0000000000000004e-08,
            "epoch": 9.95,
            "step": 5970
        },
        {
            "loss": 0.0807,
            "grad_norm": 0.08924594521522522,
            "learning_rate": 3.333333333333334e-08,
            "epoch": 9.966666666666667,
            "step": 5980
        },
        {
            "loss": 0.0004,
            "grad_norm": 0.03356528282165527,
            "learning_rate": 1.666666666666667e-08,
            "epoch": 9.983333333333333,
            "step": 5990
        },
        {
            "loss": 0.0286,
            "grad_norm": 9.746097564697266,
            "learning_rate": 0.0,
            "epoch": 10.0,
            "step": 6000
        },
        {
            "eval_loss": 0.19166699051856995,
            "eval_accuracy": 0.965,
            "eval_precision": 0.9653163051019933,
            "eval_recall": 0.965,
            "eval_f1": 0.965013169812775,
            "eval_runtime": 101.5547,
            "eval_samples_per_second": 11.816,
            "eval_steps_per_second": 1.477,
            "epoch": 10.0,
            "step": 6000
        },
        {
            "train_runtime": 18816.5303,
            "train_samples_per_second": 2.551,
            "train_steps_per_second": 0.319,
            "total_flos": 1.2629784231936e+16,
            "train_loss": 0.11088280579362375,
            "epoch": 10.0,
            "step": 6000
        },
        {
            "eval_loss": 0.18948696553707123,
            "eval_accuracy": 0.9675,
            "eval_precision": 0.9677625833503999,
            "eval_recall": 0.9675,
            "eval_f1": 0.9675161851660783,
            "eval_runtime": 126.332,
            "eval_samples_per_second": 9.499,
            "eval_steps_per_second": 1.187,
            "epoch": 10.0,
            "step": 6000
        }
    ],
    "eval_results": {
        "eval_loss": 0.18948696553707123,
        "eval_accuracy": 0.9675,
        "eval_precision": 0.9677625833503999,
        "eval_recall": 0.9675,
        "eval_f1": 0.9675161851660783,
        "eval_runtime": 126.332,
        "eval_samples_per_second": 9.499,
        "eval_steps_per_second": 1.187,
        "epoch": 10.0
    }
}