{
    "training_logs": [
        {
            "loss": 1.9095,
            "grad_norm": 7.954360485076904,
            "learning_rate": 9.983333333333333e-06,
            "epoch": 0.016666666666666666,
            "step": 10
        },
        {
            "loss": 1.7857,
            "grad_norm": 8.003809928894043,
            "learning_rate": 9.966666666666667e-06,
            "epoch": 0.03333333333333333,
            "step": 20
        },
        {
            "loss": 1.7731,
            "grad_norm": 9.526094436645508,
            "learning_rate": 9.950000000000001e-06,
            "epoch": 0.05,
            "step": 30
        },
        {
            "loss": 1.7587,
            "grad_norm": 7.797267913818359,
            "learning_rate": 9.933333333333334e-06,
            "epoch": 0.06666666666666667,
            "step": 40
        },
        {
            "loss": 1.7704,
            "grad_norm": 8.394732475280762,
            "learning_rate": 9.916666666666668e-06,
            "epoch": 0.08333333333333333,
            "step": 50
        },
        {
            "loss": 1.7571,
            "grad_norm": 7.615294933319092,
            "learning_rate": 9.9e-06,
            "epoch": 0.1,
            "step": 60
        },
        {
            "loss": 1.6818,
            "grad_norm": 7.891114711761475,
            "learning_rate": 9.883333333333334e-06,
            "epoch": 0.11666666666666667,
            "step": 70
        },
        {
            "loss": 1.7108,
            "grad_norm": 7.7785115242004395,
            "learning_rate": 9.866666666666668e-06,
            "epoch": 0.13333333333333333,
            "step": 80
        },
        {
            "loss": 1.6708,
            "grad_norm": 7.431139945983887,
            "learning_rate": 9.85e-06,
            "epoch": 0.15,
            "step": 90
        },
        {
            "loss": 1.6326,
            "grad_norm": 7.731107234954834,
            "learning_rate": 9.833333333333333e-06,
            "epoch": 0.16666666666666666,
            "step": 100
        },
        {
            "loss": 1.6401,
            "grad_norm": 7.9457783699035645,
            "learning_rate": 9.816666666666667e-06,
            "epoch": 0.18333333333333332,
            "step": 110
        },
        {
            "loss": 1.6676,
            "grad_norm": 7.8805341720581055,
            "learning_rate": 9.800000000000001e-06,
            "epoch": 0.2,
            "step": 120
        },
        {
            "loss": 1.6188,
            "grad_norm": 7.12434196472168,
            "learning_rate": 9.783333333333335e-06,
            "epoch": 0.21666666666666667,
            "step": 130
        },
        {
            "loss": 1.579,
            "grad_norm": 8.237606048583984,
            "learning_rate": 9.766666666666667e-06,
            "epoch": 0.23333333333333334,
            "step": 140
        },
        {
            "loss": 1.5314,
            "grad_norm": 7.0072221755981445,
            "learning_rate": 9.75e-06,
            "epoch": 0.25,
            "step": 150
        },
        {
            "loss": 1.5592,
            "grad_norm": 8.176112174987793,
            "learning_rate": 9.733333333333334e-06,
            "epoch": 0.26666666666666666,
            "step": 160
        },
        {
            "loss": 1.4597,
            "grad_norm": 7.601954936981201,
            "learning_rate": 9.716666666666668e-06,
            "epoch": 0.2833333333333333,
            "step": 170
        },
        {
            "loss": 1.5134,
            "grad_norm": 9.380533218383789,
            "learning_rate": 9.7e-06,
            "epoch": 0.3,
            "step": 180
        },
        {
            "loss": 1.4988,
            "grad_norm": 7.774541854858398,
            "learning_rate": 9.683333333333334e-06,
            "epoch": 0.31666666666666665,
            "step": 190
        },
        {
            "loss": 1.4734,
            "grad_norm": 10.042534828186035,
            "learning_rate": 9.666666666666667e-06,
            "epoch": 0.3333333333333333,
            "step": 200
        },
        {
            "loss": 1.4167,
            "grad_norm": 7.301825046539307,
            "learning_rate": 9.65e-06,
            "epoch": 0.35,
            "step": 210
        },
        {
            "loss": 1.3742,
            "grad_norm": 7.6402997970581055,
            "learning_rate": 9.633333333333335e-06,
            "epoch": 0.36666666666666664,
            "step": 220
        },
        {
            "loss": 1.384,
            "grad_norm": 8.16154670715332,
            "learning_rate": 9.616666666666667e-06,
            "epoch": 0.38333333333333336,
            "step": 230
        },
        {
            "loss": 1.3528,
            "grad_norm": 7.94070291519165,
            "learning_rate": 9.600000000000001e-06,
            "epoch": 0.4,
            "step": 240
        },
        {
            "loss": 1.3225,
            "grad_norm": 7.138143062591553,
            "learning_rate": 9.583333333333335e-06,
            "epoch": 0.4166666666666667,
            "step": 250
        },
        {
            "loss": 1.2939,
            "grad_norm": 7.522918224334717,
            "learning_rate": 9.566666666666668e-06,
            "epoch": 0.43333333333333335,
            "step": 260
        },
        {
            "loss": 1.2706,
            "grad_norm": 8.660194396972656,
            "learning_rate": 9.55e-06,
            "epoch": 0.45,
            "step": 270
        },
        {
            "loss": 1.3008,
            "grad_norm": 7.997690200805664,
            "learning_rate": 9.533333333333334e-06,
            "epoch": 0.4666666666666667,
            "step": 280
        },
        {
            "loss": 1.2174,
            "grad_norm": 6.894758224487305,
            "learning_rate": 9.516666666666668e-06,
            "epoch": 0.48333333333333334,
            "step": 290
        },
        {
            "loss": 1.2295,
            "grad_norm": 7.6379313468933105,
            "learning_rate": 9.5e-06,
            "epoch": 0.5,
            "step": 300
        },
        {
            "loss": 1.2499,
            "grad_norm": 8.481027603149414,
            "learning_rate": 9.483333333333335e-06,
            "epoch": 0.5166666666666667,
            "step": 310
        },
        {
            "loss": 1.1283,
            "grad_norm": 8.264766693115234,
            "learning_rate": 9.466666666666667e-06,
            "epoch": 0.5333333333333333,
            "step": 320
        },
        {
            "loss": 1.1291,
            "grad_norm": 8.716385841369629,
            "learning_rate": 9.450000000000001e-06,
            "epoch": 0.55,
            "step": 330
        },
        {
            "loss": 1.1161,
            "grad_norm": 6.994809150695801,
            "learning_rate": 9.433333333333335e-06,
            "epoch": 0.5666666666666667,
            "step": 340
        },
        {
            "loss": 1.0367,
            "grad_norm": 9.343724250793457,
            "learning_rate": 9.416666666666667e-06,
            "epoch": 0.5833333333333334,
            "step": 350
        },
        {
            "loss": 1.103,
            "grad_norm": 9.18075942993164,
            "learning_rate": 9.4e-06,
            "epoch": 0.6,
            "step": 360
        },
        {
            "loss": 1.1501,
            "grad_norm": 8.438958168029785,
            "learning_rate": 9.383333333333334e-06,
            "epoch": 0.6166666666666667,
            "step": 370
        },
        {
            "loss": 1.0375,
            "grad_norm": 8.091630935668945,
            "learning_rate": 9.366666666666668e-06,
            "epoch": 0.6333333333333333,
            "step": 380
        },
        {
            "loss": 0.9276,
            "grad_norm": 8.958128929138184,
            "learning_rate": 9.350000000000002e-06,
            "epoch": 0.65,
            "step": 390
        },
        {
            "loss": 0.9742,
            "grad_norm": 8.911609649658203,
            "learning_rate": 9.333333333333334e-06,
            "epoch": 0.6666666666666666,
            "step": 400
        },
        {
            "loss": 0.9168,
            "grad_norm": 7.934863090515137,
            "learning_rate": 9.316666666666667e-06,
            "epoch": 0.6833333333333333,
            "step": 410
        },
        {
            "loss": 0.9072,
            "grad_norm": 9.193050384521484,
            "learning_rate": 9.3e-06,
            "epoch": 0.7,
            "step": 420
        },
        {
            "loss": 0.8083,
            "grad_norm": 10.087547302246094,
            "learning_rate": 9.283333333333335e-06,
            "epoch": 0.7166666666666667,
            "step": 430
        },
        {
            "loss": 0.8144,
            "grad_norm": 7.825150489807129,
            "learning_rate": 9.266666666666667e-06,
            "epoch": 0.7333333333333333,
            "step": 440
        },
        {
            "loss": 0.8742,
            "grad_norm": 9.027332305908203,
            "learning_rate": 9.250000000000001e-06,
            "epoch": 0.75,
            "step": 450
        },
        {
            "loss": 0.7757,
            "grad_norm": 7.156112194061279,
            "learning_rate": 9.233333333333334e-06,
            "epoch": 0.7666666666666667,
            "step": 460
        },
        {
            "loss": 0.7664,
            "grad_norm": 8.28553295135498,
            "learning_rate": 9.216666666666668e-06,
            "epoch": 0.7833333333333333,
            "step": 470
        },
        {
            "loss": 0.719,
            "grad_norm": 7.013314723968506,
            "learning_rate": 9.200000000000002e-06,
            "epoch": 0.8,
            "step": 480
        },
        {
            "loss": 0.8807,
            "grad_norm": 8.489965438842773,
            "learning_rate": 9.183333333333334e-06,
            "epoch": 0.8166666666666667,
            "step": 490
        },
        {
            "loss": 0.8558,
            "grad_norm": 8.14307975769043,
            "learning_rate": 9.166666666666666e-06,
            "epoch": 0.8333333333333334,
            "step": 500
        },
        {
            "loss": 0.6917,
            "grad_norm": 10.482239723205566,
            "learning_rate": 9.15e-06,
            "epoch": 0.85,
            "step": 510
        },
        {
            "loss": 0.623,
            "grad_norm": 8.132905960083008,
            "learning_rate": 9.133333333333335e-06,
            "epoch": 0.8666666666666667,
            "step": 520
        },
        {
            "loss": 0.6949,
            "grad_norm": 6.182165622711182,
            "learning_rate": 9.116666666666667e-06,
            "epoch": 0.8833333333333333,
            "step": 530
        },
        {
            "loss": 0.8256,
            "grad_norm": 10.894614219665527,
            "learning_rate": 9.100000000000001e-06,
            "epoch": 0.9,
            "step": 540
        },
        {
            "loss": 0.7234,
            "grad_norm": 8.326443672180176,
            "learning_rate": 9.083333333333333e-06,
            "epoch": 0.9166666666666666,
            "step": 550
        },
        {
            "loss": 0.8339,
            "grad_norm": 8.718221664428711,
            "learning_rate": 9.066666666666667e-06,
            "epoch": 0.9333333333333333,
            "step": 560
        },
        {
            "loss": 0.7558,
            "grad_norm": 8.845638275146484,
            "learning_rate": 9.050000000000001e-06,
            "epoch": 0.95,
            "step": 570
        },
        {
            "loss": 0.5349,
            "grad_norm": 6.540979385375977,
            "learning_rate": 9.033333333333334e-06,
            "epoch": 0.9666666666666667,
            "step": 580
        },
        {
            "loss": 0.6488,
            "grad_norm": 9.031180381774902,
            "learning_rate": 9.016666666666666e-06,
            "epoch": 0.9833333333333333,
            "step": 590
        },
        {
            "loss": 0.6692,
            "grad_norm": 9.577651977539062,
            "learning_rate": 9e-06,
            "epoch": 1.0,
            "step": 600
        },
        {
            "eval_loss": 0.5444777607917786,
            "eval_accuracy": 0.8408333333333333,
            "eval_precision": 0.8474113048214484,
            "eval_recall": 0.8408333333333333,
            "eval_f1": 0.8407550489970862,
            "eval_runtime": 113.2844,
            "eval_samples_per_second": 10.593,
            "eval_steps_per_second": 1.324,
            "epoch": 1.0,
            "step": 600
        },
        {
            "loss": 0.6257,
            "grad_norm": 6.758422374725342,
            "learning_rate": 8.983333333333334e-06,
            "epoch": 1.0166666666666666,
            "step": 610
        },
        {
            "loss": 0.6625,
            "grad_norm": 7.66425895690918,
            "learning_rate": 8.966666666666667e-06,
            "epoch": 1.0333333333333334,
            "step": 620
        },
        {
            "loss": 0.5802,
            "grad_norm": 9.724748611450195,
            "learning_rate": 8.95e-06,
            "epoch": 1.05,
            "step": 630
        },
        {
            "loss": 0.5842,
            "grad_norm": 8.218244552612305,
            "learning_rate": 8.933333333333333e-06,
            "epoch": 1.0666666666666667,
            "step": 640
        },
        {
            "loss": 0.6256,
            "grad_norm": 9.22938346862793,
            "learning_rate": 8.916666666666667e-06,
            "epoch": 1.0833333333333333,
            "step": 650
        },
        {
            "loss": 0.4846,
            "grad_norm": 5.929748058319092,
            "learning_rate": 8.900000000000001e-06,
            "epoch": 1.1,
            "step": 660
        },
        {
            "loss": 0.6296,
            "grad_norm": 8.610109329223633,
            "learning_rate": 8.883333333333334e-06,
            "epoch": 1.1166666666666667,
            "step": 670
        },
        {
            "loss": 0.6578,
            "grad_norm": 10.650432586669922,
            "learning_rate": 8.866666666666668e-06,
            "epoch": 1.1333333333333333,
            "step": 680
        },
        {
            "loss": 0.6318,
            "grad_norm": 4.275235652923584,
            "learning_rate": 8.85e-06,
            "epoch": 1.15,
            "step": 690
        },
        {
            "loss": 0.5843,
            "grad_norm": 9.876005172729492,
            "learning_rate": 8.833333333333334e-06,
            "epoch": 1.1666666666666667,
            "step": 700
        },
        {
            "loss": 0.5219,
            "grad_norm": 8.703267097473145,
            "learning_rate": 8.816666666666668e-06,
            "epoch": 1.1833333333333333,
            "step": 710
        },
        {
            "loss": 0.4062,
            "grad_norm": 3.429757595062256,
            "learning_rate": 8.8e-06,
            "epoch": 1.2,
            "step": 720
        },
        {
            "loss": 0.4785,
            "grad_norm": 11.710888862609863,
            "learning_rate": 8.783333333333335e-06,
            "epoch": 1.2166666666666668,
            "step": 730
        },
        {
            "loss": 0.462,
            "grad_norm": 12.913642883300781,
            "learning_rate": 8.766666666666669e-06,
            "epoch": 1.2333333333333334,
            "step": 740
        },
        {
            "loss": 0.5708,
            "grad_norm": 8.07109546661377,
            "learning_rate": 8.750000000000001e-06,
            "epoch": 1.25,
            "step": 750
        },
        {
            "loss": 0.5586,
            "grad_norm": 6.313266277313232,
            "learning_rate": 8.733333333333333e-06,
            "epoch": 1.2666666666666666,
            "step": 760
        },
        {
            "loss": 0.5884,
            "grad_norm": 8.588699340820312,
            "learning_rate": 8.716666666666667e-06,
            "epoch": 1.2833333333333332,
            "step": 770
        },
        {
            "loss": 0.5125,
            "grad_norm": 2.719130039215088,
            "learning_rate": 8.700000000000001e-06,
            "epoch": 1.3,
            "step": 780
        },
        {
            "loss": 0.4551,
            "grad_norm": 7.006764888763428,
            "learning_rate": 8.683333333333334e-06,
            "epoch": 1.3166666666666667,
            "step": 790
        },
        {
            "loss": 0.4853,
            "grad_norm": 7.161996841430664,
            "learning_rate": 8.666666666666668e-06,
            "epoch": 1.3333333333333333,
            "step": 800
        },
        {
            "loss": 0.6085,
            "grad_norm": 12.625633239746094,
            "learning_rate": 8.65e-06,
            "epoch": 1.35,
            "step": 810
        },
        {
            "loss": 0.5422,
            "grad_norm": 10.614120483398438,
            "learning_rate": 8.633333333333334e-06,
            "epoch": 1.3666666666666667,
            "step": 820
        },
        {
            "loss": 0.4135,
            "grad_norm": 6.640992641448975,
            "learning_rate": 8.616666666666668e-06,
            "epoch": 1.3833333333333333,
            "step": 830
        },
        {
            "loss": 0.397,
            "grad_norm": 12.268790245056152,
            "learning_rate": 8.6e-06,
            "epoch": 1.4,
            "step": 840
        },
        {
            "loss": 0.4961,
            "grad_norm": 2.11523175239563,
            "learning_rate": 8.583333333333333e-06,
            "epoch": 1.4166666666666667,
            "step": 850
        },
        {
            "loss": 0.4658,
            "grad_norm": 10.444721221923828,
            "learning_rate": 8.566666666666667e-06,
            "epoch": 1.4333333333333333,
            "step": 860
        },
        {
            "loss": 0.6619,
            "grad_norm": 11.68044376373291,
            "learning_rate": 8.550000000000001e-06,
            "epoch": 1.45,
            "step": 870
        },
        {
            "loss": 0.5414,
            "grad_norm": 9.374436378479004,
            "learning_rate": 8.533333333333335e-06,
            "epoch": 1.4666666666666668,
            "step": 880
        },
        {
            "loss": 0.4873,
            "grad_norm": 3.4910383224487305,
            "learning_rate": 8.516666666666668e-06,
            "epoch": 1.4833333333333334,
            "step": 890
        },
        {
            "loss": 0.3677,
            "grad_norm": 5.876993179321289,
            "learning_rate": 8.5e-06,
            "epoch": 1.5,
            "step": 900
        },
        {
            "loss": 0.5575,
            "grad_norm": 6.194106101989746,
            "learning_rate": 8.483333333333334e-06,
            "epoch": 1.5166666666666666,
            "step": 910
        },
        {
            "loss": 0.4685,
            "grad_norm": 11.270179748535156,
            "learning_rate": 8.466666666666668e-06,
            "epoch": 1.5333333333333332,
            "step": 920
        },
        {
            "loss": 0.4008,
            "grad_norm": 14.83774471282959,
            "learning_rate": 8.45e-06,
            "epoch": 1.55,
            "step": 930
        },
        {
            "loss": 0.4257,
            "grad_norm": 8.391493797302246,
            "learning_rate": 8.433333333333334e-06,
            "epoch": 1.5666666666666667,
            "step": 940
        },
        {
            "loss": 0.4678,
            "grad_norm": 12.751416206359863,
            "learning_rate": 8.416666666666667e-06,
            "epoch": 1.5833333333333335,
            "step": 950
        },
        {
            "loss": 0.4049,
            "grad_norm": 9.019501686096191,
            "learning_rate": 8.400000000000001e-06,
            "epoch": 1.6,
            "step": 960
        },
        {
            "loss": 0.4401,
            "grad_norm": 8.848780632019043,
            "learning_rate": 8.383333333333335e-06,
            "epoch": 1.6166666666666667,
            "step": 970
        },
        {
            "loss": 0.4341,
            "grad_norm": 11.946819305419922,
            "learning_rate": 8.366666666666667e-06,
            "epoch": 1.6333333333333333,
            "step": 980
        },
        {
            "loss": 0.32,
            "grad_norm": 6.784460544586182,
            "learning_rate": 8.35e-06,
            "epoch": 1.65,
            "step": 990
        },
        {
            "loss": 0.4469,
            "grad_norm": 8.741340637207031,
            "learning_rate": 8.333333333333334e-06,
            "epoch": 1.6666666666666665,
            "step": 1000
        },
        {
            "loss": 0.4083,
            "grad_norm": 7.672242641448975,
            "learning_rate": 8.316666666666668e-06,
            "epoch": 1.6833333333333333,
            "step": 1010
        },
        {
            "loss": 0.3621,
            "grad_norm": 11.698626518249512,
            "learning_rate": 8.3e-06,
            "epoch": 1.7,
            "step": 1020
        },
        {
            "loss": 0.509,
            "grad_norm": 22.59157943725586,
            "learning_rate": 8.283333333333334e-06,
            "epoch": 1.7166666666666668,
            "step": 1030
        },
        {
            "loss": 0.4775,
            "grad_norm": 8.411389350891113,
            "learning_rate": 8.266666666666667e-06,
            "epoch": 1.7333333333333334,
            "step": 1040
        },
        {
            "loss": 0.4188,
            "grad_norm": 6.639674186706543,
            "learning_rate": 8.25e-06,
            "epoch": 1.75,
            "step": 1050
        },
        {
            "loss": 0.3337,
            "grad_norm": 6.489199161529541,
            "learning_rate": 8.233333333333335e-06,
            "epoch": 1.7666666666666666,
            "step": 1060
        },
        {
            "loss": 0.2804,
            "grad_norm": 3.086426019668579,
            "learning_rate": 8.216666666666667e-06,
            "epoch": 1.7833333333333332,
            "step": 1070
        },
        {
            "loss": 0.4154,
            "grad_norm": 7.997756481170654,
            "learning_rate": 8.2e-06,
            "epoch": 1.8,
            "step": 1080
        },
        {
            "loss": 0.3072,
            "grad_norm": 6.970251083374023,
            "learning_rate": 8.183333333333333e-06,
            "epoch": 1.8166666666666667,
            "step": 1090
        },
        {
            "loss": 0.3532,
            "grad_norm": 6.415294170379639,
            "learning_rate": 8.166666666666668e-06,
            "epoch": 1.8333333333333335,
            "step": 1100
        },
        {
            "loss": 0.4442,
            "grad_norm": 16.07005500793457,
            "learning_rate": 8.15e-06,
            "epoch": 1.85,
            "step": 1110
        },
        {
            "loss": 0.4528,
            "grad_norm": 8.397736549377441,
            "learning_rate": 8.133333333333334e-06,
            "epoch": 1.8666666666666667,
            "step": 1120
        },
        {
            "loss": 0.295,
            "grad_norm": 5.484150409698486,
            "learning_rate": 8.116666666666666e-06,
            "epoch": 1.8833333333333333,
            "step": 1130
        },
        {
            "loss": 0.4605,
            "grad_norm": 9.753403663635254,
            "learning_rate": 8.1e-06,
            "epoch": 1.9,
            "step": 1140
        },
        {
            "loss": 0.4114,
            "grad_norm": 9.554768562316895,
            "learning_rate": 8.083333333333334e-06,
            "epoch": 1.9166666666666665,
            "step": 1150
        },
        {
            "loss": 0.4168,
            "grad_norm": 1.6130783557891846,
            "learning_rate": 8.066666666666667e-06,
            "epoch": 1.9333333333333333,
            "step": 1160
        },
        {
            "loss": 0.3602,
            "grad_norm": 7.387263774871826,
            "learning_rate": 8.050000000000001e-06,
            "epoch": 1.95,
            "step": 1170
        },
        {
            "loss": 0.3037,
            "grad_norm": 3.309535503387451,
            "learning_rate": 8.033333333333335e-06,
            "epoch": 1.9666666666666668,
            "step": 1180
        },
        {
            "loss": 0.394,
            "grad_norm": 11.38912296295166,
            "learning_rate": 8.016666666666667e-06,
            "epoch": 1.9833333333333334,
            "step": 1190
        },
        {
            "loss": 0.3709,
            "grad_norm": 2.8647830486297607,
            "learning_rate": 8.000000000000001e-06,
            "epoch": 2.0,
            "step": 1200
        },
        {
            "eval_loss": 0.3638123571872711,
            "eval_accuracy": 0.8883333333333333,
            "eval_precision": 0.8937055918169288,
            "eval_recall": 0.8883333333333333,
            "eval_f1": 0.8887297244230742,
            "eval_runtime": 162.8558,
            "eval_samples_per_second": 7.368,
            "eval_steps_per_second": 0.921,
            "epoch": 2.0,
            "step": 1200
        },
        {
            "loss": 0.247,
            "grad_norm": 4.596135139465332,
            "learning_rate": 7.983333333333334e-06,
            "epoch": 2.0166666666666666,
            "step": 1210
        },
        {
            "loss": 0.4624,
            "grad_norm": 19.44668960571289,
            "learning_rate": 7.966666666666668e-06,
            "epoch": 2.033333333333333,
            "step": 1220
        },
        {
            "loss": 0.4284,
            "grad_norm": 13.286505699157715,
            "learning_rate": 7.950000000000002e-06,
            "epoch": 2.05,
            "step": 1230
        },
        {
            "loss": 0.3411,
            "grad_norm": 12.616390228271484,
            "learning_rate": 7.933333333333334e-06,
            "epoch": 2.066666666666667,
            "step": 1240
        },
        {
            "loss": 0.3018,
            "grad_norm": 6.727102756500244,
            "learning_rate": 7.916666666666667e-06,
            "epoch": 2.0833333333333335,
            "step": 1250
        },
        {
            "loss": 0.3114,
            "grad_norm": 6.7287774085998535,
            "learning_rate": 7.9e-06,
            "epoch": 2.1,
            "step": 1260
        },
        {
            "loss": 0.3222,
            "grad_norm": 2.8272876739501953,
            "learning_rate": 7.883333333333335e-06,
            "epoch": 2.1166666666666667,
            "step": 1270
        },
        {
            "loss": 0.3769,
            "grad_norm": 9.873106002807617,
            "learning_rate": 7.866666666666667e-06,
            "epoch": 2.1333333333333333,
            "step": 1280
        },
        {
            "loss": 0.4466,
            "grad_norm": 17.485675811767578,
            "learning_rate": 7.850000000000001e-06,
            "epoch": 2.15,
            "step": 1290
        },
        {
            "loss": 0.1764,
            "grad_norm": 5.870113372802734,
            "learning_rate": 7.833333333333333e-06,
            "epoch": 2.1666666666666665,
            "step": 1300
        },
        {
            "loss": 0.47,
            "grad_norm": 9.668453216552734,
            "learning_rate": 7.816666666666667e-06,
            "epoch": 2.183333333333333,
            "step": 1310
        },
        {
            "loss": 0.347,
            "grad_norm": 12.2838134765625,
            "learning_rate": 7.800000000000002e-06,
            "epoch": 2.2,
            "step": 1320
        },
        {
            "loss": 0.3828,
            "grad_norm": 12.095175743103027,
            "learning_rate": 7.783333333333334e-06,
            "epoch": 2.216666666666667,
            "step": 1330
        },
        {
            "loss": 0.2726,
            "grad_norm": 12.147957801818848,
            "learning_rate": 7.766666666666666e-06,
            "epoch": 2.2333333333333334,
            "step": 1340
        },
        {
            "loss": 0.3214,
            "grad_norm": 7.403600215911865,
            "learning_rate": 7.75e-06,
            "epoch": 2.25,
            "step": 1350
        },
        {
            "loss": 0.3802,
            "grad_norm": 18.8166446685791,
            "learning_rate": 7.733333333333334e-06,
            "epoch": 2.2666666666666666,
            "step": 1360
        },
        {
            "loss": 0.3972,
            "grad_norm": 3.870170831680298,
            "learning_rate": 7.716666666666667e-06,
            "epoch": 2.283333333333333,
            "step": 1370
        },
        {
            "loss": 0.2377,
            "grad_norm": 8.496048927307129,
            "learning_rate": 7.7e-06,
            "epoch": 2.3,
            "step": 1380
        },
        {
            "loss": 0.3173,
            "grad_norm": 8.103413581848145,
            "learning_rate": 7.683333333333333e-06,
            "epoch": 2.3166666666666664,
            "step": 1390
        },
        {
            "loss": 0.4214,
            "grad_norm": 8.851737022399902,
            "learning_rate": 7.666666666666667e-06,
            "epoch": 2.3333333333333335,
            "step": 1400
        },
        {
            "loss": 0.2485,
            "grad_norm": 1.3760566711425781,
            "learning_rate": 7.650000000000001e-06,
            "epoch": 2.35,
            "step": 1410
        },
        {
            "loss": 0.3075,
            "grad_norm": 10.304375648498535,
            "learning_rate": 7.633333333333334e-06,
            "epoch": 2.3666666666666667,
            "step": 1420
        },
        {
            "loss": 0.5046,
            "grad_norm": 12.000863075256348,
            "learning_rate": 7.616666666666668e-06,
            "epoch": 2.3833333333333333,
            "step": 1430
        },
        {
            "loss": 0.4055,
            "grad_norm": 15.889266014099121,
            "learning_rate": 7.600000000000001e-06,
            "epoch": 2.4,
            "step": 1440
        },
        {
            "loss": 0.4981,
            "grad_norm": 11.292738914489746,
            "learning_rate": 7.583333333333333e-06,
            "epoch": 2.4166666666666665,
            "step": 1450
        },
        {
            "loss": 0.4647,
            "grad_norm": 12.478596687316895,
            "learning_rate": 7.566666666666667e-06,
            "epoch": 2.4333333333333336,
            "step": 1460
        },
        {
            "loss": 0.2646,
            "grad_norm": 3.912928342819214,
            "learning_rate": 7.5500000000000006e-06,
            "epoch": 2.45,
            "step": 1470
        },
        {
            "loss": 0.23,
            "grad_norm": 4.841770172119141,
            "learning_rate": 7.533333333333334e-06,
            "epoch": 2.466666666666667,
            "step": 1480
        },
        {
            "loss": 0.2774,
            "grad_norm": 13.091907501220703,
            "learning_rate": 7.516666666666668e-06,
            "epoch": 2.4833333333333334,
            "step": 1490
        },
        {
            "loss": 0.2871,
            "grad_norm": 5.5660505294799805,
            "learning_rate": 7.500000000000001e-06,
            "epoch": 2.5,
            "step": 1500
        },
        {
            "loss": 0.329,
            "grad_norm": 9.794629096984863,
            "learning_rate": 7.483333333333333e-06,
            "epoch": 2.5166666666666666,
            "step": 1510
        },
        {
            "loss": 0.4448,
            "grad_norm": 6.41090726852417,
            "learning_rate": 7.4666666666666675e-06,
            "epoch": 2.533333333333333,
            "step": 1520
        },
        {
            "loss": 0.4141,
            "grad_norm": 6.575207233428955,
            "learning_rate": 7.450000000000001e-06,
            "epoch": 2.55,
            "step": 1530
        },
        {
            "loss": 0.3535,
            "grad_norm": 14.978118896484375,
            "learning_rate": 7.433333333333334e-06,
            "epoch": 2.5666666666666664,
            "step": 1540
        },
        {
            "loss": 0.4001,
            "grad_norm": 9.126429557800293,
            "learning_rate": 7.416666666666668e-06,
            "epoch": 2.5833333333333335,
            "step": 1550
        },
        {
            "loss": 0.4019,
            "grad_norm": 7.132734775543213,
            "learning_rate": 7.4e-06,
            "epoch": 2.6,
            "step": 1560
        },
        {
            "loss": 0.1703,
            "grad_norm": 2.4825546741485596,
            "learning_rate": 7.3833333333333335e-06,
            "epoch": 2.6166666666666667,
            "step": 1570
        },
        {
            "loss": 0.2914,
            "grad_norm": 11.274001121520996,
            "learning_rate": 7.3666666666666676e-06,
            "epoch": 2.6333333333333333,
            "step": 1580
        },
        {
            "loss": 0.3089,
            "grad_norm": 7.8751540184021,
            "learning_rate": 7.350000000000001e-06,
            "epoch": 2.65,
            "step": 1590
        },
        {
            "loss": 0.4089,
            "grad_norm": 10.934666633605957,
            "learning_rate": 7.333333333333333e-06,
            "epoch": 2.6666666666666665,
            "step": 1600
        },
        {
            "loss": 0.5008,
            "grad_norm": 7.930577278137207,
            "learning_rate": 7.316666666666667e-06,
            "epoch": 2.6833333333333336,
            "step": 1610
        },
        {
            "loss": 0.3167,
            "grad_norm": 8.138191223144531,
            "learning_rate": 7.3e-06,
            "epoch": 2.7,
            "step": 1620
        },
        {
            "loss": 0.3473,
            "grad_norm": 9.39590835571289,
            "learning_rate": 7.2833333333333345e-06,
            "epoch": 2.716666666666667,
            "step": 1630
        },
        {
            "loss": 0.1825,
            "grad_norm": 3.0598576068878174,
            "learning_rate": 7.266666666666668e-06,
            "epoch": 2.7333333333333334,
            "step": 1640
        },
        {
            "loss": 0.4587,
            "grad_norm": 12.752875328063965,
            "learning_rate": 7.25e-06,
            "epoch": 2.75,
            "step": 1650
        },
        {
            "loss": 0.3718,
            "grad_norm": 10.89582347869873,
            "learning_rate": 7.233333333333334e-06,
            "epoch": 2.7666666666666666,
            "step": 1660
        },
        {
            "loss": 0.3806,
            "grad_norm": 4.742217540740967,
            "learning_rate": 7.216666666666667e-06,
            "epoch": 2.783333333333333,
            "step": 1670
        },
        {
            "loss": 0.2826,
            "grad_norm": 4.846105575561523,
            "learning_rate": 7.2000000000000005e-06,
            "epoch": 2.8,
            "step": 1680
        },
        {
            "loss": 0.3418,
            "grad_norm": 11.142988204956055,
            "learning_rate": 7.183333333333335e-06,
            "epoch": 2.8166666666666664,
            "step": 1690
        },
        {
            "loss": 0.2637,
            "grad_norm": 16.76405906677246,
            "learning_rate": 7.166666666666667e-06,
            "epoch": 2.8333333333333335,
            "step": 1700
        },
        {
            "loss": 0.2965,
            "grad_norm": 2.2912120819091797,
            "learning_rate": 7.15e-06,
            "epoch": 2.85,
            "step": 1710
        },
        {
            "loss": 0.3311,
            "grad_norm": 13.089212417602539,
            "learning_rate": 7.133333333333334e-06,
            "epoch": 2.8666666666666667,
            "step": 1720
        },
        {
            "loss": 0.3104,
            "grad_norm": 8.845441818237305,
            "learning_rate": 7.116666666666667e-06,
            "epoch": 2.8833333333333333,
            "step": 1730
        },
        {
            "loss": 0.1958,
            "grad_norm": 3.8438498973846436,
            "learning_rate": 7.100000000000001e-06,
            "epoch": 2.9,
            "step": 1740
        },
        {
            "loss": 0.2969,
            "grad_norm": 15.39337158203125,
            "learning_rate": 7.083333333333335e-06,
            "epoch": 2.9166666666666665,
            "step": 1750
        },
        {
            "loss": 0.2818,
            "grad_norm": 3.2553367614746094,
            "learning_rate": 7.066666666666667e-06,
            "epoch": 2.9333333333333336,
            "step": 1760
        },
        {
            "loss": 0.3635,
            "grad_norm": 3.4850635528564453,
            "learning_rate": 7.05e-06,
            "epoch": 2.95,
            "step": 1770
        },
        {
            "loss": 0.3056,
            "grad_norm": 8.601727485656738,
            "learning_rate": 7.033333333333334e-06,
            "epoch": 2.966666666666667,
            "step": 1780
        },
        {
            "loss": 0.4457,
            "grad_norm": 16.193004608154297,
            "learning_rate": 7.0166666666666675e-06,
            "epoch": 2.9833333333333334,
            "step": 1790
        },
        {
            "loss": 0.4278,
            "grad_norm": 14.952281951904297,
            "learning_rate": 7e-06,
            "epoch": 3.0,
            "step": 1800
        },
        {
            "eval_loss": 0.3328837454319,
            "eval_accuracy": 0.8958333333333334,
            "eval_precision": 0.9003576147822634,
            "eval_recall": 0.8958333333333334,
            "eval_f1": 0.8963822886570753,
            "eval_runtime": 165.3877,
            "eval_samples_per_second": 7.256,
            "eval_steps_per_second": 0.907,
            "epoch": 3.0,
            "step": 1800
        },
        {
            "loss": 0.4056,
            "grad_norm": 4.732051849365234,
            "learning_rate": 6.983333333333334e-06,
            "epoch": 3.0166666666666666,
            "step": 1810
        },
        {
            "loss": 0.2699,
            "grad_norm": 13.51949691772461,
            "learning_rate": 6.966666666666667e-06,
            "epoch": 3.033333333333333,
            "step": 1820
        },
        {
            "loss": 0.2494,
            "grad_norm": 10.237061500549316,
            "learning_rate": 6.95e-06,
            "epoch": 3.05,
            "step": 1830
        },
        {
            "loss": 0.3158,
            "grad_norm": 16.89426612854004,
            "learning_rate": 6.9333333333333344e-06,
            "epoch": 3.066666666666667,
            "step": 1840
        },
        {
            "loss": 0.2674,
            "grad_norm": 6.069257736206055,
            "learning_rate": 6.916666666666667e-06,
            "epoch": 3.0833333333333335,
            "step": 1850
        },
        {
            "loss": 0.1838,
            "grad_norm": 5.120703220367432,
            "learning_rate": 6.9e-06,
            "epoch": 3.1,
            "step": 1860
        },
        {
            "loss": 0.2494,
            "grad_norm": 9.74067211151123,
            "learning_rate": 6.883333333333334e-06,
            "epoch": 3.1166666666666667,
            "step": 1870
        },
        {
            "loss": 0.2955,
            "grad_norm": 10.74453353881836,
            "learning_rate": 6.866666666666667e-06,
            "epoch": 3.1333333333333333,
            "step": 1880
        },
        {
            "loss": 0.2105,
            "grad_norm": 8.566086769104004,
            "learning_rate": 6.850000000000001e-06,
            "epoch": 3.15,
            "step": 1890
        },
        {
            "loss": 0.1496,
            "grad_norm": 4.351747035980225,
            "learning_rate": 6.833333333333334e-06,
            "epoch": 3.1666666666666665,
            "step": 1900
        },
        {
            "loss": 0.275,
            "grad_norm": 14.531471252441406,
            "learning_rate": 6.816666666666667e-06,
            "epoch": 3.183333333333333,
            "step": 1910
        },
        {
            "loss": 0.2862,
            "grad_norm": 6.655972957611084,
            "learning_rate": 6.800000000000001e-06,
            "epoch": 3.2,
            "step": 1920
        },
        {
            "loss": 0.2411,
            "grad_norm": 5.379061222076416,
            "learning_rate": 6.783333333333334e-06,
            "epoch": 3.216666666666667,
            "step": 1930
        },
        {
            "loss": 0.4121,
            "grad_norm": 10.108139991760254,
            "learning_rate": 6.7666666666666665e-06,
            "epoch": 3.2333333333333334,
            "step": 1940
        },
        {
            "loss": 0.3256,
            "grad_norm": 9.965911865234375,
            "learning_rate": 6.750000000000001e-06,
            "epoch": 3.25,
            "step": 1950
        },
        {
            "loss": 0.395,
            "grad_norm": 2.1357123851776123,
            "learning_rate": 6.733333333333334e-06,
            "epoch": 3.2666666666666666,
            "step": 1960
        },
        {
            "loss": 0.3087,
            "grad_norm": 5.851401329040527,
            "learning_rate": 6.716666666666667e-06,
            "epoch": 3.283333333333333,
            "step": 1970
        },
        {
            "loss": 0.3331,
            "grad_norm": 15.3194580078125,
            "learning_rate": 6.700000000000001e-06,
            "epoch": 3.3,
            "step": 1980
        },
        {
            "loss": 0.2341,
            "grad_norm": 7.329178333282471,
            "learning_rate": 6.683333333333334e-06,
            "epoch": 3.3166666666666664,
            "step": 1990
        },
        {
            "loss": 0.2088,
            "grad_norm": 15.693411827087402,
            "learning_rate": 6.666666666666667e-06,
            "epoch": 3.3333333333333335,
            "step": 2000
        },
        {
            "loss": 0.3176,
            "grad_norm": 14.679054260253906,
            "learning_rate": 6.650000000000001e-06,
            "epoch": 3.35,
            "step": 2010
        },
        {
            "loss": 0.2428,
            "grad_norm": 12.362092018127441,
            "learning_rate": 6.633333333333334e-06,
            "epoch": 3.3666666666666667,
            "step": 2020
        },
        {
            "loss": 0.2246,
            "grad_norm": 22.398168563842773,
            "learning_rate": 6.616666666666667e-06,
            "epoch": 3.3833333333333333,
            "step": 2030
        },
        {
            "loss": 0.2126,
            "grad_norm": 11.64953899383545,
            "learning_rate": 6.600000000000001e-06,
            "epoch": 3.4,
            "step": 2040
        },
        {
            "loss": 0.2742,
            "grad_norm": 7.0637664794921875,
            "learning_rate": 6.5833333333333335e-06,
            "epoch": 3.4166666666666665,
            "step": 2050
        },
        {
            "loss": 0.2601,
            "grad_norm": 2.576897621154785,
            "learning_rate": 6.566666666666667e-06,
            "epoch": 3.4333333333333336,
            "step": 2060
        },
        {
            "loss": 0.4336,
            "grad_norm": 10.541772842407227,
            "learning_rate": 6.550000000000001e-06,
            "epoch": 3.45,
            "step": 2070
        },
        {
            "loss": 0.3166,
            "grad_norm": 10.915837287902832,
            "learning_rate": 6.533333333333334e-06,
            "epoch": 3.466666666666667,
            "step": 2080
        },
        {
            "loss": 0.2049,
            "grad_norm": 5.540414810180664,
            "learning_rate": 6.516666666666666e-06,
            "epoch": 3.4833333333333334,
            "step": 2090
        },
        {
            "loss": 0.2668,
            "grad_norm": 9.550728797912598,
            "learning_rate": 6.5000000000000004e-06,
            "epoch": 3.5,
            "step": 2100
        },
        {
            "loss": 0.2277,
            "grad_norm": 3.434968948364258,
            "learning_rate": 6.483333333333334e-06,
            "epoch": 3.5166666666666666,
            "step": 2110
        },
        {
            "loss": 0.3087,
            "grad_norm": 3.8697268962860107,
            "learning_rate": 6.466666666666667e-06,
            "epoch": 3.533333333333333,
            "step": 2120
        },
        {
            "loss": 0.4061,
            "grad_norm": 12.301863670349121,
            "learning_rate": 6.450000000000001e-06,
            "epoch": 3.55,
            "step": 2130
        },
        {
            "loss": 0.3166,
            "grad_norm": 8.603313446044922,
            "learning_rate": 6.433333333333333e-06,
            "epoch": 3.5666666666666664,
            "step": 2140
        },
        {
            "loss": 0.3971,
            "grad_norm": 7.468178749084473,
            "learning_rate": 6.416666666666667e-06,
            "epoch": 3.5833333333333335,
            "step": 2150
        },
        {
            "loss": 0.2726,
            "grad_norm": 3.3838932514190674,
            "learning_rate": 6.4000000000000006e-06,
            "epoch": 3.6,
            "step": 2160
        },
        {
            "loss": 0.3055,
            "grad_norm": 1.0419961214065552,
            "learning_rate": 6.383333333333334e-06,
            "epoch": 3.6166666666666667,
            "step": 2170
        },
        {
            "loss": 0.3753,
            "grad_norm": 18.31198501586914,
            "learning_rate": 6.366666666666668e-06,
            "epoch": 3.6333333333333333,
            "step": 2180
        },
        {
            "loss": 0.2726,
            "grad_norm": 10.614712715148926,
            "learning_rate": 6.35e-06,
            "epoch": 3.65,
            "step": 2190
        },
        {
            "loss": 0.2106,
            "grad_norm": 11.168688774108887,
            "learning_rate": 6.333333333333333e-06,
            "epoch": 3.6666666666666665,
            "step": 2200
        },
        {
            "loss": 0.2456,
            "grad_norm": 12.081109046936035,
            "learning_rate": 6.3166666666666675e-06,
            "epoch": 3.6833333333333336,
            "step": 2210
        },
        {
            "loss": 0.4298,
            "grad_norm": 8.795758247375488,
            "learning_rate": 6.300000000000001e-06,
            "epoch": 3.7,
            "step": 2220
        },
        {
            "loss": 0.1916,
            "grad_norm": 8.148676872253418,
            "learning_rate": 6.283333333333334e-06,
            "epoch": 3.716666666666667,
            "step": 2230
        },
        {
            "loss": 0.3763,
            "grad_norm": 8.695422172546387,
            "learning_rate": 6.266666666666668e-06,
            "epoch": 3.7333333333333334,
            "step": 2240
        },
        {
            "loss": 0.3303,
            "grad_norm": 17.748077392578125,
            "learning_rate": 6.25e-06,
            "epoch": 3.75,
            "step": 2250
        },
        {
            "loss": 0.3997,
            "grad_norm": 16.821409225463867,
            "learning_rate": 6.2333333333333335e-06,
            "epoch": 3.7666666666666666,
            "step": 2260
        },
        {
            "loss": 0.3213,
            "grad_norm": 18.59960174560547,
            "learning_rate": 6.2166666666666676e-06,
            "epoch": 3.783333333333333,
            "step": 2270
        },
        {
            "loss": 0.4313,
            "grad_norm": 6.9866862297058105,
            "learning_rate": 6.200000000000001e-06,
            "epoch": 3.8,
            "step": 2280
        },
        {
            "loss": 0.1689,
            "grad_norm": 1.2706211805343628,
            "learning_rate": 6.183333333333333e-06,
            "epoch": 3.8166666666666664,
            "step": 2290
        },
        {
            "loss": 0.3326,
            "grad_norm": 5.880048751831055,
            "learning_rate": 6.166666666666667e-06,
            "epoch": 3.8333333333333335,
            "step": 2300
        },
        {
            "loss": 0.207,
            "grad_norm": 2.9672276973724365,
            "learning_rate": 6.15e-06,
            "epoch": 3.85,
            "step": 2310
        },
        {
            "loss": 0.2449,
            "grad_norm": 11.072299003601074,
            "learning_rate": 6.133333333333334e-06,
            "epoch": 3.8666666666666667,
            "step": 2320
        },
        {
            "loss": 0.2869,
            "grad_norm": 15.42689037322998,
            "learning_rate": 6.116666666666668e-06,
            "epoch": 3.8833333333333333,
            "step": 2330
        },
        {
            "loss": 0.2219,
            "grad_norm": 16.74136734008789,
            "learning_rate": 6.1e-06,
            "epoch": 3.9,
            "step": 2340
        },
        {
            "loss": 0.2938,
            "grad_norm": 2.7580084800720215,
            "learning_rate": 6.083333333333333e-06,
            "epoch": 3.9166666666666665,
            "step": 2350
        },
        {
            "loss": 0.286,
            "grad_norm": 11.257203102111816,
            "learning_rate": 6.066666666666667e-06,
            "epoch": 3.9333333333333336,
            "step": 2360
        },
        {
            "loss": 0.2971,
            "grad_norm": 7.902373790740967,
            "learning_rate": 6.0500000000000005e-06,
            "epoch": 3.95,
            "step": 2370
        },
        {
            "loss": 0.2128,
            "grad_norm": 6.548805236816406,
            "learning_rate": 6.033333333333335e-06,
            "epoch": 3.966666666666667,
            "step": 2380
        },
        {
            "loss": 0.276,
            "grad_norm": 1.6906770467758179,
            "learning_rate": 6.016666666666667e-06,
            "epoch": 3.9833333333333334,
            "step": 2390
        },
        {
            "loss": 0.3531,
            "grad_norm": 1.8989841938018799,
            "learning_rate": 6e-06,
            "epoch": 4.0,
            "step": 2400
        },
        {
            "eval_loss": 0.31003043055534363,
            "eval_accuracy": 0.9041666666666667,
            "eval_precision": 0.906318660066258,
            "eval_recall": 0.9041666666666667,
            "eval_f1": 0.9043605725929587,
            "eval_runtime": 169.0712,
            "eval_samples_per_second": 7.098,
            "eval_steps_per_second": 0.887,
            "epoch": 4.0,
            "step": 2400
        },
        {
            "loss": 0.2467,
            "grad_norm": 9.67902660369873,
            "learning_rate": 5.983333333333334e-06,
            "epoch": 4.016666666666667,
            "step": 2410
        },
        {
            "loss": 0.2689,
            "grad_norm": 17.529184341430664,
            "learning_rate": 5.966666666666667e-06,
            "epoch": 4.033333333333333,
            "step": 2420
        },
        {
            "loss": 0.1505,
            "grad_norm": 12.033650398254395,
            "learning_rate": 5.950000000000001e-06,
            "epoch": 4.05,
            "step": 2430
        },
        {
            "loss": 0.324,
            "grad_norm": 8.568212509155273,
            "learning_rate": 5.933333333333335e-06,
            "epoch": 4.066666666666666,
            "step": 2440
        },
        {
            "loss": 0.2568,
            "grad_norm": 6.708075046539307,
            "learning_rate": 5.916666666666667e-06,
            "epoch": 4.083333333333333,
            "step": 2450
        },
        {
            "loss": 0.3544,
            "grad_norm": 9.621341705322266,
            "learning_rate": 5.9e-06,
            "epoch": 4.1,
            "step": 2460
        },
        {
            "loss": 0.3182,
            "grad_norm": 26.923532485961914,
            "learning_rate": 5.883333333333334e-06,
            "epoch": 4.116666666666666,
            "step": 2470
        },
        {
            "loss": 0.2283,
            "grad_norm": 10.39654541015625,
            "learning_rate": 5.8666666666666675e-06,
            "epoch": 4.133333333333334,
            "step": 2480
        },
        {
            "loss": 0.3273,
            "grad_norm": 15.476777076721191,
            "learning_rate": 5.85e-06,
            "epoch": 4.15,
            "step": 2490
        },
        {
            "loss": 0.2451,
            "grad_norm": 12.600465774536133,
            "learning_rate": 5.833333333333334e-06,
            "epoch": 4.166666666666667,
            "step": 2500
        },
        {
            "loss": 0.2501,
            "grad_norm": 19.820640563964844,
            "learning_rate": 5.816666666666667e-06,
            "epoch": 4.183333333333334,
            "step": 2510
        },
        {
            "loss": 0.3594,
            "grad_norm": 10.797075271606445,
            "learning_rate": 5.8e-06,
            "epoch": 4.2,
            "step": 2520
        },
        {
            "loss": 0.2874,
            "grad_norm": 2.129506826400757,
            "learning_rate": 5.7833333333333344e-06,
            "epoch": 4.216666666666667,
            "step": 2530
        },
        {
            "loss": 0.2299,
            "grad_norm": 7.632070541381836,
            "learning_rate": 5.766666666666667e-06,
            "epoch": 4.233333333333333,
            "step": 2540
        },
        {
            "loss": 0.1964,
            "grad_norm": 5.000175476074219,
            "learning_rate": 5.75e-06,
            "epoch": 4.25,
            "step": 2550
        },
        {
            "loss": 0.2728,
            "grad_norm": 3.6338889598846436,
            "learning_rate": 5.733333333333334e-06,
            "epoch": 4.266666666666667,
            "step": 2560
        },
        {
            "loss": 0.1575,
            "grad_norm": 5.538021564483643,
            "learning_rate": 5.716666666666667e-06,
            "epoch": 4.283333333333333,
            "step": 2570
        },
        {
            "loss": 0.3957,
            "grad_norm": 15.472576141357422,
            "learning_rate": 5.7e-06,
            "epoch": 4.3,
            "step": 2580
        },
        {
            "loss": 0.2509,
            "grad_norm": 12.331437110900879,
            "learning_rate": 5.683333333333334e-06,
            "epoch": 4.316666666666666,
            "step": 2590
        },
        {
            "loss": 0.3367,
            "grad_norm": 14.570642471313477,
            "learning_rate": 5.666666666666667e-06,
            "epoch": 4.333333333333333,
            "step": 2600
        },
        {
            "loss": 0.3189,
            "grad_norm": 5.287402153015137,
            "learning_rate": 5.65e-06,
            "epoch": 4.35,
            "step": 2610
        },
        {
            "loss": 0.1783,
            "grad_norm": 9.385981559753418,
            "learning_rate": 5.633333333333334e-06,
            "epoch": 4.366666666666666,
            "step": 2620
        },
        {
            "loss": 0.1902,
            "grad_norm": 2.303709030151367,
            "learning_rate": 5.6166666666666665e-06,
            "epoch": 4.383333333333334,
            "step": 2630
        },
        {
            "loss": 0.3851,
            "grad_norm": 7.702710151672363,
            "learning_rate": 5.600000000000001e-06,
            "epoch": 4.4,
            "step": 2640
        },
        {
            "loss": 0.2596,
            "grad_norm": 9.034758567810059,
            "learning_rate": 5.583333333333334e-06,
            "epoch": 4.416666666666667,
            "step": 2650
        },
        {
            "loss": 0.304,
            "grad_norm": 19.31509017944336,
            "learning_rate": 5.566666666666667e-06,
            "epoch": 4.433333333333334,
            "step": 2660
        },
        {
            "loss": 0.3224,
            "grad_norm": 14.711685180664062,
            "learning_rate": 5.550000000000001e-06,
            "epoch": 4.45,
            "step": 2670
        },
        {
            "loss": 0.2888,
            "grad_norm": 5.246616363525391,
            "learning_rate": 5.533333333333334e-06,
            "epoch": 4.466666666666667,
            "step": 2680
        },
        {
            "loss": 0.2204,
            "grad_norm": 5.651686191558838,
            "learning_rate": 5.516666666666667e-06,
            "epoch": 4.483333333333333,
            "step": 2690
        },
        {
            "loss": 0.2546,
            "grad_norm": 2.229132890701294,
            "learning_rate": 5.500000000000001e-06,
            "epoch": 4.5,
            "step": 2700
        },
        {
            "loss": 0.2123,
            "grad_norm": 10.476078033447266,
            "learning_rate": 5.483333333333334e-06,
            "epoch": 4.516666666666667,
            "step": 2710
        },
        {
            "loss": 0.1654,
            "grad_norm": 6.7281036376953125,
            "learning_rate": 5.466666666666667e-06,
            "epoch": 4.533333333333333,
            "step": 2720
        },
        {
            "loss": 0.4812,
            "grad_norm": 0.1767924427986145,
            "learning_rate": 5.450000000000001e-06,
            "epoch": 4.55,
            "step": 2730
        },
        {
            "loss": 0.1578,
            "grad_norm": 10.157631874084473,
            "learning_rate": 5.4333333333333335e-06,
            "epoch": 4.566666666666666,
            "step": 2740
        },
        {
            "loss": 0.1758,
            "grad_norm": 9.220051765441895,
            "learning_rate": 5.416666666666667e-06,
            "epoch": 4.583333333333333,
            "step": 2750
        },
        {
            "loss": 0.238,
            "grad_norm": 8.07634449005127,
            "learning_rate": 5.400000000000001e-06,
            "epoch": 4.6,
            "step": 2760
        },
        {
            "loss": 0.3355,
            "grad_norm": 12.282087326049805,
            "learning_rate": 5.383333333333334e-06,
            "epoch": 4.616666666666667,
            "step": 2770
        },
        {
            "loss": 0.2473,
            "grad_norm": 10.944229125976562,
            "learning_rate": 5.366666666666666e-06,
            "epoch": 4.633333333333333,
            "step": 2780
        },
        {
            "loss": 0.2185,
            "grad_norm": 11.952129364013672,
            "learning_rate": 5.3500000000000004e-06,
            "epoch": 4.65,
            "step": 2790
        },
        {
            "loss": 0.3318,
            "grad_norm": 0.8207849264144897,
            "learning_rate": 5.333333333333334e-06,
            "epoch": 4.666666666666667,
            "step": 2800
        },
        {
            "loss": 0.2397,
            "grad_norm": 6.868300437927246,
            "learning_rate": 5.316666666666667e-06,
            "epoch": 4.683333333333334,
            "step": 2810
        },
        {
            "loss": 0.2271,
            "grad_norm": 14.819596290588379,
            "learning_rate": 5.300000000000001e-06,
            "epoch": 4.7,
            "step": 2820
        },
        {
            "loss": 0.3532,
            "grad_norm": 1.8213711977005005,
            "learning_rate": 5.283333333333333e-06,
            "epoch": 4.716666666666667,
            "step": 2830
        },
        {
            "loss": 0.2935,
            "grad_norm": 23.102487564086914,
            "learning_rate": 5.2666666666666665e-06,
            "epoch": 4.733333333333333,
            "step": 2840
        },
        {
            "loss": 0.2254,
            "grad_norm": 15.895340919494629,
            "learning_rate": 5.2500000000000006e-06,
            "epoch": 4.75,
            "step": 2850
        },
        {
            "loss": 0.3174,
            "grad_norm": 1.232080340385437,
            "learning_rate": 5.233333333333334e-06,
            "epoch": 4.766666666666667,
            "step": 2860
        },
        {
            "loss": 0.2679,
            "grad_norm": 13.989264488220215,
            "learning_rate": 5.216666666666666e-06,
            "epoch": 4.783333333333333,
            "step": 2870
        },
        {
            "loss": 0.4108,
            "grad_norm": 18.89752960205078,
            "learning_rate": 5.2e-06,
            "epoch": 4.8,
            "step": 2880
        },
        {
            "loss": 0.3397,
            "grad_norm": 1.8585361242294312,
            "learning_rate": 5.183333333333333e-06,
            "epoch": 4.816666666666666,
            "step": 2890
        },
        {
            "loss": 0.1829,
            "grad_norm": 14.434073448181152,
            "learning_rate": 5.1666666666666675e-06,
            "epoch": 4.833333333333333,
            "step": 2900
        },
        {
            "loss": 0.1502,
            "grad_norm": 1.3906868696212769,
            "learning_rate": 5.150000000000001e-06,
            "epoch": 4.85,
            "step": 2910
        },
        {
            "loss": 0.3727,
            "grad_norm": 6.04777193069458,
            "learning_rate": 5.133333333333334e-06,
            "epoch": 4.866666666666667,
            "step": 2920
        },
        {
            "loss": 0.2146,
            "grad_norm": 1.9908188581466675,
            "learning_rate": 5.116666666666668e-06,
            "epoch": 4.883333333333333,
            "step": 2930
        },
        {
            "loss": 0.2653,
            "grad_norm": 4.465250015258789,
            "learning_rate": 5.1e-06,
            "epoch": 4.9,
            "step": 2940
        },
        {
            "loss": 0.2425,
            "grad_norm": 6.858553409576416,
            "learning_rate": 5.0833333333333335e-06,
            "epoch": 4.916666666666667,
            "step": 2950
        },
        {
            "loss": 0.2657,
            "grad_norm": 6.937963962554932,
            "learning_rate": 5.0666666666666676e-06,
            "epoch": 4.933333333333334,
            "step": 2960
        },
        {
            "loss": 0.2185,
            "grad_norm": 20.633182525634766,
            "learning_rate": 5.050000000000001e-06,
            "epoch": 4.95,
            "step": 2970
        },
        {
            "loss": 0.2742,
            "grad_norm": 13.34694766998291,
            "learning_rate": 5.033333333333333e-06,
            "epoch": 4.966666666666667,
            "step": 2980
        },
        {
            "loss": 0.2386,
            "grad_norm": 10.075190544128418,
            "learning_rate": 5.016666666666667e-06,
            "epoch": 4.983333333333333,
            "step": 2990
        },
        {
            "loss": 0.2194,
            "grad_norm": 18.21858787536621,
            "learning_rate": 5e-06,
            "epoch": 5.0,
            "step": 3000
        },
        {
            "eval_loss": 0.3098784387111664,
            "eval_accuracy": 0.905,
            "eval_precision": 0.9088811836925794,
            "eval_recall": 0.905,
            "eval_f1": 0.9056271010851403,
            "eval_runtime": 166.8771,
            "eval_samples_per_second": 7.191,
            "eval_steps_per_second": 0.899,
            "epoch": 5.0,
            "step": 3000
        },
        {
            "loss": 0.2435,
            "grad_norm": 10.523037910461426,
            "learning_rate": 4.983333333333334e-06,
            "epoch": 5.016666666666667,
            "step": 3010
        },
        {
            "loss": 0.2367,
            "grad_norm": 2.7892112731933594,
            "learning_rate": 4.966666666666667e-06,
            "epoch": 5.033333333333333,
            "step": 3020
        },
        {
            "loss": 0.3048,
            "grad_norm": 5.897225856781006,
            "learning_rate": 4.95e-06,
            "epoch": 5.05,
            "step": 3030
        },
        {
            "loss": 0.2414,
            "grad_norm": 14.194087982177734,
            "learning_rate": 4.933333333333334e-06,
            "epoch": 5.066666666666666,
            "step": 3040
        },
        {
            "loss": 0.3164,
            "grad_norm": 16.37943458557129,
            "learning_rate": 4.9166666666666665e-06,
            "epoch": 5.083333333333333,
            "step": 3050
        },
        {
            "loss": 0.3395,
            "grad_norm": 3.7746994495391846,
            "learning_rate": 4.9000000000000005e-06,
            "epoch": 5.1,
            "step": 3060
        },
        {
            "loss": 0.1102,
            "grad_norm": 3.7376179695129395,
            "learning_rate": 4.883333333333334e-06,
            "epoch": 5.116666666666666,
            "step": 3070
        },
        {
            "loss": 0.1518,
            "grad_norm": 5.839764595031738,
            "learning_rate": 4.866666666666667e-06,
            "epoch": 5.133333333333334,
            "step": 3080
        },
        {
            "loss": 0.2188,
            "grad_norm": 10.32228946685791,
            "learning_rate": 4.85e-06,
            "epoch": 5.15,
            "step": 3090
        },
        {
            "loss": 0.2336,
            "grad_norm": 10.852478981018066,
            "learning_rate": 4.833333333333333e-06,
            "epoch": 5.166666666666667,
            "step": 3100
        },
        {
            "loss": 0.281,
            "grad_norm": 10.612698554992676,
            "learning_rate": 4.816666666666667e-06,
            "epoch": 5.183333333333334,
            "step": 3110
        },
        {
            "loss": 0.2836,
            "grad_norm": 17.619354248046875,
            "learning_rate": 4.800000000000001e-06,
            "epoch": 5.2,
            "step": 3120
        },
        {
            "loss": 0.2193,
            "grad_norm": 2.006620168685913,
            "learning_rate": 4.783333333333334e-06,
            "epoch": 5.216666666666667,
            "step": 3130
        },
        {
            "loss": 0.1751,
            "grad_norm": 10.598559379577637,
            "learning_rate": 4.766666666666667e-06,
            "epoch": 5.233333333333333,
            "step": 3140
        },
        {
            "loss": 0.1961,
            "grad_norm": 1.2625057697296143,
            "learning_rate": 4.75e-06,
            "epoch": 5.25,
            "step": 3150
        },
        {
            "loss": 0.4572,
            "grad_norm": 2.1215858459472656,
            "learning_rate": 4.7333333333333335e-06,
            "epoch": 5.266666666666667,
            "step": 3160
        },
        {
            "loss": 0.2099,
            "grad_norm": 4.9916486740112305,
            "learning_rate": 4.7166666666666675e-06,
            "epoch": 5.283333333333333,
            "step": 3170
        },
        {
            "loss": 0.3589,
            "grad_norm": 15.71414566040039,
            "learning_rate": 4.7e-06,
            "epoch": 5.3,
            "step": 3180
        },
        {
            "loss": 0.1925,
            "grad_norm": 0.8799918293952942,
            "learning_rate": 4.683333333333334e-06,
            "epoch": 5.316666666666666,
            "step": 3190
        },
        {
            "loss": 0.2444,
            "grad_norm": 5.212307929992676,
            "learning_rate": 4.666666666666667e-06,
            "epoch": 5.333333333333333,
            "step": 3200
        },
        {
            "loss": 0.3754,
            "grad_norm": 5.579143047332764,
            "learning_rate": 4.65e-06,
            "epoch": 5.35,
            "step": 3210
        },
        {
            "loss": 0.2447,
            "grad_norm": 15.067681312561035,
            "learning_rate": 4.633333333333334e-06,
            "epoch": 5.366666666666666,
            "step": 3220
        },
        {
            "loss": 0.2919,
            "grad_norm": 15.41493034362793,
            "learning_rate": 4.616666666666667e-06,
            "epoch": 5.383333333333334,
            "step": 3230
        },
        {
            "loss": 0.2571,
            "grad_norm": 0.2326660305261612,
            "learning_rate": 4.600000000000001e-06,
            "epoch": 5.4,
            "step": 3240
        },
        {
            "loss": 0.1875,
            "grad_norm": 8.608628273010254,
            "learning_rate": 4.583333333333333e-06,
            "epoch": 5.416666666666667,
            "step": 3250
        },
        {
            "loss": 0.2604,
            "grad_norm": 6.940024375915527,
            "learning_rate": 4.566666666666667e-06,
            "epoch": 5.433333333333334,
            "step": 3260
        },
        {
            "loss": 0.37,
            "grad_norm": 18.461179733276367,
            "learning_rate": 4.5500000000000005e-06,
            "epoch": 5.45,
            "step": 3270
        },
        {
            "loss": 0.1422,
            "grad_norm": 11.017404556274414,
            "learning_rate": 4.533333333333334e-06,
            "epoch": 5.466666666666667,
            "step": 3280
        },
        {
            "loss": 0.2115,
            "grad_norm": 6.984160423278809,
            "learning_rate": 4.516666666666667e-06,
            "epoch": 5.483333333333333,
            "step": 3290
        },
        {
            "loss": 0.2288,
            "grad_norm": 2.7025914192199707,
            "learning_rate": 4.5e-06,
            "epoch": 5.5,
            "step": 3300
        },
        {
            "loss": 0.1735,
            "grad_norm": 15.031702041625977,
            "learning_rate": 4.483333333333333e-06,
            "epoch": 5.516666666666667,
            "step": 3310
        },
        {
            "loss": 0.1851,
            "grad_norm": 1.4336694478988647,
            "learning_rate": 4.4666666666666665e-06,
            "epoch": 5.533333333333333,
            "step": 3320
        },
        {
            "loss": 0.1526,
            "grad_norm": 8.99441909790039,
            "learning_rate": 4.450000000000001e-06,
            "epoch": 5.55,
            "step": 3330
        },
        {
            "loss": 0.2551,
            "grad_norm": 13.842679023742676,
            "learning_rate": 4.433333333333334e-06,
            "epoch": 5.566666666666666,
            "step": 3340
        },
        {
            "loss": 0.3334,
            "grad_norm": 2.5900990962982178,
            "learning_rate": 4.416666666666667e-06,
            "epoch": 5.583333333333333,
            "step": 3350
        },
        {
            "loss": 0.2708,
            "grad_norm": 10.073884963989258,
            "learning_rate": 4.4e-06,
            "epoch": 5.6,
            "step": 3360
        },
        {
            "loss": 0.177,
            "grad_norm": 14.919837951660156,
            "learning_rate": 4.383333333333334e-06,
            "epoch": 5.616666666666667,
            "step": 3370
        },
        {
            "loss": 0.1758,
            "grad_norm": 3.1743052005767822,
            "learning_rate": 4.366666666666667e-06,
            "epoch": 5.633333333333333,
            "step": 3380
        },
        {
            "loss": 0.2466,
            "grad_norm": 1.0905649662017822,
            "learning_rate": 4.350000000000001e-06,
            "epoch": 5.65,
            "step": 3390
        },
        {
            "loss": 0.2591,
            "grad_norm": 14.271462440490723,
            "learning_rate": 4.333333333333334e-06,
            "epoch": 5.666666666666667,
            "step": 3400
        },
        {
            "loss": 0.2771,
            "grad_norm": 8.168747901916504,
            "learning_rate": 4.316666666666667e-06,
            "epoch": 5.683333333333334,
            "step": 3410
        },
        {
            "loss": 0.2586,
            "grad_norm": 17.542448043823242,
            "learning_rate": 4.3e-06,
            "epoch": 5.7,
            "step": 3420
        },
        {
            "loss": 0.2617,
            "grad_norm": 14.46296215057373,
            "learning_rate": 4.2833333333333335e-06,
            "epoch": 5.716666666666667,
            "step": 3430
        },
        {
            "loss": 0.3137,
            "grad_norm": 13.546470642089844,
            "learning_rate": 4.266666666666668e-06,
            "epoch": 5.733333333333333,
            "step": 3440
        },
        {
            "loss": 0.2075,
            "grad_norm": 10.005480766296387,
            "learning_rate": 4.25e-06,
            "epoch": 5.75,
            "step": 3450
        },
        {
            "loss": 0.2652,
            "grad_norm": 6.313272476196289,
            "learning_rate": 4.233333333333334e-06,
            "epoch": 5.766666666666667,
            "step": 3460
        },
        {
            "loss": 0.5048,
            "grad_norm": 9.809060096740723,
            "learning_rate": 4.216666666666667e-06,
            "epoch": 5.783333333333333,
            "step": 3470
        },
        {
            "loss": 0.2253,
            "grad_norm": 9.090887069702148,
            "learning_rate": 4.2000000000000004e-06,
            "epoch": 5.8,
            "step": 3480
        },
        {
            "loss": 0.2298,
            "grad_norm": 10.570164680480957,
            "learning_rate": 4.183333333333334e-06,
            "epoch": 5.816666666666666,
            "step": 3490
        },
        {
            "loss": 0.2369,
            "grad_norm": 8.444282531738281,
            "learning_rate": 4.166666666666667e-06,
            "epoch": 5.833333333333333,
            "step": 3500
        },
        {
            "loss": 0.215,
            "grad_norm": 10.701610565185547,
            "learning_rate": 4.15e-06,
            "epoch": 5.85,
            "step": 3510
        },
        {
            "loss": 0.2564,
            "grad_norm": 16.749717712402344,
            "learning_rate": 4.133333333333333e-06,
            "epoch": 5.866666666666667,
            "step": 3520
        },
        {
            "loss": 0.2521,
            "grad_norm": 0.6877553462982178,
            "learning_rate": 4.116666666666667e-06,
            "epoch": 5.883333333333333,
            "step": 3530
        },
        {
            "loss": 0.2683,
            "grad_norm": 23.29631233215332,
            "learning_rate": 4.1e-06,
            "epoch": 5.9,
            "step": 3540
        },
        {
            "loss": 0.1739,
            "grad_norm": 7.281147003173828,
            "learning_rate": 4.083333333333334e-06,
            "epoch": 5.916666666666667,
            "step": 3550
        },
        {
            "loss": 0.2878,
            "grad_norm": 1.0007047653198242,
            "learning_rate": 4.066666666666667e-06,
            "epoch": 5.933333333333334,
            "step": 3560
        },
        {
            "loss": 0.2657,
            "grad_norm": 2.579326868057251,
            "learning_rate": 4.05e-06,
            "epoch": 5.95,
            "step": 3570
        },
        {
            "loss": 0.3063,
            "grad_norm": 11.668505668640137,
            "learning_rate": 4.033333333333333e-06,
            "epoch": 5.966666666666667,
            "step": 3580
        },
        {
            "loss": 0.3364,
            "grad_norm": 18.17635154724121,
            "learning_rate": 4.0166666666666675e-06,
            "epoch": 5.983333333333333,
            "step": 3590
        },
        {
            "loss": 0.2729,
            "grad_norm": 21.9066162109375,
            "learning_rate": 4.000000000000001e-06,
            "epoch": 6.0,
            "step": 3600
        },
        {
            "eval_loss": 0.29914718866348267,
            "eval_accuracy": 0.9066666666666666,
            "eval_precision": 0.9094058456279619,
            "eval_recall": 0.9066666666666666,
            "eval_f1": 0.90698980991721,
            "eval_runtime": 163.4168,
            "eval_samples_per_second": 7.343,
            "eval_steps_per_second": 0.918,
            "epoch": 6.0,
            "step": 3600
        },
        {
            "loss": 0.2061,
            "grad_norm": 18.778493881225586,
            "learning_rate": 3.983333333333334e-06,
            "epoch": 6.016666666666667,
            "step": 3610
        },
        {
            "loss": 0.3454,
            "grad_norm": 16.634204864501953,
            "learning_rate": 3.966666666666667e-06,
            "epoch": 6.033333333333333,
            "step": 3620
        },
        {
            "loss": 0.3938,
            "grad_norm": 9.151500701904297,
            "learning_rate": 3.95e-06,
            "epoch": 6.05,
            "step": 3630
        },
        {
            "loss": 0.1651,
            "grad_norm": 6.720244884490967,
            "learning_rate": 3.9333333333333335e-06,
            "epoch": 6.066666666666666,
            "step": 3640
        },
        {
            "loss": 0.1364,
            "grad_norm": 1.5662715435028076,
            "learning_rate": 3.916666666666667e-06,
            "epoch": 6.083333333333333,
            "step": 3650
        },
        {
            "loss": 0.3601,
            "grad_norm": 9.725520133972168,
            "learning_rate": 3.900000000000001e-06,
            "epoch": 6.1,
            "step": 3660
        },
        {
            "loss": 0.2173,
            "grad_norm": 2.690300703048706,
            "learning_rate": 3.883333333333333e-06,
            "epoch": 6.116666666666666,
            "step": 3670
        },
        {
            "loss": 0.2154,
            "grad_norm": 8.910110473632812,
            "learning_rate": 3.866666666666667e-06,
            "epoch": 6.133333333333334,
            "step": 3680
        },
        {
            "loss": 0.2345,
            "grad_norm": 16.656946182250977,
            "learning_rate": 3.85e-06,
            "epoch": 6.15,
            "step": 3690
        },
        {
            "loss": 0.2074,
            "grad_norm": 17.71890640258789,
            "learning_rate": 3.833333333333334e-06,
            "epoch": 6.166666666666667,
            "step": 3700
        },
        {
            "loss": 0.1261,
            "grad_norm": 16.34333610534668,
            "learning_rate": 3.816666666666667e-06,
            "epoch": 6.183333333333334,
            "step": 3710
        },
        {
            "loss": 0.272,
            "grad_norm": 18.996397018432617,
            "learning_rate": 3.8000000000000005e-06,
            "epoch": 6.2,
            "step": 3720
        },
        {
            "loss": 0.2594,
            "grad_norm": 23.76759910583496,
            "learning_rate": 3.7833333333333337e-06,
            "epoch": 6.216666666666667,
            "step": 3730
        },
        {
            "loss": 0.1357,
            "grad_norm": 11.840847969055176,
            "learning_rate": 3.766666666666667e-06,
            "epoch": 6.233333333333333,
            "step": 3740
        },
        {
            "loss": 0.1532,
            "grad_norm": 0.5426775813102722,
            "learning_rate": 3.7500000000000005e-06,
            "epoch": 6.25,
            "step": 3750
        },
        {
            "loss": 0.2698,
            "grad_norm": 20.754226684570312,
            "learning_rate": 3.7333333333333337e-06,
            "epoch": 6.266666666666667,
            "step": 3760
        },
        {
            "loss": 0.2169,
            "grad_norm": 15.783522605895996,
            "learning_rate": 3.716666666666667e-06,
            "epoch": 6.283333333333333,
            "step": 3770
        },
        {
            "loss": 0.3591,
            "grad_norm": 31.131328582763672,
            "learning_rate": 3.7e-06,
            "epoch": 6.3,
            "step": 3780
        },
        {
            "loss": 0.2598,
            "grad_norm": 18.618196487426758,
            "learning_rate": 3.6833333333333338e-06,
            "epoch": 6.316666666666666,
            "step": 3790
        },
        {
            "loss": 0.1923,
            "grad_norm": 22.508380889892578,
            "learning_rate": 3.6666666666666666e-06,
            "epoch": 6.333333333333333,
            "step": 3800
        },
        {
            "loss": 0.1584,
            "grad_norm": 7.704029083251953,
            "learning_rate": 3.65e-06,
            "epoch": 6.35,
            "step": 3810
        },
        {
            "loss": 0.4065,
            "grad_norm": 11.092573165893555,
            "learning_rate": 3.633333333333334e-06,
            "epoch": 6.366666666666666,
            "step": 3820
        },
        {
            "loss": 0.2584,
            "grad_norm": 14.777832984924316,
            "learning_rate": 3.616666666666667e-06,
            "epoch": 6.383333333333334,
            "step": 3830
        },
        {
            "loss": 0.3203,
            "grad_norm": 10.52979850769043,
            "learning_rate": 3.6000000000000003e-06,
            "epoch": 6.4,
            "step": 3840
        },
        {
            "loss": 0.1998,
            "grad_norm": 13.594335556030273,
            "learning_rate": 3.5833333333333335e-06,
            "epoch": 6.416666666666667,
            "step": 3850
        },
        {
            "loss": 0.1582,
            "grad_norm": 0.9108844995498657,
            "learning_rate": 3.566666666666667e-06,
            "epoch": 6.433333333333334,
            "step": 3860
        },
        {
            "loss": 0.2821,
            "grad_norm": 14.687895774841309,
            "learning_rate": 3.5500000000000003e-06,
            "epoch": 6.45,
            "step": 3870
        },
        {
            "loss": 0.2925,
            "grad_norm": 17.461530685424805,
            "learning_rate": 3.5333333333333335e-06,
            "epoch": 6.466666666666667,
            "step": 3880
        },
        {
            "loss": 0.1033,
            "grad_norm": 2.4466872215270996,
            "learning_rate": 3.516666666666667e-06,
            "epoch": 6.483333333333333,
            "step": 3890
        },
        {
            "loss": 0.1612,
            "grad_norm": 5.154117107391357,
            "learning_rate": 3.5e-06,
            "epoch": 6.5,
            "step": 3900
        },
        {
            "loss": 0.3423,
            "grad_norm": 3.9305410385131836,
            "learning_rate": 3.4833333333333336e-06,
            "epoch": 6.516666666666667,
            "step": 3910
        },
        {
            "loss": 0.4203,
            "grad_norm": 10.826456069946289,
            "learning_rate": 3.4666666666666672e-06,
            "epoch": 6.533333333333333,
            "step": 3920
        },
        {
            "loss": 0.332,
            "grad_norm": 0.5831263065338135,
            "learning_rate": 3.45e-06,
            "epoch": 6.55,
            "step": 3930
        },
        {
            "loss": 0.2422,
            "grad_norm": 13.32477855682373,
            "learning_rate": 3.4333333333333336e-06,
            "epoch": 6.566666666666666,
            "step": 3940
        },
        {
            "loss": 0.4045,
            "grad_norm": 14.771601676940918,
            "learning_rate": 3.416666666666667e-06,
            "epoch": 6.583333333333333,
            "step": 3950
        },
        {
            "loss": 0.1626,
            "grad_norm": 9.104538917541504,
            "learning_rate": 3.4000000000000005e-06,
            "epoch": 6.6,
            "step": 3960
        },
        {
            "loss": 0.1696,
            "grad_norm": 1.8295278549194336,
            "learning_rate": 3.3833333333333333e-06,
            "epoch": 6.616666666666667,
            "step": 3970
        },
        {
            "loss": 0.2766,
            "grad_norm": 8.154358863830566,
            "learning_rate": 3.366666666666667e-06,
            "epoch": 6.633333333333333,
            "step": 3980
        },
        {
            "loss": 0.2165,
            "grad_norm": 10.483407974243164,
            "learning_rate": 3.3500000000000005e-06,
            "epoch": 6.65,
            "step": 3990
        },
        {
            "loss": 0.3065,
            "grad_norm": 1.7930915355682373,
            "learning_rate": 3.3333333333333333e-06,
            "epoch": 6.666666666666667,
            "step": 4000
        },
        {
            "loss": 0.2068,
            "grad_norm": 9.424504280090332,
            "learning_rate": 3.316666666666667e-06,
            "epoch": 6.683333333333334,
            "step": 4010
        },
        {
            "loss": 0.3231,
            "grad_norm": 13.658639907836914,
            "learning_rate": 3.3000000000000006e-06,
            "epoch": 6.7,
            "step": 4020
        },
        {
            "loss": 0.1372,
            "grad_norm": 1.639115333557129,
            "learning_rate": 3.2833333333333334e-06,
            "epoch": 6.716666666666667,
            "step": 4030
        },
        {
            "loss": 0.1773,
            "grad_norm": 20.305688858032227,
            "learning_rate": 3.266666666666667e-06,
            "epoch": 6.733333333333333,
            "step": 4040
        },
        {
            "loss": 0.2632,
            "grad_norm": 2.6217293739318848,
            "learning_rate": 3.2500000000000002e-06,
            "epoch": 6.75,
            "step": 4050
        },
        {
            "loss": 0.2058,
            "grad_norm": 0.9385153651237488,
            "learning_rate": 3.2333333333333334e-06,
            "epoch": 6.766666666666667,
            "step": 4060
        },
        {
            "loss": 0.433,
            "grad_norm": 13.409192085266113,
            "learning_rate": 3.2166666666666666e-06,
            "epoch": 6.783333333333333,
            "step": 4070
        },
        {
            "loss": 0.2646,
            "grad_norm": 16.630237579345703,
            "learning_rate": 3.2000000000000003e-06,
            "epoch": 6.8,
            "step": 4080
        },
        {
            "loss": 0.4707,
            "grad_norm": 11.494707107543945,
            "learning_rate": 3.183333333333334e-06,
            "epoch": 6.816666666666666,
            "step": 4090
        },
        {
            "loss": 0.2145,
            "grad_norm": 0.2871963381767273,
            "learning_rate": 3.1666666666666667e-06,
            "epoch": 6.833333333333333,
            "step": 4100
        },
        {
            "loss": 0.1981,
            "grad_norm": 0.21863719820976257,
            "learning_rate": 3.1500000000000003e-06,
            "epoch": 6.85,
            "step": 4110
        },
        {
            "loss": 0.2174,
            "grad_norm": 10.519652366638184,
            "learning_rate": 3.133333333333334e-06,
            "epoch": 6.866666666666667,
            "step": 4120
        },
        {
            "loss": 0.2761,
            "grad_norm": 6.169397830963135,
            "learning_rate": 3.1166666666666668e-06,
            "epoch": 6.883333333333333,
            "step": 4130
        },
        {
            "loss": 0.2033,
            "grad_norm": 4.695978164672852,
            "learning_rate": 3.1000000000000004e-06,
            "epoch": 6.9,
            "step": 4140
        },
        {
            "loss": 0.1477,
            "grad_norm": 10.76511001586914,
            "learning_rate": 3.0833333333333336e-06,
            "epoch": 6.916666666666667,
            "step": 4150
        },
        {
            "loss": 0.2444,
            "grad_norm": 27.53693962097168,
            "learning_rate": 3.066666666666667e-06,
            "epoch": 6.933333333333334,
            "step": 4160
        },
        {
            "loss": 0.1794,
            "grad_norm": 6.6335320472717285,
            "learning_rate": 3.05e-06,
            "epoch": 6.95,
            "step": 4170
        },
        {
            "loss": 0.2438,
            "grad_norm": 2.935903787612915,
            "learning_rate": 3.0333333333333337e-06,
            "epoch": 6.966666666666667,
            "step": 4180
        },
        {
            "loss": 0.3163,
            "grad_norm": 0.2995243966579437,
            "learning_rate": 3.0166666666666673e-06,
            "epoch": 6.983333333333333,
            "step": 4190
        },
        {
            "loss": 0.0957,
            "grad_norm": 3.052353620529175,
            "learning_rate": 3e-06,
            "epoch": 7.0,
            "step": 4200
        },
        {
            "eval_loss": 0.3022059500217438,
            "eval_accuracy": 0.91,
            "eval_precision": 0.9134130460852781,
            "eval_recall": 0.91,
            "eval_f1": 0.9103954294502354,
            "eval_runtime": 162.4039,
            "eval_samples_per_second": 7.389,
            "eval_steps_per_second": 0.924,
            "epoch": 7.0,
            "step": 4200
        },
        {
            "loss": 0.3843,
            "grad_norm": 12.01709270477295,
            "learning_rate": 2.9833333333333337e-06,
            "epoch": 7.016666666666667,
            "step": 4210
        },
        {
            "loss": 0.194,
            "grad_norm": 8.493707656860352,
            "learning_rate": 2.9666666666666673e-06,
            "epoch": 7.033333333333333,
            "step": 4220
        },
        {
            "loss": 0.2657,
            "grad_norm": 14.437027931213379,
            "learning_rate": 2.95e-06,
            "epoch": 7.05,
            "step": 4230
        },
        {
            "loss": 0.3041,
            "grad_norm": 16.21970558166504,
            "learning_rate": 2.9333333333333338e-06,
            "epoch": 7.066666666666666,
            "step": 4240
        },
        {
            "loss": 0.2421,
            "grad_norm": 30.44116973876953,
            "learning_rate": 2.916666666666667e-06,
            "epoch": 7.083333333333333,
            "step": 4250
        },
        {
            "loss": 0.2998,
            "grad_norm": 10.816898345947266,
            "learning_rate": 2.9e-06,
            "epoch": 7.1,
            "step": 4260
        },
        {
            "loss": 0.3077,
            "grad_norm": 10.39533519744873,
            "learning_rate": 2.8833333333333334e-06,
            "epoch": 7.116666666666666,
            "step": 4270
        },
        {
            "loss": 0.1469,
            "grad_norm": 2.152674674987793,
            "learning_rate": 2.866666666666667e-06,
            "epoch": 7.133333333333334,
            "step": 4280
        },
        {
            "loss": 0.1457,
            "grad_norm": 1.8220542669296265,
            "learning_rate": 2.85e-06,
            "epoch": 7.15,
            "step": 4290
        },
        {
            "loss": 0.2834,
            "grad_norm": 15.408492088317871,
            "learning_rate": 2.8333333333333335e-06,
            "epoch": 7.166666666666667,
            "step": 4300
        },
        {
            "loss": 0.3595,
            "grad_norm": 16.487606048583984,
            "learning_rate": 2.816666666666667e-06,
            "epoch": 7.183333333333334,
            "step": 4310
        },
        {
            "loss": 0.0932,
            "grad_norm": 1.307471752166748,
            "learning_rate": 2.8000000000000003e-06,
            "epoch": 7.2,
            "step": 4320
        },
        {
            "loss": 0.1789,
            "grad_norm": 3.247194766998291,
            "learning_rate": 2.7833333333333335e-06,
            "epoch": 7.216666666666667,
            "step": 4330
        },
        {
            "loss": 0.281,
            "grad_norm": 20.06399154663086,
            "learning_rate": 2.766666666666667e-06,
            "epoch": 7.233333333333333,
            "step": 4340
        },
        {
            "loss": 0.1375,
            "grad_norm": 8.078242301940918,
            "learning_rate": 2.7500000000000004e-06,
            "epoch": 7.25,
            "step": 4350
        },
        {
            "loss": 0.1634,
            "grad_norm": 9.713712692260742,
            "learning_rate": 2.7333333333333336e-06,
            "epoch": 7.266666666666667,
            "step": 4360
        },
        {
            "loss": 0.1882,
            "grad_norm": 29.14756965637207,
            "learning_rate": 2.7166666666666668e-06,
            "epoch": 7.283333333333333,
            "step": 4370
        },
        {
            "loss": 0.1148,
            "grad_norm": 7.0339765548706055,
            "learning_rate": 2.7000000000000004e-06,
            "epoch": 7.3,
            "step": 4380
        },
        {
            "loss": 0.34,
            "grad_norm": 23.376636505126953,
            "learning_rate": 2.683333333333333e-06,
            "epoch": 7.316666666666666,
            "step": 4390
        },
        {
            "loss": 0.2414,
            "grad_norm": 3.3337926864624023,
            "learning_rate": 2.666666666666667e-06,
            "epoch": 7.333333333333333,
            "step": 4400
        },
        {
            "loss": 0.2432,
            "grad_norm": 13.949480056762695,
            "learning_rate": 2.6500000000000005e-06,
            "epoch": 7.35,
            "step": 4410
        },
        {
            "loss": 0.2254,
            "grad_norm": 21.470291137695312,
            "learning_rate": 2.6333333333333332e-06,
            "epoch": 7.366666666666666,
            "step": 4420
        },
        {
            "loss": 0.4303,
            "grad_norm": 16.58213996887207,
            "learning_rate": 2.616666666666667e-06,
            "epoch": 7.383333333333334,
            "step": 4430
        },
        {
            "loss": 0.132,
            "grad_norm": 14.704232215881348,
            "learning_rate": 2.6e-06,
            "epoch": 7.4,
            "step": 4440
        },
        {
            "loss": 0.1351,
            "grad_norm": 0.7235897779464722,
            "learning_rate": 2.5833333333333337e-06,
            "epoch": 7.416666666666667,
            "step": 4450
        },
        {
            "loss": 0.2259,
            "grad_norm": 1.6833919286727905,
            "learning_rate": 2.566666666666667e-06,
            "epoch": 7.433333333333334,
            "step": 4460
        },
        {
            "loss": 0.1841,
            "grad_norm": 0.4024644196033478,
            "learning_rate": 2.55e-06,
            "epoch": 7.45,
            "step": 4470
        },
        {
            "loss": 0.2944,
            "grad_norm": 2.4754250049591064,
            "learning_rate": 2.5333333333333338e-06,
            "epoch": 7.466666666666667,
            "step": 4480
        },
        {
            "loss": 0.3939,
            "grad_norm": 0.3174735903739929,
            "learning_rate": 2.5166666666666666e-06,
            "epoch": 7.483333333333333,
            "step": 4490
        },
        {
            "loss": 0.1889,
            "grad_norm": 10.323551177978516,
            "learning_rate": 2.5e-06,
            "epoch": 7.5,
            "step": 4500
        },
        {
            "loss": 0.2272,
            "grad_norm": 1.2738114595413208,
            "learning_rate": 2.4833333333333334e-06,
            "epoch": 7.516666666666667,
            "step": 4510
        },
        {
            "loss": 0.1562,
            "grad_norm": 21.93187141418457,
            "learning_rate": 2.466666666666667e-06,
            "epoch": 7.533333333333333,
            "step": 4520
        },
        {
            "loss": 0.3584,
            "grad_norm": 23.280778884887695,
            "learning_rate": 2.4500000000000003e-06,
            "epoch": 7.55,
            "step": 4530
        },
        {
            "loss": 0.4623,
            "grad_norm": 23.80376625061035,
            "learning_rate": 2.4333333333333335e-06,
            "epoch": 7.566666666666666,
            "step": 4540
        },
        {
            "loss": 0.1904,
            "grad_norm": 18.467700958251953,
            "learning_rate": 2.4166666666666667e-06,
            "epoch": 7.583333333333333,
            "step": 4550
        },
        {
            "loss": 0.2171,
            "grad_norm": 9.076483726501465,
            "learning_rate": 2.4000000000000003e-06,
            "epoch": 7.6,
            "step": 4560
        },
        {
            "loss": 0.2126,
            "grad_norm": 1.3404887914657593,
            "learning_rate": 2.3833333333333335e-06,
            "epoch": 7.616666666666667,
            "step": 4570
        },
        {
            "loss": 0.1735,
            "grad_norm": 9.279133796691895,
            "learning_rate": 2.3666666666666667e-06,
            "epoch": 7.633333333333333,
            "step": 4580
        },
        {
            "loss": 0.2683,
            "grad_norm": 8.530689239501953,
            "learning_rate": 2.35e-06,
            "epoch": 7.65,
            "step": 4590
        },
        {
            "loss": 0.3193,
            "grad_norm": 14.380796432495117,
            "learning_rate": 2.3333333333333336e-06,
            "epoch": 7.666666666666667,
            "step": 4600
        },
        {
            "loss": 0.1291,
            "grad_norm": 2.254178762435913,
            "learning_rate": 2.316666666666667e-06,
            "epoch": 7.683333333333334,
            "step": 4610
        },
        {
            "loss": 0.1982,
            "grad_norm": 7.521166801452637,
            "learning_rate": 2.3000000000000004e-06,
            "epoch": 7.7,
            "step": 4620
        },
        {
            "loss": 0.2219,
            "grad_norm": 12.646183967590332,
            "learning_rate": 2.2833333333333336e-06,
            "epoch": 7.716666666666667,
            "step": 4630
        },
        {
            "loss": 0.2245,
            "grad_norm": 19.424116134643555,
            "learning_rate": 2.266666666666667e-06,
            "epoch": 7.733333333333333,
            "step": 4640
        },
        {
            "loss": 0.2326,
            "grad_norm": 12.013113021850586,
            "learning_rate": 2.25e-06,
            "epoch": 7.75,
            "step": 4650
        },
        {
            "loss": 0.4226,
            "grad_norm": 19.34074592590332,
            "learning_rate": 2.2333333333333333e-06,
            "epoch": 7.766666666666667,
            "step": 4660
        },
        {
            "loss": 0.1709,
            "grad_norm": 2.455848455429077,
            "learning_rate": 2.216666666666667e-06,
            "epoch": 7.783333333333333,
            "step": 4670
        },
        {
            "loss": 0.1876,
            "grad_norm": 12.820414543151855,
            "learning_rate": 2.2e-06,
            "epoch": 7.8,
            "step": 4680
        },
        {
            "loss": 0.2587,
            "grad_norm": 13.87507152557373,
            "learning_rate": 2.1833333333333333e-06,
            "epoch": 7.816666666666666,
            "step": 4690
        },
        {
            "loss": 0.0829,
            "grad_norm": 8.213235855102539,
            "learning_rate": 2.166666666666667e-06,
            "epoch": 7.833333333333333,
            "step": 4700
        },
        {
            "loss": 0.2442,
            "grad_norm": 17.793733596801758,
            "learning_rate": 2.15e-06,
            "epoch": 7.85,
            "step": 4710
        },
        {
            "loss": 0.2584,
            "grad_norm": 1.0069853067398071,
            "learning_rate": 2.133333333333334e-06,
            "epoch": 7.866666666666667,
            "step": 4720
        },
        {
            "loss": 0.3895,
            "grad_norm": 14.85412883758545,
            "learning_rate": 2.116666666666667e-06,
            "epoch": 7.883333333333333,
            "step": 4730
        },
        {
            "loss": 0.2958,
            "grad_norm": 19.564558029174805,
            "learning_rate": 2.1000000000000002e-06,
            "epoch": 7.9,
            "step": 4740
        },
        {
            "loss": 0.1865,
            "grad_norm": 11.627650260925293,
            "learning_rate": 2.0833333333333334e-06,
            "epoch": 7.916666666666667,
            "step": 4750
        },
        {
            "loss": 0.4249,
            "grad_norm": 12.247771263122559,
            "learning_rate": 2.0666666666666666e-06,
            "epoch": 7.933333333333334,
            "step": 4760
        },
        {
            "loss": 0.1266,
            "grad_norm": 15.63188648223877,
            "learning_rate": 2.05e-06,
            "epoch": 7.95,
            "step": 4770
        },
        {
            "loss": 0.3771,
            "grad_norm": 11.05239486694336,
            "learning_rate": 2.0333333333333335e-06,
            "epoch": 7.966666666666667,
            "step": 4780
        },
        {
            "loss": 0.1965,
            "grad_norm": 19.139694213867188,
            "learning_rate": 2.0166666666666667e-06,
            "epoch": 7.983333333333333,
            "step": 4790
        },
        {
            "loss": 0.1631,
            "grad_norm": 0.5913451313972473,
            "learning_rate": 2.0000000000000003e-06,
            "epoch": 8.0,
            "step": 4800
        },
        {
            "eval_loss": 0.2986871302127838,
            "eval_accuracy": 0.9091666666666667,
            "eval_precision": 0.911649814350284,
            "eval_recall": 0.9091666666666667,
            "eval_f1": 0.9094096886483908,
            "eval_runtime": 160.4278,
            "eval_samples_per_second": 7.48,
            "eval_steps_per_second": 0.935,
            "epoch": 8.0,
            "step": 4800
        },
        {
            "loss": 0.2136,
            "grad_norm": 27.801095962524414,
            "learning_rate": 1.9833333333333335e-06,
            "epoch": 8.016666666666667,
            "step": 4810
        },
        {
            "loss": 0.159,
            "grad_norm": 15.795966148376465,
            "learning_rate": 1.9666666666666668e-06,
            "epoch": 8.033333333333333,
            "step": 4820
        },
        {
            "loss": 0.2619,
            "grad_norm": 15.357419967651367,
            "learning_rate": 1.9500000000000004e-06,
            "epoch": 8.05,
            "step": 4830
        },
        {
            "loss": 0.3675,
            "grad_norm": 11.987998008728027,
            "learning_rate": 1.9333333333333336e-06,
            "epoch": 8.066666666666666,
            "step": 4840
        },
        {
            "loss": 0.261,
            "grad_norm": 11.578689575195312,
            "learning_rate": 1.916666666666667e-06,
            "epoch": 8.083333333333334,
            "step": 4850
        },
        {
            "loss": 0.2861,
            "grad_norm": 1.603281021118164,
            "learning_rate": 1.9000000000000002e-06,
            "epoch": 8.1,
            "step": 4860
        },
        {
            "loss": 0.187,
            "grad_norm": 0.9889153838157654,
            "learning_rate": 1.8833333333333334e-06,
            "epoch": 8.116666666666667,
            "step": 4870
        },
        {
            "loss": 0.1654,
            "grad_norm": 13.533662796020508,
            "learning_rate": 1.8666666666666669e-06,
            "epoch": 8.133333333333333,
            "step": 4880
        },
        {
            "loss": 0.1552,
            "grad_norm": 15.215307235717773,
            "learning_rate": 1.85e-06,
            "epoch": 8.15,
            "step": 4890
        },
        {
            "loss": 0.2233,
            "grad_norm": 14.344466209411621,
            "learning_rate": 1.8333333333333333e-06,
            "epoch": 8.166666666666666,
            "step": 4900
        },
        {
            "loss": 0.2409,
            "grad_norm": 0.9030619859695435,
            "learning_rate": 1.816666666666667e-06,
            "epoch": 8.183333333333334,
            "step": 4910
        },
        {
            "loss": 0.2313,
            "grad_norm": 19.334335327148438,
            "learning_rate": 1.8000000000000001e-06,
            "epoch": 8.2,
            "step": 4920
        },
        {
            "loss": 0.1498,
            "grad_norm": 22.613962173461914,
            "learning_rate": 1.7833333333333336e-06,
            "epoch": 8.216666666666667,
            "step": 4930
        },
        {
            "loss": 0.4272,
            "grad_norm": 7.284353256225586,
            "learning_rate": 1.7666666666666668e-06,
            "epoch": 8.233333333333333,
            "step": 4940
        },
        {
            "loss": 0.2894,
            "grad_norm": 11.2527437210083,
            "learning_rate": 1.75e-06,
            "epoch": 8.25,
            "step": 4950
        },
        {
            "loss": 0.2417,
            "grad_norm": 10.30331802368164,
            "learning_rate": 1.7333333333333336e-06,
            "epoch": 8.266666666666667,
            "step": 4960
        },
        {
            "loss": 0.1518,
            "grad_norm": 5.857719421386719,
            "learning_rate": 1.7166666666666668e-06,
            "epoch": 8.283333333333333,
            "step": 4970
        },
        {
            "loss": 0.0973,
            "grad_norm": 8.076013565063477,
            "learning_rate": 1.7000000000000002e-06,
            "epoch": 8.3,
            "step": 4980
        },
        {
            "loss": 0.195,
            "grad_norm": 11.265681266784668,
            "learning_rate": 1.6833333333333335e-06,
            "epoch": 8.316666666666666,
            "step": 4990
        },
        {
            "loss": 0.0902,
            "grad_norm": 0.3253473937511444,
            "learning_rate": 1.6666666666666667e-06,
            "epoch": 8.333333333333334,
            "step": 5000
        },
        {
            "loss": 0.1682,
            "grad_norm": 9.991375923156738,
            "learning_rate": 1.6500000000000003e-06,
            "epoch": 8.35,
            "step": 5010
        },
        {
            "loss": 0.216,
            "grad_norm": 8.27320384979248,
            "learning_rate": 1.6333333333333335e-06,
            "epoch": 8.366666666666667,
            "step": 5020
        },
        {
            "loss": 0.1741,
            "grad_norm": 19.17234230041504,
            "learning_rate": 1.6166666666666667e-06,
            "epoch": 8.383333333333333,
            "step": 5030
        },
        {
            "loss": 0.3718,
            "grad_norm": 15.511192321777344,
            "learning_rate": 1.6000000000000001e-06,
            "epoch": 8.4,
            "step": 5040
        },
        {
            "loss": 0.3033,
            "grad_norm": 1.8456993103027344,
            "learning_rate": 1.5833333333333333e-06,
            "epoch": 8.416666666666666,
            "step": 5050
        },
        {
            "loss": 0.1829,
            "grad_norm": 0.3362736105918884,
            "learning_rate": 1.566666666666667e-06,
            "epoch": 8.433333333333334,
            "step": 5060
        },
        {
            "loss": 0.193,
            "grad_norm": 1.7420693635940552,
            "learning_rate": 1.5500000000000002e-06,
            "epoch": 8.45,
            "step": 5070
        },
        {
            "loss": 0.2723,
            "grad_norm": 13.938892364501953,
            "learning_rate": 1.5333333333333334e-06,
            "epoch": 8.466666666666667,
            "step": 5080
        },
        {
            "loss": 0.2232,
            "grad_norm": 0.37549901008605957,
            "learning_rate": 1.5166666666666668e-06,
            "epoch": 8.483333333333333,
            "step": 5090
        },
        {
            "loss": 0.2782,
            "grad_norm": 2.780587911605835,
            "learning_rate": 1.5e-06,
            "epoch": 8.5,
            "step": 5100
        },
        {
            "loss": 0.3042,
            "grad_norm": 0.33644822239875793,
            "learning_rate": 1.4833333333333337e-06,
            "epoch": 8.516666666666667,
            "step": 5110
        },
        {
            "loss": 0.1153,
            "grad_norm": 3.284601926803589,
            "learning_rate": 1.4666666666666669e-06,
            "epoch": 8.533333333333333,
            "step": 5120
        },
        {
            "loss": 0.2952,
            "grad_norm": 4.037311553955078,
            "learning_rate": 1.45e-06,
            "epoch": 8.55,
            "step": 5130
        },
        {
            "loss": 0.3299,
            "grad_norm": 0.7970399856567383,
            "learning_rate": 1.4333333333333335e-06,
            "epoch": 8.566666666666666,
            "step": 5140
        },
        {
            "loss": 0.159,
            "grad_norm": 14.127250671386719,
            "learning_rate": 1.4166666666666667e-06,
            "epoch": 8.583333333333334,
            "step": 5150
        },
        {
            "loss": 0.2506,
            "grad_norm": 8.552166938781738,
            "learning_rate": 1.4000000000000001e-06,
            "epoch": 8.6,
            "step": 5160
        },
        {
            "loss": 0.1622,
            "grad_norm": 1.864323377609253,
            "learning_rate": 1.3833333333333336e-06,
            "epoch": 8.616666666666667,
            "step": 5170
        },
        {
            "loss": 0.1912,
            "grad_norm": 2.348207712173462,
            "learning_rate": 1.3666666666666668e-06,
            "epoch": 8.633333333333333,
            "step": 5180
        },
        {
            "loss": 0.2351,
            "grad_norm": 17.40170669555664,
            "learning_rate": 1.3500000000000002e-06,
            "epoch": 8.65,
            "step": 5190
        },
        {
            "loss": 0.1517,
            "grad_norm": 3.014094829559326,
            "learning_rate": 1.3333333333333334e-06,
            "epoch": 8.666666666666666,
            "step": 5200
        },
        {
            "loss": 0.1169,
            "grad_norm": 28.044727325439453,
            "learning_rate": 1.3166666666666666e-06,
            "epoch": 8.683333333333334,
            "step": 5210
        },
        {
            "loss": 0.1209,
            "grad_norm": 13.993327140808105,
            "learning_rate": 1.3e-06,
            "epoch": 8.7,
            "step": 5220
        },
        {
            "loss": 0.2256,
            "grad_norm": 0.5870344638824463,
            "learning_rate": 1.2833333333333335e-06,
            "epoch": 8.716666666666667,
            "step": 5230
        },
        {
            "loss": 0.263,
            "grad_norm": 2.5885074138641357,
            "learning_rate": 1.2666666666666669e-06,
            "epoch": 8.733333333333333,
            "step": 5240
        },
        {
            "loss": 0.2076,
            "grad_norm": 9.161056518554688,
            "learning_rate": 1.25e-06,
            "epoch": 8.75,
            "step": 5250
        },
        {
            "loss": 0.0736,
            "grad_norm": 10.19121265411377,
            "learning_rate": 1.2333333333333335e-06,
            "epoch": 8.766666666666667,
            "step": 5260
        },
        {
            "loss": 0.2622,
            "grad_norm": 14.716716766357422,
            "learning_rate": 1.2166666666666667e-06,
            "epoch": 8.783333333333333,
            "step": 5270
        },
        {
            "loss": 0.2201,
            "grad_norm": 15.682977676391602,
            "learning_rate": 1.2000000000000002e-06,
            "epoch": 8.8,
            "step": 5280
        },
        {
            "loss": 0.1746,
            "grad_norm": 11.187527656555176,
            "learning_rate": 1.1833333333333334e-06,
            "epoch": 8.816666666666666,
            "step": 5290
        },
        {
            "loss": 0.2757,
            "grad_norm": 1.898780107498169,
            "learning_rate": 1.1666666666666668e-06,
            "epoch": 8.833333333333334,
            "step": 5300
        },
        {
            "loss": 0.4373,
            "grad_norm": 27.567407608032227,
            "learning_rate": 1.1500000000000002e-06,
            "epoch": 8.85,
            "step": 5310
        },
        {
            "loss": 0.114,
            "grad_norm": 12.026131629943848,
            "learning_rate": 1.1333333333333334e-06,
            "epoch": 8.866666666666667,
            "step": 5320
        },
        {
            "loss": 0.1703,
            "grad_norm": 12.99587345123291,
            "learning_rate": 1.1166666666666666e-06,
            "epoch": 8.883333333333333,
            "step": 5330
        },
        {
            "loss": 0.2903,
            "grad_norm": 9.759332656860352,
            "learning_rate": 1.1e-06,
            "epoch": 8.9,
            "step": 5340
        },
        {
            "loss": 0.2399,
            "grad_norm": 20.345727920532227,
            "learning_rate": 1.0833333333333335e-06,
            "epoch": 8.916666666666666,
            "step": 5350
        },
        {
            "loss": 0.2542,
            "grad_norm": 0.14096267521381378,
            "learning_rate": 1.066666666666667e-06,
            "epoch": 8.933333333333334,
            "step": 5360
        },
        {
            "loss": 0.1642,
            "grad_norm": 0.7965407967567444,
            "learning_rate": 1.0500000000000001e-06,
            "epoch": 8.95,
            "step": 5370
        },
        {
            "loss": 0.3438,
            "grad_norm": 19.763057708740234,
            "learning_rate": 1.0333333333333333e-06,
            "epoch": 8.966666666666667,
            "step": 5380
        },
        {
            "loss": 0.3545,
            "grad_norm": 10.541882514953613,
            "learning_rate": 1.0166666666666667e-06,
            "epoch": 8.983333333333333,
            "step": 5390
        },
        {
            "loss": 0.3859,
            "grad_norm": 17.835803985595703,
            "learning_rate": 1.0000000000000002e-06,
            "epoch": 9.0,
            "step": 5400
        },
        {
            "eval_loss": 0.2969299256801605,
            "eval_accuracy": 0.9116666666666666,
            "eval_precision": 0.9142521773735073,
            "eval_recall": 0.9116666666666666,
            "eval_f1": 0.9119584346167087,
            "eval_runtime": 159.3726,
            "eval_samples_per_second": 7.53,
            "eval_steps_per_second": 0.941,
            "epoch": 9.0,
            "step": 5400
        },
        {
            "loss": 0.2668,
            "grad_norm": 17.51555824279785,
            "learning_rate": 9.833333333333334e-07,
            "epoch": 9.016666666666667,
            "step": 5410
        },
        {
            "loss": 0.3187,
            "grad_norm": 12.036752700805664,
            "learning_rate": 9.666666666666668e-07,
            "epoch": 9.033333333333333,
            "step": 5420
        },
        {
            "loss": 0.2059,
            "grad_norm": 1.493148684501648,
            "learning_rate": 9.500000000000001e-07,
            "epoch": 9.05,
            "step": 5430
        },
        {
            "loss": 0.3202,
            "grad_norm": 15.44321346282959,
            "learning_rate": 9.333333333333334e-07,
            "epoch": 9.066666666666666,
            "step": 5440
        },
        {
            "loss": 0.1167,
            "grad_norm": 3.1214168071746826,
            "learning_rate": 9.166666666666666e-07,
            "epoch": 9.083333333333334,
            "step": 5450
        },
        {
            "loss": 0.2119,
            "grad_norm": 17.959413528442383,
            "learning_rate": 9.000000000000001e-07,
            "epoch": 9.1,
            "step": 5460
        },
        {
            "loss": 0.2957,
            "grad_norm": 8.63185977935791,
            "learning_rate": 8.833333333333334e-07,
            "epoch": 9.116666666666667,
            "step": 5470
        },
        {
            "loss": 0.2499,
            "grad_norm": 6.5519890785217285,
            "learning_rate": 8.666666666666668e-07,
            "epoch": 9.133333333333333,
            "step": 5480
        },
        {
            "loss": 0.2345,
            "grad_norm": 0.504207193851471,
            "learning_rate": 8.500000000000001e-07,
            "epoch": 9.15,
            "step": 5490
        },
        {
            "loss": 0.2361,
            "grad_norm": 7.523524284362793,
            "learning_rate": 8.333333333333333e-07,
            "epoch": 9.166666666666666,
            "step": 5500
        },
        {
            "loss": 0.163,
            "grad_norm": 6.165937423706055,
            "learning_rate": 8.166666666666668e-07,
            "epoch": 9.183333333333334,
            "step": 5510
        },
        {
            "loss": 0.3764,
            "grad_norm": 10.233673095703125,
            "learning_rate": 8.000000000000001e-07,
            "epoch": 9.2,
            "step": 5520
        },
        {
            "loss": 0.2066,
            "grad_norm": 26.75025749206543,
            "learning_rate": 7.833333333333335e-07,
            "epoch": 9.216666666666667,
            "step": 5530
        },
        {
            "loss": 0.1866,
            "grad_norm": 11.992746353149414,
            "learning_rate": 7.666666666666667e-07,
            "epoch": 9.233333333333333,
            "step": 5540
        },
        {
            "loss": 0.2403,
            "grad_norm": 8.279459953308105,
            "learning_rate": 7.5e-07,
            "epoch": 9.25,
            "step": 5550
        },
        {
            "loss": 0.1739,
            "grad_norm": 13.763640403747559,
            "learning_rate": 7.333333333333334e-07,
            "epoch": 9.266666666666667,
            "step": 5560
        },
        {
            "loss": 0.1815,
            "grad_norm": 2.0449745655059814,
            "learning_rate": 7.166666666666668e-07,
            "epoch": 9.283333333333333,
            "step": 5570
        },
        {
            "loss": 0.3066,
            "grad_norm": 16.300260543823242,
            "learning_rate": 7.000000000000001e-07,
            "epoch": 9.3,
            "step": 5580
        },
        {
            "loss": 0.3269,
            "grad_norm": 4.301780700683594,
            "learning_rate": 6.833333333333334e-07,
            "epoch": 9.316666666666666,
            "step": 5590
        },
        {
            "loss": 0.3209,
            "grad_norm": 20.34726905822754,
            "learning_rate": 6.666666666666667e-07,
            "epoch": 9.333333333333334,
            "step": 5600
        },
        {
            "loss": 0.2994,
            "grad_norm": 10.319707870483398,
            "learning_rate": 6.5e-07,
            "epoch": 9.35,
            "step": 5610
        },
        {
            "loss": 0.1979,
            "grad_norm": 2.4674952030181885,
            "learning_rate": 6.333333333333334e-07,
            "epoch": 9.366666666666667,
            "step": 5620
        },
        {
            "loss": 0.4286,
            "grad_norm": 0.5826043486595154,
            "learning_rate": 6.166666666666668e-07,
            "epoch": 9.383333333333333,
            "step": 5630
        },
        {
            "loss": 0.1322,
            "grad_norm": 16.716083526611328,
            "learning_rate": 6.000000000000001e-07,
            "epoch": 9.4,
            "step": 5640
        },
        {
            "loss": 0.0996,
            "grad_norm": 5.239623069763184,
            "learning_rate": 5.833333333333334e-07,
            "epoch": 9.416666666666666,
            "step": 5650
        },
        {
            "loss": 0.0935,
            "grad_norm": 0.6128754615783691,
            "learning_rate": 5.666666666666667e-07,
            "epoch": 9.433333333333334,
            "step": 5660
        },
        {
            "loss": 0.2197,
            "grad_norm": 20.639995574951172,
            "learning_rate": 5.5e-07,
            "epoch": 9.45,
            "step": 5670
        },
        {
            "loss": 0.1954,
            "grad_norm": 1.4261648654937744,
            "learning_rate": 5.333333333333335e-07,
            "epoch": 9.466666666666667,
            "step": 5680
        },
        {
            "loss": 0.1017,
            "grad_norm": 16.63800621032715,
            "learning_rate": 5.166666666666667e-07,
            "epoch": 9.483333333333333,
            "step": 5690
        },
        {
            "loss": 0.1315,
            "grad_norm": 1.3355087041854858,
            "learning_rate": 5.000000000000001e-07,
            "epoch": 9.5,
            "step": 5700
        },
        {
            "loss": 0.27,
            "grad_norm": 0.310324102640152,
            "learning_rate": 4.833333333333334e-07,
            "epoch": 9.516666666666667,
            "step": 5710
        },
        {
            "loss": 0.2313,
            "grad_norm": 9.154842376708984,
            "learning_rate": 4.666666666666667e-07,
            "epoch": 9.533333333333333,
            "step": 5720
        },
        {
            "loss": 0.1691,
            "grad_norm": 0.9220123291015625,
            "learning_rate": 4.5000000000000003e-07,
            "epoch": 9.55,
            "step": 5730
        },
        {
            "loss": 0.2646,
            "grad_norm": 0.5900332927703857,
            "learning_rate": 4.333333333333334e-07,
            "epoch": 9.566666666666666,
            "step": 5740
        },
        {
            "loss": 0.2733,
            "grad_norm": 0.4150998592376709,
            "learning_rate": 4.1666666666666667e-07,
            "epoch": 9.583333333333334,
            "step": 5750
        },
        {
            "loss": 0.2073,
            "grad_norm": 16.08316993713379,
            "learning_rate": 4.0000000000000003e-07,
            "epoch": 9.6,
            "step": 5760
        },
        {
            "loss": 0.2499,
            "grad_norm": 24.670743942260742,
            "learning_rate": 3.8333333333333335e-07,
            "epoch": 9.616666666666667,
            "step": 5770
        },
        {
            "loss": 0.2307,
            "grad_norm": 15.190457344055176,
            "learning_rate": 3.666666666666667e-07,
            "epoch": 9.633333333333333,
            "step": 5780
        },
        {
            "loss": 0.3131,
            "grad_norm": 15.074323654174805,
            "learning_rate": 3.5000000000000004e-07,
            "epoch": 9.65,
            "step": 5790
        },
        {
            "loss": 0.1178,
            "grad_norm": 10.085447311401367,
            "learning_rate": 3.3333333333333335e-07,
            "epoch": 9.666666666666666,
            "step": 5800
        },
        {
            "loss": 0.3697,
            "grad_norm": 22.874828338623047,
            "learning_rate": 3.166666666666667e-07,
            "epoch": 9.683333333333334,
            "step": 5810
        },
        {
            "loss": 0.3353,
            "grad_norm": 3.7233264446258545,
            "learning_rate": 3.0000000000000004e-07,
            "epoch": 9.7,
            "step": 5820
        },
        {
            "loss": 0.1576,
            "grad_norm": 6.122485637664795,
            "learning_rate": 2.8333333333333336e-07,
            "epoch": 9.716666666666667,
            "step": 5830
        },
        {
            "loss": 0.1878,
            "grad_norm": 12.923656463623047,
            "learning_rate": 2.666666666666667e-07,
            "epoch": 9.733333333333333,
            "step": 5840
        },
        {
            "loss": 0.3501,
            "grad_norm": 12.861528396606445,
            "learning_rate": 2.5000000000000004e-07,
            "epoch": 9.75,
            "step": 5850
        },
        {
            "loss": 0.3657,
            "grad_norm": 18.759244918823242,
            "learning_rate": 2.3333333333333336e-07,
            "epoch": 9.766666666666667,
            "step": 5860
        },
        {
            "loss": 0.2788,
            "grad_norm": 8.865570068359375,
            "learning_rate": 2.166666666666667e-07,
            "epoch": 9.783333333333333,
            "step": 5870
        },
        {
            "loss": 0.1107,
            "grad_norm": 5.551924228668213,
            "learning_rate": 2.0000000000000002e-07,
            "epoch": 9.8,
            "step": 5880
        },
        {
            "loss": 0.251,
            "grad_norm": 1.431130051612854,
            "learning_rate": 1.8333333333333336e-07,
            "epoch": 9.816666666666666,
            "step": 5890
        },
        {
            "loss": 0.1493,
            "grad_norm": 11.639857292175293,
            "learning_rate": 1.6666666666666668e-07,
            "epoch": 9.833333333333334,
            "step": 5900
        },
        {
            "loss": 0.1858,
            "grad_norm": 2.0986945629119873,
            "learning_rate": 1.5000000000000002e-07,
            "epoch": 9.85,
            "step": 5910
        },
        {
            "loss": 0.275,
            "grad_norm": 0.9633440971374512,
            "learning_rate": 1.3333333333333336e-07,
            "epoch": 9.866666666666667,
            "step": 5920
        },
        {
            "loss": 0.3026,
            "grad_norm": 8.439467430114746,
            "learning_rate": 1.1666666666666668e-07,
            "epoch": 9.883333333333333,
            "step": 5930
        },
        {
            "loss": 0.106,
            "grad_norm": 1.6324599981307983,
            "learning_rate": 1.0000000000000001e-07,
            "epoch": 9.9,
            "step": 5940
        },
        {
            "loss": 0.1904,
            "grad_norm": 10.271844863891602,
            "learning_rate": 8.333333333333334e-08,
            "epoch": 9.916666666666666,
            "step": 5950
        },
        {
            "loss": 0.1176,
            "grad_norm": 0.957936704158783,
            "learning_rate": 6.666666666666668e-08,
            "epoch": 9.933333333333334,
            "step": 5960
        },
        {
            "loss": 0.2031,
            "grad_norm": 10.34504222869873,
            "learning_rate": 5.0000000000000004e-08,
            "epoch": 9.95,
            "step": 5970
        },
        {
            "loss": 0.165,
            "grad_norm": 0.686289370059967,
            "learning_rate": 3.333333333333334e-08,
            "epoch": 9.966666666666667,
            "step": 5980
        },
        {
            "loss": 0.2764,
            "grad_norm": 14.536575317382812,
            "learning_rate": 1.666666666666667e-08,
            "epoch": 9.983333333333333,
            "step": 5990
        },
        {
            "loss": 0.275,
            "grad_norm": 11.22829818725586,
            "learning_rate": 0.0,
            "epoch": 10.0,
            "step": 6000
        },
        {
            "eval_loss": 0.2992420494556427,
            "eval_accuracy": 0.9108333333333334,
            "eval_precision": 0.9133855922590234,
            "eval_recall": 0.9108333333333334,
            "eval_f1": 0.9110569797530362,
            "eval_runtime": 160.5997,
            "eval_samples_per_second": 7.472,
            "eval_steps_per_second": 0.934,
            "epoch": 10.0,
            "step": 6000
        },
        {
            "train_runtime": 18233.4511,
            "train_samples_per_second": 2.633,
            "train_steps_per_second": 0.329,
            "total_flos": 1.2804524015616e+16,
            "train_loss": 0.3777238287627697,
            "epoch": 10.0,
            "step": 6000
        },
        {
            "eval_loss": 0.2969299256801605,
            "eval_accuracy": 0.9116666666666666,
            "eval_precision": 0.9142521773735073,
            "eval_recall": 0.9116666666666666,
            "eval_f1": 0.9119584346167087,
            "eval_runtime": 163.0831,
            "eval_samples_per_second": 7.358,
            "eval_steps_per_second": 0.92,
            "epoch": 10.0,
            "step": 6000
        }
    ],
    "eval_results": {
        "eval_loss": 0.2969299256801605,
        "eval_accuracy": 0.9116666666666666,
        "eval_precision": 0.9142521773735073,
        "eval_recall": 0.9116666666666666,
        "eval_f1": 0.9119584346167087,
        "eval_runtime": 163.0831,
        "eval_samples_per_second": 7.358,
        "eval_steps_per_second": 0.92,
        "epoch": 10.0
    }
}